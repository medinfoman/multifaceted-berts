{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0562b48",
   "metadata": {},
   "source": [
    "# Source to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import re\n",
    "import six\n",
    "import glob\n",
    "\n",
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google BERT\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e4c355",
   "metadata": {},
   "source": [
    "# Split sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKoreanOnly(text):\n",
    "    hangulonly = re.compile('[^ ㄱ-ㅣ가-힣]+')\n",
    "    result = hangulonly.sub('', text)\n",
    "    return result\n",
    "\n",
    "def getEnglishOnly(text):\n",
    "    enonly = re.compile('[^ a-zA-Z]+')\n",
    "    result = enonly.sub('', text)\n",
    "    return result\n",
    "\n",
    "def parseRule1(lines):\n",
    "    lines = lines.split('\\n')\n",
    "    \n",
    "    regexp_rear = re.compile('(니다\\.?|었음\\.?|였음\\.?|있음\\.?(?!\\)|없음\\.?|'\\\n",
    "                        '행함\\.?|렸다(\\.|,)?)|옴\\.|받음\\.|호전(됨)?\\.|임\\.|짐\\.|았음\\.|퇴원함\\.|'\\\n",
    "                        '다고 함\\.|발생함\\.|함\\.|됨\\.)')\n",
    "    regexp_front = re.compile('\\*검사')\n",
    "    \n",
    "    newlines = []\n",
    "    linestack = \"\"\n",
    "    for line in lines:\n",
    "\n",
    "        korOnlyTmp = getKoreanOnly(line).strip()\n",
    "        engOnlyTmp = getEnglishOnly(line).strip()\n",
    "        if len(korOnlyTmp)<2 and len(engOnlyTmp)<2:\n",
    "            newlines.append(line.strip())\n",
    "            continue\n",
    "        \n",
    "        # lines of hangul        \n",
    "        if len(korOnlyTmp)>2:\n",
    "            if (('안녕' in line) and ('선생님' in line)):\n",
    "                if linestack!='':\n",
    "                    iter = re.finditer(regexp_rear, linestack)\n",
    "                    indices_r = [m.end(0) for m in iter]\n",
    "                    iter = re.finditer(regexp_front, linestack)\n",
    "                    indices_f = [m.start(0) for m in iter]\n",
    "                    indices = []\n",
    "                    for i in range(len(indices_r)):\n",
    "                        indices.append(indices_r[i])\n",
    "                    for i in range(len(indices_f)):\n",
    "                        indices.append(indices_f[i])\n",
    "                    indices = np.array(indices)\n",
    "                    indices.sort()\n",
    "                    indices = np.unique(indices)\n",
    "                    \n",
    "                    lastidx = 0\n",
    "                    for i in range(len(indices)):\n",
    "                        linetmp = textsep(linestack[lastidx:indices[i]].strip())\n",
    "                        if linetmp!=\"\":\n",
    "                            newlines.append(linetmp)\n",
    "                        lastidx=indices[i]\n",
    "    \n",
    "                    if lastidx!=len(linestack):\n",
    "                        linetmp = textsep(linestack[lastidx:].strip())\n",
    "                        if linetmp!=\"\":\n",
    "                            newlines.append(linetmp)\n",
    "                    linestack=\"\"\n",
    "    \n",
    "                linetmp = textsep(line.strip())\n",
    "                newlines.append(linetmp)\n",
    "                continue\n",
    "            \n",
    "            else:  \n",
    "                linestack = linestack.strip() + ' ' + line.strip()\n",
    "                \n",
    "        # non-Hangul lines\n",
    "        else:\n",
    "            if linestack!='':\n",
    "                iter = re.finditer(regexp_rear, linestack)\n",
    "                indices_r = [m.end(0) for m in iter]\n",
    "                iter = re.finditer(regexp_front, linestack)\n",
    "                indices_f = [m.start(0) for m in iter]\n",
    "                indices = []\n",
    "                for i in range(len(indices_r)):\n",
    "                    indices.append(indices_r[i])\n",
    "                for i in range(len(indices_f)):\n",
    "                    indices.append(indices_f[i])\n",
    "                indices = np.array(indices)\n",
    "                indices.sort()\n",
    "                indices = np.unique(indices)\n",
    "                \n",
    "                lastidx = 0\n",
    "                for i in range(len(indices)):\n",
    "                    linetmp = textsep(linestack[lastidx:indices[i]].strip())        \n",
    "                    if linetmp!=\"\":\n",
    "                        newlines.append(linetmp)\n",
    "                    lastidx=indices[i]\n",
    "                \n",
    "                if lastidx!=len(linestack):\n",
    "                    linetmp = textsep(linestack[lastidx:].strip())\n",
    "                    if linetmp!=\"\":\n",
    "                        newlines.append(linetmp)\n",
    "                linestack=\"\"\n",
    "\n",
    "            # add current line\n",
    "            linetmp = textsep(line.strip())\n",
    "            newlines.append(linetmp)\n",
    "    \n",
    "    \n",
    "    if linestack!='':\n",
    "        # linestack 에 쌓인 문장들을 구분\n",
    "        iter = re.finditer(regexp_rear, linestack)\n",
    "        indices_r = [m.end(0) for m in iter]\n",
    "        iter = re.finditer(regexp_front, linestack)\n",
    "        indices_f = [m.start(0) for m in iter]\n",
    "        indices = []\n",
    "        for i in range(len(indices_r)):\n",
    "            indices.append(indices_r[i])\n",
    "        for i in range(len(indices_f)):\n",
    "            indices.append(indices_f[i])\n",
    "        indices = np.array(indices)\n",
    "        indices.sort()\n",
    "        indices = np.unique(indices)\n",
    "                \n",
    "        lastidx = 0\n",
    "        for i in range(len(indices)):\n",
    "            linetmp = textsep(linestack[lastidx:indices[i]].strip())\n",
    "            if linetmp!=\"\":\n",
    "                newlines.append(linetmp)\n",
    "            lastidx=indices[i]\n",
    "    \n",
    "        if lastidx!=len(linestack):\n",
    "            linetmp = textsep(linestack[lastidx:].strip())\n",
    "            if linetmp!=\"\":\n",
    "                newlines.append(linetmp)\n",
    "        linestack=\"\"\n",
    "        \n",
    "    return '\\n'.join(newlines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec919ee",
   "metadata": {},
   "source": [
    "# add whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ae29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_en_ko(text):\n",
    "    sep_en_ko = re.compile('[a-zA-Z][가-힇]') \n",
    "    iter = re.finditer(sep_en_ko, text)\n",
    "    indices = [m.end(0) for m in iter]\n",
    "    \n",
    "    lastidx = 0\n",
    "    seperatedtext=[]\n",
    "    if len(indices) >= 1:\n",
    "        lastidx = 0\n",
    "        for i in range(len(indices)):\n",
    "            seperatedtext.append(text[lastidx:indices[i]-1].strip())\n",
    "            lastidx=indices[i]-1\n",
    "        seperatedtext.append(text[lastidx:len(text)].strip())\n",
    "        \n",
    "        text = \" \".join(seperatedtext)\n",
    "\n",
    "    # 한글 영문\n",
    "    sep_ko_en = re.compile('[가-힇][a-zA-Z]') \n",
    "    iter = re.finditer(sep_ko_en, text)\n",
    "    indices = [m.end(0) for m in iter]\n",
    "\n",
    "    lastidx = 0\n",
    "    seperatedtext=[]\n",
    "    if len(indices) >= 1:\n",
    "        lastidx = 0\n",
    "        for i in range(len(indices)):\n",
    "            seperatedtext.append(text[lastidx:indices[i]-1].strip())\n",
    "            lastidx=indices[i]-1\n",
    "        seperatedtext.append(text[lastidx:len(text)].strip())\n",
    "        \n",
    "        text = \" \".join(seperatedtext)\n",
    "        \n",
    "    return text\n",
    "\n",
    "whitespace_en_ko(\"그러나ER에서 곧바로 ICU 전동함   올라온 직후, cardiac arrest    -> CPR, but not resuscitation   -> expire\")\n",
    "whitespace_en_ko(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def whitespace_number_spchar_char(text):\n",
    "    sep_en_ko = re.compile('[a-zA-Z가-힇][.0-9 \\(\\{\\[\\-_\\]\\}\\]\\\\\\/]') \n",
    "    iter = re.finditer(sep_en_ko, text)\n",
    "    indices = [m.end(0) for m in iter]\n",
    "    \n",
    "    lastidx = 0\n",
    "    seperatedtext=[]\n",
    "    if len(indices) >= 1:\n",
    "        lastidx = 0\n",
    "        for i in range(len(indices)):\n",
    "#             print(text[lastidx:indices[i]-1])\n",
    "            seperatedtext.append(text[lastidx:indices[i]-1].strip())\n",
    "            lastidx=indices[i]-1\n",
    "        seperatedtext.append(text[lastidx:len(text)].strip())\n",
    "        \n",
    "        text = \" \".join(seperatedtext)\n",
    "#         print(\"text: \", text)\n",
    "\n",
    "    \n",
    "    sep_ko_en = re.compile('[.0-9 \\(\\{\\[\\-_\\]\\}\\]\\\\\\/][a-zA-Z가-힇]') \n",
    "    iter = re.finditer(sep_ko_en, text)\n",
    "    indices = [m.end(0) for m in iter]\n",
    "\n",
    "    lastidx = 0\n",
    "    seperatedtext=[]\n",
    "    if len(indices) >= 1:\n",
    "        lastidx = 0\n",
    "        for i in range(len(indices)):\n",
    "#             print(text[lastidx:indices[i]-1])\n",
    "            seperatedtext.append(text[lastidx:indices[i]-1].strip())\n",
    "            lastidx=indices[i]-1\n",
    "        seperatedtext.append(text[lastidx:len(text)].strip())\n",
    "        \n",
    "        text = \" \".join(seperatedtext)\n",
    "#         print(\"text: \", text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "text = \"Ultracet ER tab(Acetaminopehn 650mg/Tramadol 75mg) 1tab [P.O] daily pc 많이 아플 때 추가로 드세요 X10 Days\"\n",
    "whitespace_number_spchar_char(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108918d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_num_day(text):\n",
    "    days_sep = re.compile('[ 0-9]+일\\s+[ 0-9]+회\\s+\\*\\s+[ 0-9]+일') \n",
    "    \n",
    "    iter = re.finditer(days_sep, text)\n",
    "    indices = [m.end(0) for m in iter]\n",
    "    \n",
    "    seperatedtext=[]\n",
    "    if len(indices) >= 1:\n",
    "        lastidx = 0\n",
    "        for i in range(len(indices)):\n",
    "            seperatedtext.append(text[lastidx:indices[i]].strip())\n",
    "            lastidx=indices[i]\n",
    "    else:\n",
    "        return text\n",
    "    \n",
    "    return '\\n'.join(seperatedtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba71544",
   "metadata": {},
   "source": [
    "### special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d294f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def textsep(text):\n",
    "    text = re.sub(r'(?<=\\()\\+', ' + ', text)\n",
    "    text = re.sub(r'\\+(?=\\))', ' + ', text)\n",
    "    text = re.sub(r'(?<=/)\\+(?=/)', ' + ', text)\n",
    "    text = re.sub(r'\\(\\+\\)', ' ( + ) ', text)\n",
    "    text = re.sub(r'(?<=/)\\+', ' + ', text)  # /+ => / +\n",
    "    text = re.sub(r'\\+(?=/)', ' + ', text)\n",
    "    text = re.sub(r'\\s+\\+\\s+', ' + ', text) #+ 를 ' + ' 으로\n",
    "    \n",
    "    text = re.sub(r'(?<=\\()\\-', ' - ', text)\n",
    "    text = re.sub(r'\\-(?=\\))', ' - ', text)\n",
    "    text = re.sub(r'(?<=/)\\-(?=/)', ' - ', text)\n",
    "    text = re.sub(r'\\(\\-\\)', ' ( - ) ', text)\n",
    "    text = re.sub(r'(?<=/)\\-', ' - ', text)  # /- => / -\n",
    "    text = re.sub(r'\\-(?=/)', ' - ', text)\n",
    "    text = re.sub(r'\\s+\\-\\s+', ' - ', text) #-를 ' - ' 으로\n",
    "    \n",
    "    text = re.sub(r'\\s?\\(\\s?', ' ( ', text)\n",
    "    text = re.sub(r'\\s?\\)\\s?', ' ) ', text)\n",
    "    \n",
    "    # date expression with - \n",
    "    text = re.sub(r'(?<=\\d)\\-(?=\\d)', ' - ', text)\n",
    "    text = re.sub(r'(?<=\\d)\\/(?=\\d)', ' / ', text)\n",
    "    text = re.sub(r'(?<=\\d)\\.(?=\\d)', ' . ', text)\n",
    "        \n",
    "    text = re.sub(r'x ray', 'x-ray ', text)\n",
    "    text = re.sub(r'X ray', 'X-ray ', text)\n",
    "    \n",
    "    # P/E>   P/E : \n",
    "    text = re.sub(r'(?<=[.,a-zA-Z])\\>', ' > ', text)\n",
    "    \n",
    "    # ,    ' , '\n",
    "    text = re.sub(r'\\,', ' , ', text)\n",
    "    text = re.sub(r'\\s+,\\s+', ' , ', text) #, 를 ' , ' 으로\n",
    "    \n",
    "    # :  ' : '\n",
    "    text = re.sub(r'(?<=[a-zA-Z])\\:', ' : ', text)\n",
    "    text = re.sub(r'\\s+\\:\\s+', ' : ', text) #, 를 ' , ' 으로\n",
    "    \n",
    "    text = re.sub(r'#\\s?[0-9]+\\.\\s+', '', text)\n",
    "    \n",
    "    text = re.sub(r'\\-{5,}', '', text)\n",
    "    text = re.sub(r'\\={5,}', '', text)\n",
    "    \n",
    "    text = re.sub(r'\\(', ' ( ', text)\n",
    "    text = re.sub(r'\\s+\\(\\s+', ' ( ', text)\n",
    "    \n",
    "    text = re.sub(r'\\)', ' ) ', text)\n",
    "    text = re.sub(r'\\s+\\)\\s+', ' ) ', text)\n",
    "    \n",
    "    text = re.sub(r'\\:', ' : ', text)\n",
    "    text = re.sub(r'\\s+\\:\\s+', ' : ', text)\n",
    "    \n",
    "    text = re.sub(r';', ' ; ', text)\n",
    "    text = re.sub(r'\\s+;\\s+', ' ; ', text)\n",
    "    \n",
    "    text = re.sub(r'(?<=[0-9])일', ' 일', text)\n",
    "    text = re.sub(r'(?<=[0-9])회', ' 회', text)\n",
    "\n",
    "    text = re.sub(r'tab', ' tab ', text)\n",
    "    \n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\t+', '\\t', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "text = \"수진 ( 진료 ) 일 : 2011-01-13 진료과 : 소화기내과 Description <- 소견 및 계획 : s )\"\n",
    "text_ = textsep(text)\n",
    "print(text)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd55ba7",
   "metadata": {},
   "source": [
    "## deidentification\n",
    "- After delete information automatically based on rulebase, manually deleted sensitive contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7038cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deidentificaiton(text):    \n",
    "    text = re.sub(r'([가-힇])+(\\s+)?배상', 'OOO 배상', text)\n",
    "    text = re.sub(r'([가-힇])+(\\s+)?교수', 'OOO 교수', text)\n",
    "    text = re.sub(r'([가-힇])+(\\s+)?선생님', 'OOO 선생님', text)\n",
    "    text = re.sub(r'pf(\\s+)?([가-힇])+', 'pf OOO', text)\n",
    "    text = re.sub(r'pf\\.(\\s+)?([가-힇])+', 'pf. OOO', text)\n",
    "    text = re.sub(r'Pf\\.(\\s+)?([가-힇])+', 'pf. OOO', text)\n",
    "    text = re.sub(r'PA(\\s+)?([가-힇])+', 'PA OOO', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "text = \"PA 김경모\"\n",
    "deidentificaiton(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8b788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_enter(lines):\n",
    "    del_entered = []\n",
    "    lines = str(lines).split(\"\\n\")\n",
    "    for l in range(len(lines)):\n",
    "        line = lines[l].strip(\"\\n\")\n",
    "        line = lines[l].strip()\n",
    "        if line==\"\":\n",
    "            continue\n",
    "        else:\n",
    "            del_entered.append(line)\n",
    "    \n",
    "    newlines = \"\\n\".join(del_entered)\n",
    "    \n",
    "    return newlines\n",
    "\n",
    "lines = \"GW으로 검사하심\\n\\n\\n낮에 누가 집에 다녀간다\"\n",
    "delete_enter(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aec8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tab changed to \"    \"\n",
    "def tab_to_spaces(line):\n",
    "    line = line.replace(\"\\t\", \"    \")\n",
    "    return line\n",
    "tab_to_spaces(\"Uric acid\t5 . 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dacfe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(lines):\n",
    "    if len(str(lines).strip(\"\\n\").strip())==0:\n",
    "        return []\n",
    "    data = []\n",
    "    lines = str(lines).split(\"\\n\")\n",
    "    for l in range(len(lines)):\n",
    "        line = lines[l].strip()\n",
    "        line = tab_to_spaces(line)\n",
    "        line = delete_enter(line)\n",
    "        line = parseRule1(line)\n",
    "        line = textsep(line)\n",
    "\n",
    "        line = deidentificaiton(line)\n",
    "        line = whitespace_en_ko(line)\n",
    "        line = whitespace_number_spchar_char(line)        \n",
    "        line = delete_enter(line)\n",
    "        line = line.strip()\n",
    "        if line==\"\":\n",
    "            continue\n",
    "        data.append(line)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378fbc7d",
   "metadata": {},
   "source": [
    "# Merge section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd920250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def runloop(ws, outputpath):\n",
    "    start_time = time.time()\n",
    "\n",
    "    doccount = 0\n",
    "    docperfile = 50\n",
    "\n",
    "    docout_lastnum = 0\n",
    "    documents = \"\"\n",
    "    \n",
    "    for row in range(0, ws.shape[0]):\n",
    "        pt_current = ws['fake_id'][row] # ubuntu\n",
    "        dt_current = ws['수진(진료)일'][row]\n",
    "        dp_current = ws['서식명'][row]\n",
    "        se_current = ws['서식항목명'][row]\n",
    "        \n",
    "        dp_current = ws['서식명'][row]\n",
    "        prep = text_preprocessing(dp_current)\n",
    "        dp_current = \" \".join(prep) # document species name should be in a row\n",
    "        \n",
    "        se_current = ws['서식항목명'][row]\n",
    "        prep = text_preprocessing(se_current)\n",
    "        se_current = \" \".join(prep) # section name should be in a row\n",
    "        \n",
    "        data = ws['서식내용'][row] \n",
    "        prep = text_preprocessing(data)\n",
    "        data = \"\\n\".join(prep) # original text allow enter values\n",
    "                \n",
    "        # check the the target document\n",
    "        # because target document lines are shown in multiple rows\n",
    "        if row==0:\n",
    "            documents = \"서식명 : \"+dp_current+\" 진료일 : \"+textsep(dt_current) + \"\\n\"\n",
    "            documents = documents+se_current + \" : \"+data + \"\\n\"\n",
    "        \n",
    "        elif pt_current==pt_last and dt_last==dt_current and dp_current==dp_last:\n",
    "            documents = documents + se_current + \" : \"+data + \"\\n\"\n",
    "        \n",
    "        else:\n",
    "            doccount = doccount+1\n",
    "\n",
    "            target_folder = output_directory+\"/\"+str(((pt_last//docperfile)+1)*docperfile)\n",
    "            if not os.path.exists(target_folder):\n",
    "                os.makedirs(target_folder)\n",
    "            target_txt = str(\"\".join(['0']*(8-len(str(pt_last)))) + str(int(pt_last)))\n",
    "            dout = target_folder+\"/\"+str(target_txt)+\".txt\"\n",
    "\n",
    "            f = open(dout, \"a\")\n",
    "            f.write(documents+\"\\n\")\n",
    "            f.close()\n",
    "\n",
    "            documents = \"서식명 : \"+dp_current+\" 진료일 : \"+textsep(dt_current) + \"\\n\"\n",
    "            documents = documents+se_current + \" : \"+data + \"\\n\"\n",
    "        \n",
    "        # prepare next step\n",
    "        pt_last = pt_current\n",
    "        dp_last = dp_current\n",
    "        dt_last = dt_current\n",
    "        \n",
    "    # rest\n",
    "    if documents!='':\n",
    "        target_folder = output_directory+\"/\"+str(((pt_last//docperfile)+1)*docperfile)\n",
    "        if not os.path.exists(target_folder):\n",
    "            os.makedirs(target_folder)\n",
    "        target_txt = str(\"\".join(['0']*(8-len(str(pt_last)))) + str(int(pt_last)))\n",
    "        dout = target_folder+\"/\"+str(target_txt)+\".txt\"\n",
    "        \n",
    "        f = open(dout, \"a\")\n",
    "        f.write(documents+\"\\n\\n\")\n",
    "        f.close()\n",
    "        \n",
    "        documents=\"\"\n",
    "\n",
    "    #write time lap\n",
    "    file1 = open(\"sentenceSplitTimelap.txt\",\"a\")\n",
    "    file1.write(\"\\n--- %s finished seconds ---\" % (time.time() - start_time))\n",
    "    file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea16d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_directory = \"./SNUH_visit_2011to2020\"\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "filenames = glob.glob('../sources/visits_2011to2020/*.xlsx')\n",
    "filenames.sort()\n",
    "print(filenames)\n",
    "for f in range(len(filenames)):\n",
    "    reading_file = filenames[f].split(\"/\")[-1]\n",
    "    print(\"reading_file: \", reading_file)\n",
    "    \n",
    "    print(\"reading...\")\n",
    "    ws = pd.read_excel(filenames[f])\n",
    "    \n",
    "    print(\"processing...\")\n",
    "    runloop(ws, outputpath=output_directory)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d347c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
