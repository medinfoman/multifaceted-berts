{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f11145",
   "metadata": {},
   "source": [
    "# Make cache (MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install boto3\n",
    "# pip install requests\n",
    "# pip install tqdm\n",
    "\n",
    "import os\n",
    "import six\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "import glob\n",
    "\n",
    "from tokenization import BertTokenizer\n",
    "import tokenization as tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0505025",
   "metadata": {},
   "source": [
    "## 4. write_instance_to_example_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a65b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, \n",
    "                 masked_lm_ids_maxseq,\n",
    "                 next_sentence_labels):\n",
    "        self.input_ids=input_ids, \n",
    "        self.input_mask=input_mask, \n",
    "        self.segment_ids=segment_ids, \n",
    "        self.masked_lm_ids_maxseq=masked_lm_ids_maxseq, \n",
    "        self.next_sentence_labels=next_sentence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instance_to_example_files(instances, \n",
    "                                    tokenizer, \n",
    "                                    vocab_words,\n",
    "                                    max_seq_length,\n",
    "                                    max_predictions_per_seq, \n",
    "                                    outfilename):\n",
    "    features = []\n",
    "    \n",
    "    pad_id = tokenizer.convert_tokens_to_ids([\"[PAD]\"])\n",
    "    pad_id = pad_id[0]\n",
    "    \n",
    "    for (inst_index, instance) in enumerate(instances):\n",
    "        \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)        \n",
    "        input_mask = [1] * len(input_ids)\n",
    "        segment_ids = list(instance.segment_ids)\n",
    "        assert len(input_ids) <= max_seq_length\n",
    "\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "\n",
    "        next_sentence_label = [0]*max_seq_length\n",
    "        next_sentence_label[0] = 1 if instance.is_random_next else 0\n",
    "        \n",
    "        labels_tmp = instance.masked_lm_labels\n",
    "        \n",
    "        masked_lm_labels = [0]*max_seq_length\n",
    "        for m in range(len(masked_lm_labels)):\n",
    "            if labels_tmp[m]!=0:\n",
    "                masked_lm_labels[m] = vocab_words.w_to_i[labels_tmp[m]]\n",
    "\n",
    "        features.append(\n",
    "            TrainingInstance(\n",
    "                input_ids=input_ids, \n",
    "                input_mask=input_mask, \n",
    "                segment_ids=segment_ids, \n",
    "                masked_lm_ids_maxseq=masked_lm_labels, \n",
    "                next_sentence_labels=next_sentence_label\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    if len(features)!=0:\n",
    "        with open(outfilename, 'wb') as output:\n",
    "            pickle.dump(features, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7edfe31",
   "metadata": {},
   "source": [
    "## 3. create_masked_lm_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da91150",
   "metadata": {},
   "outputs": [],
   "source": [
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ce0a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masked_lm_predictions(tokens, masked_lm_prob,\n",
    "                                 max_predictions_per_seq, rng, vocab_words, max_seq_length):\n",
    "    \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "    cand_indexes = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        cand_indexes.append(i)\n",
    "\n",
    "    rng.shuffle(cand_indexes)    \n",
    "    output_tokens = list(tokens)\n",
    "    \n",
    "    num_to_pmax = max(1, int(round(len(tokens) * masked_lm_prob)))\n",
    "    num_to_predict = min(max_predictions_per_seq, num_to_pmax)\n",
    "    \n",
    "    # masked lm token 목록\n",
    "    masked_lms = []\n",
    "    covered_indexes = set()\n",
    "    \n",
    "    for index in cand_indexes:\n",
    "        if len(masked_lms) >= num_to_predict:\n",
    "            break\n",
    "            \n",
    "        if index in covered_indexes:\n",
    "            continue\n",
    "        covered_indexes.add(index)\n",
    "\n",
    "        masked_token = None\n",
    "        # 80% of the time, replace with [MASK]\n",
    "        if rng.random() < 0.8:\n",
    "            masked_token = \"[MASK]\"\n",
    "        else:\n",
    "            # 10% of the time, keep original\n",
    "            if rng.random() < 0.5:\n",
    "                masked_token = tokens[index]\n",
    "              # 10% of the time, replace with random word\n",
    "            else:\n",
    "                while(True):\n",
    "                    randome_mask_loc = rng.randint(0, len(vocab_words.i_to_w) - 1)\n",
    "                    masked_token = vocab_words.i_to_w[randome_mask_loc]\n",
    "                    if \"unused\" not in masked_token:\n",
    "                        break\n",
    "                \n",
    "        output_tokens[index] = masked_token        \n",
    "        masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "    \n",
    "    masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "    masked_lm_labels = [0]*max_seq_length\n",
    "    for p in masked_lms:\n",
    "        masked_lm_labels[p.index] = p.label\n",
    "    \n",
    "    return (output_tokens, masked_lm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "\n",
    "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "        assert len(trunc_tokens) >= 1\n",
    "\n",
    "        if rng.random() < 0.5:\n",
    "            del trunc_tokens[0]\n",
    "        else:\n",
    "            trunc_tokens.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302adda0",
   "metadata": {},
   "source": [
    "## 2. create_instances_from_document\n",
    "    - make features for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance_tmp(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, tokens, segment_ids, masked_lm_labels,\n",
    "               is_random_next):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_random_next = is_random_next\n",
    "        self.masked_lm_labels = masked_lm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49dc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(\n",
    "    all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, rng, vocab_words, tokenizer):\n",
    "    \n",
    "    \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
    "    \n",
    "    # make data for MLM task within one patient's records\n",
    "    # document contains multiple records of a patient\n",
    "    document = all_documents[document_index]\n",
    "\n",
    "    # [CLS], [SEP], [SEP]\n",
    "    max_num_tokens = max_seq_length - 3\n",
    "\n",
    "    target_seq_length = max_num_tokens\n",
    "    if rng.random() < short_seq_prob:\n",
    "        target_seq_length = rng.randint(2, max_num_tokens)\n",
    "    \n",
    "    instances = []\n",
    "    \n",
    "    current_chunk = []\n",
    "    \n",
    "    current_length = 0\n",
    "\n",
    "    max_sampling = 40\n",
    "    if len(document)<=max_sampling:\n",
    "        i = 0\n",
    "    else:\n",
    "        i = rng.randint(0, len(document) - 1 - max_sampling)\n",
    "    \n",
    "    while i < len(document):\n",
    "        segment = document[i]\n",
    "        current_chunk = current_chunk + segment\n",
    "        current_length += len(segment)\n",
    "        \n",
    "        if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "            if current_chunk:\n",
    "                is_random_next = False\n",
    "                a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "                \n",
    "                tokens_a = current_chunk[0:a_end]                \n",
    "                tokens_b = current_chunk[a_end:len(current_chunk)]\n",
    "                \n",
    "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
    "\n",
    "                # tokens: \n",
    "                # segment_ids: segment A:0, segment B: 1\n",
    "                tokens = []\n",
    "                segment_ids = []\n",
    "                tokens.append(\"[CLS]\")\n",
    "                segment_ids.append(0)\n",
    "                for token in tokens_a:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(0)\n",
    "\n",
    "                tokens.append(\"[SEP]\")\n",
    "                segment_ids.append(0)\n",
    "\n",
    "                for token in tokens_b:\n",
    "                    tokens.append(token)\n",
    "                    segment_ids.append(1)\n",
    "                tokens.append(\"[SEP]\")\n",
    "                segment_ids.append(1)\n",
    "                \n",
    "                (tokens, masked_lm_labels) = create_masked_lm_predictions(\n",
    "                    tokens, masked_lm_prob, max_predictions_per_seq, rng, vocab_words, max_seq_length)\n",
    "            \n",
    "                instance = TrainingInstance_tmp(\n",
    "                    tokens=tokens,\n",
    "                    segment_ids=segment_ids,\n",
    "                    is_random_next=is_random_next,\n",
    "                    masked_lm_labels=masked_lm_labels)\n",
    "                instances.append(instance)\n",
    "            \n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "#     print(\"len(instances): \", len(instances))\n",
    "    \n",
    "    \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ebc3f",
   "metadata": {},
   "source": [
    "## 1. Create Training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(input_files, tokenizer, vocab_words, max_seq_length,\n",
    "                              dupe_factor, short_seq_prob, masked_lm_prob, \n",
    "                              max_predictions_per_seq, rng, data_species, do_lower_case):\n",
    "    \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "    ###########################################################\n",
    "    def read_documents(targetfile):\n",
    "        f = open(targetfile, \"r\")\n",
    "        lines = f.readlines()\n",
    "        \n",
    "        document = []\n",
    "        \n",
    "        stack = []\n",
    "        for l in range(len(lines)):\n",
    "            line = lines[l].strip(\"\\n\").strip()\n",
    "            line = convert_to_unicode(line) # 221019 추가\n",
    "            \n",
    "            if line!=\"\":\n",
    "                if do_lower_case==True:\n",
    "                    line = line.lower()\n",
    "                    \n",
    "                tokenized = tokenizer.tokenize(line) # bertbase\n",
    "                stack.append(tokenized)\n",
    "            \n",
    "            else:\n",
    "                if len(stack)!=0:\n",
    "                    document.append(stack)\n",
    "                    stack = []\n",
    "        \n",
    "        if len(stack)!=0:\n",
    "            document.append(stack)\n",
    "            stack = []\n",
    "        \n",
    "        return document\n",
    "    ###########################################################\n",
    "    \n",
    "    documents = []\n",
    "    \n",
    "    print(\"reading...\")\n",
    "    for i in range(len(input_files)):\n",
    "        # One patient's all records are are concatenated to treat as a one doc. \n",
    "        pt_doc = read_documents(input_files[i])\n",
    "        concat_doc = []\n",
    "        for d in range(len(pt_doc)):\n",
    "            concat_doc = concat_doc + pt_doc[d]\n",
    "        \n",
    "        # all patient records\n",
    "        documents.append(concat_doc)\n",
    "    print(\"done read\")\n",
    "\n",
    "    instances = []    \n",
    "    \n",
    "    # 10 times\n",
    "    for dup in range(dupe_factor):        \n",
    "        print(\"dupe time: \", dup)\n",
    "        cutlength = 512-3 # cls, sep, sep\n",
    "        \n",
    "        arranged_documents = []\n",
    "        # documents == jump == the number of patients in a input\n",
    "        for d in range(len(documents)):\n",
    "            doc = documents[d]\n",
    "            \n",
    "            new_doc = []\n",
    "            \n",
    "            doc_tokens = []\n",
    "            \n",
    "            # segments loop\n",
    "            for c in range(len(doc)):\n",
    "                doc_tokens = doc_tokens + doc[c]\n",
    "            \n",
    "            # make segment (max length: 512==cutlength)\n",
    "            for j in range(0, len(doc_tokens), cutlength):\n",
    "                segment = doc_tokens[j:j+cutlength]\n",
    "                if len(segment)>=3: \n",
    "                    new_doc.append(segment)\n",
    "                \n",
    "            arranged_documents.append(new_doc)\n",
    "            \n",
    "    \n",
    "        # make a instance\n",
    "        for document_index in range(len(arranged_documents)):\n",
    "            if (document_index+1)%100==0:\n",
    "                print(\"document_index: \", (document_index+1), \"/\", len(arranged_documents))\n",
    "            \n",
    "            instances.extend(\n",
    "                create_instances_from_document(\n",
    "                    arranged_documents, document_index, max_seq_length, short_seq_prob,\n",
    "                    masked_lm_prob, max_predictions_per_seq, rng, vocab_words, tokenizer))\n",
    "            \n",
    "            \n",
    "    print(\"len(instances): \", len(instances))\n",
    "    rng.shuffle(instances)\n",
    "    \n",
    "    return instances\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "        \n",
    "        \n",
    "class Vocab_words(object):\n",
    "    def __init__(self, vocab_file):\n",
    "        self.i_to_w = {}\n",
    "        self.w_to_i = {}\n",
    "        self.getvocab(vocab_file)\n",
    "#         print(self.w_to_i)\n",
    "\n",
    "    def getvocab(self, vocab_file):\n",
    "        f = open(vocab_file, 'r')\n",
    "        lines = f.readlines()\n",
    "        for l in range(len(lines)):\n",
    "            #term = lines[l].strip(\"\\n\")\n",
    "            term = lines[l].strip()\n",
    "            term = convert_to_unicode(term)\n",
    "            self.i_to_w[l] = term\n",
    "            self.w_to_i[term] = l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529cf20",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    # required\n",
    "    input_files, \n",
    "    vocab_file, \n",
    "    outdir,\n",
    "    \n",
    "    do_lower_case=True,  # Cased model\n",
    "    max_seq_length=512, \n",
    "    max_predictions_per_seq = 20, \n",
    "    random_seed=12345, \n",
    "    dupe_factor = 1, \n",
    "    masked_lm_prob = 0.15,\n",
    "    short_seq_prob = 0.1,\n",
    "    data_species = \"\"\n",
    "    ):\n",
    "        \n",
    "    print(\"vocab_file: \", vocab_file)\n",
    "    tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case, max_len=max_seq_length)\n",
    "    vocab_words = Vocab_words(vocab_file)\n",
    "    #print(\"vocab_words.w_to_i: \", vocab_words.w_to_i)\n",
    "    \n",
    "    print(\"data_species: \", data_species)\n",
    "    print(\"max_predictions_per_seq: \", max_predictions_per_seq)\n",
    "    \n",
    "    # random random seed\n",
    "    randseed = random.randint(1, 1000)\n",
    "    print(\"randseed: \", randseed)\n",
    "    rng = random.Random(randseed)\n",
    "    \n",
    "    print(\"len(input_files): \", len(input_files))\n",
    "    rng.shuffle(input_files)\n",
    "    \n",
    "    # jump: the number of patients in a cache file\n",
    "    jump = 100\n",
    "    for i in range(0, len(input_files), jump):\n",
    "        sampled_inputfiles = input_files[i:i+jump]\n",
    "        print(\"len(sampled_inputfiles): \", len(sampled_inputfiles))\n",
    "        print(i, \"~\", i+jump, \"/\", len(input_files))\n",
    "        \n",
    "        instances = create_training_instances(\n",
    "            sampled_inputfiles, tokenizer, vocab_words, max_seq_length, dupe_factor,\n",
    "            short_seq_prob, masked_lm_prob, max_predictions_per_seq, rng, data_species, do_lower_case)\n",
    "\n",
    "        idx_range = (i+jump)\n",
    "        filename = str(\"\".join(['0']*(8-len(str(idx_range)))) + str(int(idx_range)))+\".cache\"        \n",
    "        print(\"filename: \", filename)\n",
    "        \n",
    "        write_instance_to_example_files(\n",
    "                        instances=instances, \n",
    "                        tokenizer=tokenizer,\n",
    "                        vocab_words=vocab_words,\n",
    "                        max_seq_length=max_seq_length,\n",
    "                        max_predictions_per_seq=max_predictions_per_seq, \n",
    "                        outfilename=outdir+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12926e5f",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951fc4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"Generating Features...\")\n",
    "\n",
    "upper_path = \"..\"\n",
    "target_paths = [\"../00_data4pretrain/SNUH_visit_2011to2020\"]\n",
    "vocab_path =  \"../../../otherberts/bertbase_cased\"\n",
    "\n",
    "# folder loop\n",
    "files = []\n",
    "for t in range(len(target_paths)):\n",
    "    patients = []\n",
    "    \n",
    "    print(\"target_paths[t]: \", target_paths[t])\n",
    "    \n",
    "    # groups\n",
    "    groups = glob.glob(upper_path + \"/\" + target_paths[t]+\"/*\")\n",
    "    print(\"len(groups): \", len(groups))\n",
    "    \n",
    "    for g in range(len(groups)):\n",
    "        patients = patients + glob.glob(groups[g]+\"/*.txt\")\n",
    "    print(\"len(patients): \", len(patients))\n",
    "    \n",
    "    files = files + patients\n",
    "patients = []\n",
    "\n",
    "out_directory = \"./cache/\"\n",
    "if not os.path.exists(out_directory):\n",
    "    os.makedirs(out_directory)\n",
    "\n",
    "main(\n",
    "    # required\n",
    "    input_files = files, \n",
    "    outdir = out_directory,\n",
    "    vocab_file = vocab_path+'/vocab.txt', \n",
    "\n",
    "    # optional\n",
    "    do_lower_case = False,\n",
    "    max_seq_length = 512, \n",
    "    max_predictions_per_seq = 76,\n",
    "    dupe_factor = 10,\n",
    "    masked_lm_prob = 0.15,\n",
    "    short_seq_prob = 0.1\n",
    ")\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f131f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df13fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28aa2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f886b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
