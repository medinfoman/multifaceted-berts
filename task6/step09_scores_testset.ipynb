{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd07cb6c",
   "metadata": {},
   "source": [
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0541e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import sequence_labeling\n",
    "# bert\n",
    "from tokenization import BertTokenizer\n",
    "import tokenization as tokenization\n",
    "# kobert tokenizer\n",
    "import gluonnlp as nlp\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "\n",
    "vocab_paths = [\n",
    "    \"../otherberts/bertbase_cased\",\n",
    "    \"../otherberts/mbert_cased\",\n",
    "    \"../otherberts/bioBERT\",\n",
    "    \"../otherberts/KoBERT\",\n",
    "]\n",
    "\n",
    "preds_paths = [\n",
    "    \"./data/08_preds/ver9.1.4_521121_epoch2/test_pred\",\n",
    "    \"./data/08_preds/ver8.1.4_1142642_epoch2/test_pred\",\n",
    "    './data/08_preds/ver11.1.4_521079_epoch2/test_pred',\n",
    "    './data/08_preds/ver12.1.4_407013_epoch2/test_pred',\n",
    "]\n",
    "\n",
    "assert len(vocab_paths)==len(preds_paths)\n",
    "\n",
    "\n",
    "# vocab_words\n",
    "class Vocab_words(object):\n",
    "    def __init__(self, vocab_file):\n",
    "        self.i_to_w = {}\n",
    "        self.w_to_i = {}\n",
    "        self.getvocab(vocab_file)\n",
    "\n",
    "    def getvocab(self, vocab_file):\n",
    "        f = open(vocab_file, 'r')\n",
    "        lines = f.readlines()\n",
    "        for l in range(len(lines)):\n",
    "            term = lines[l].strip(\"\\n\")\n",
    "            term = convert_to_unicode(term)\n",
    "            self.i_to_w[int(l)] = term\n",
    "            self.w_to_i[term] = int(l)\n",
    "            \n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(targetfile, mode):\n",
    "    f = open(\"./data/05_samples/\"+str(mode)+\"/\"+str(targetfile), \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    questions = []\n",
    "    labels = []\n",
    "    for l in range(len(lines)):\n",
    "        line = lines[l].strip(\"\\n\")\n",
    "        if line==\"\":\n",
    "            continue\n",
    "            \n",
    "        # info line\n",
    "        if \"[START_QUESTION]\"==line:\n",
    "            quest = []\n",
    "            label_q = []\n",
    "        elif \"[END_QUESTION]\"==line:\n",
    "            assert len(quest)==5\n",
    "            assert len(quest)==len(label_q)\n",
    "            \n",
    "            questions.append(quest)\n",
    "            labels.append(label_q)\n",
    "            \n",
    "        elif line!=0:\n",
    "            filename  = line.split(\"\\t\")[0]\n",
    "            doc_order = int(line.split(\"\\t\")[1])\n",
    "            date      = line.split(\"\\t\")[2]\n",
    "            content   = line.split(\"\\t\")[3]\n",
    "            \n",
    "            if doc_order==4:\n",
    "                label_tmp = 1\n",
    "            else:\n",
    "                label_tmp = 0\n",
    "                \n",
    "            quest.append(content)\n",
    "            label_q.append(label_tmp)\n",
    "        \n",
    "    return questions, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac0804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation 1. Evaluation viewed as a problem of selecting one of four documents\n",
    "def prediction(filepath, vocab_path, collect_tokens=True):\n",
    "    file = open(filepath, \"r\")\n",
    "    filename = filepath.split(\"/\")[-1]\n",
    "    filename = filename.split(\".\")[0]\n",
    "    lines = file.readlines()\n",
    "    \n",
    "    fileinfo = read_documents(filename+\".txt\", mode=\"test\")\n",
    "    fileinfo = fileinfo[0]\n",
    "    \n",
    "    probs = []\n",
    "    labels = []\n",
    "    outtext = []\n",
    "    if collect_tokens==True:\n",
    "        # vocab\n",
    "        if \"mbert_\" in vocab_path.lower():\n",
    "            vocab_file = vocab_path+'/vocab.txt'\n",
    "            tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=False, max_len=512)\n",
    "            vocab_words = list(tokenizer.vocab.keys())\n",
    "\n",
    "        elif \"biobert_\" in vocab_path.lower():\n",
    "            vocab_file = vocab_path+'/vocab.txt'\n",
    "            tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=True, max_len=512)\n",
    "            vocab_words = list(tokenizer.vocab.keys())\n",
    "\n",
    "        elif \"bertbase_\" in vocab_path.lower():\n",
    "            vocab_file = vocab_path+'/vocab.txt'\n",
    "            tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=True, max_len=512)\n",
    "            vocab_words = list(tokenizer.vocab.keys())\n",
    "\n",
    "        elif \"kobert\" in vocab_path.lower():\n",
    "            vocab_file = \"../otherberts/KoBERT/models/vocab.txt\"\n",
    "            vocab_words = Vocab_words(vocab_file)\n",
    "\n",
    "            # sptokenizer\n",
    "            spmodel = \"../otherberts/KoBERT/models/spiece.model\"\n",
    "            tokenizer = spm.SentencePieceProcessor()\n",
    "            tokenizer.load(spmodel)\n",
    "        else:\n",
    "            vocab_file = vocab_path+'/vocab.txt'\n",
    "            vocab_words = Vocab_words(vocab_file)\n",
    "\n",
    "        for p in range(len(lines)):\n",
    "            line = lines[p].strip(\"\\n\")\n",
    "            prob = float(line.split(\"\\t\")[0])\n",
    "            label = int(line.split(\"\\t\")[1])\n",
    "            query = line.split(\"\\t\")[2]\n",
    "            query_tokens = query.split(\",\")\n",
    "            tokens_q = []\n",
    "            for i in range(len(query_tokens)):\n",
    "                token_id = int(query_tokens[i])\n",
    "                if \"mbert\" in vocab_path.lower():\n",
    "                    token_str = vocab_words[token_id]\n",
    "                elif \"biobert\" in vocab_path.lower():\n",
    "                    token_str = vocab_words[token_id]\n",
    "                elif \"bertbase\" in vocab_path.lower():\n",
    "                    token_str = vocab_words[token_id]\n",
    "                elif \"kobert\" in vocab_path.lower():\n",
    "                    token_str = vocab_words.i_to_w[token_id]\n",
    "                else:\n",
    "                    token_str = vocab_words.i_to_w[token_id]\n",
    "\n",
    "                if token_str!=\"[PAD]\":\n",
    "                    tokens_q.append(token_str)\n",
    "\n",
    "            value = line.split(\"\\t\")[3]\n",
    "            value_tokens = value.split(\",\")\n",
    "            tokens_v = []        \n",
    "            for i in range(len(value_tokens)):\n",
    "                token_id = int(value_tokens[i])\n",
    "                if \"mbert\" in vocab_path.lower():\n",
    "                    token_str = vocab_words[token_id]\n",
    "                elif \"biobert\" in vocab_path.lower():\n",
    "                    token_str = vocab_words[token_id]\n",
    "                elif \"bertbase\" in vocab_path.lower():\n",
    "                    token_str = vocab_words[token_id]\n",
    "                elif \"kobert\" in vocab_path.lower():\n",
    "                    token_str = vocab_words.i_to_w[token_id]\n",
    "                else:\n",
    "                    token_str = vocab_words.i_to_w[token_id]\n",
    "\n",
    "                if token_str!=\"[PAD]\":\n",
    "                    tokens_v.append(token_str)\n",
    "\n",
    "\n",
    "            probs.append(prob)\n",
    "            labels.append(label)\n",
    "            \n",
    "            outtext.append(str(fileinfo[p])+\"\\t\"+str(line.split(\"\\t\")[0])+\"\\t\"+str(line.split(\"\\t\")[1]) +\"\\t\"\n",
    "                           + \" \".join(tokens_q) +\"\\t\"+ \" \".join(tokens_v) )\n",
    "\n",
    "    else:\n",
    "        for p in range(len(lines)):\n",
    "            line = lines[p].strip(\"\\n\")\n",
    "            prob = float(line.split(\"\\t\")[0])\n",
    "            label = int(line.split(\"\\t\")[1])\n",
    "            probs.append(prob)\n",
    "            labels.append(label)\n",
    "        \n",
    "    ######## Evaluation 1. Evaluation viewed as a problem of selecting one of four documents ########\n",
    "    evals = []\n",
    "    \n",
    "    for p in range(0, len(probs), 4):\n",
    "        probs_ = torch.tensor(probs)\n",
    "        labels_ = torch.tensor(labels)\n",
    "\n",
    "        pred_idx = torch.argmax(probs_).detach().item()\n",
    "        label_idx = torch.argmax(labels_).detach().item()\n",
    "        if collect_tokens==True:\n",
    "            for o in range(0, len(outtext)):\n",
    "                if o==pred_idx:\n",
    "                    outtext[o] = \"SELECT\\t\"+outtext[o]\n",
    "                else:\n",
    "                    outtext[o] = \"\\t\"+outtext[o]\n",
    "\n",
    "        is_true_highest1 = 0\n",
    "        if pred_idx==label_idx:\n",
    "            is_true_highest1 = 1\n",
    "    #     print(\"is_true: \", is_true)\n",
    "        evals.append(is_true_highest1)\n",
    "    ########################################################################\n",
    "    \n",
    "    return \"\\n\".join(outtext), evals\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a4c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_char(tag):\n",
    "    result = []\n",
    "    for t in range(len(tag)):\n",
    "        if tag[t]==1:\n",
    "            result.append([\"B-entail\"])\n",
    "        elif tag[t]==0:\n",
    "            result.append([\"B-notent\"])\n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e265e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation 1. Evaluation viewed as a problem of selecting one of four documents\n",
    "for p in range(len(preds_paths)):\n",
    "    exact_match = []\n",
    "\n",
    "    path = preds_paths[p]+\"/*.txt\"\n",
    "    files = glob.glob(path)\n",
    "    files.sort()\n",
    "\n",
    "    vocab_path = vocab_paths[p]\n",
    "\n",
    "    # 예측 결과를 내뱉는 곳\n",
    "    outpath = \"./data/scores/test_acc\"\n",
    "    print(\"outpath: \", outpath)\n",
    "    if not os.path.exists(outpath):\n",
    "        os.makedirs(outpath)\n",
    "    output_eval_preds = os.path.join(outpath, str(preds_paths[p].split(\"/\")[-2]))\n",
    "    print(\"output_eval_preds: \", output_eval_preds)\n",
    "    \n",
    "\n",
    "    evaluations = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for f in range(len(files)):\n",
    "        if f%100==0:\n",
    "            print(str(f)+\"/\"+str(len(files)))\n",
    "\n",
    "        filename = files[f].split(\"/\")[-1]\n",
    "        outtext, evals = prediction(files[f], vocab_path, collect_tokens=False) \n",
    "        \n",
    "        if len(outtext)>0:\n",
    "            file = open(output_eval_preds+\"/\"+filename, \"w\")\n",
    "            file.write(outtext)\n",
    "            file.close()\n",
    "    \n",
    "        evaluations = evaluations + evals\n",
    "    \n",
    "    accuracy = sum(evaluations)/len(evaluations)\n",
    "    output_eval_file = os.path.join(outpath, str(preds_paths[p].split(\"/\")[-2])+\"_eval1.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        writer.write(\"accuracy: \"+str(accuracy))\n",
    "\n",
    "    print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e94de57",
   "metadata": {},
   "source": [
    "## merge eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea5e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "filepaths = glob.glob(\"./data/scores/test_acc/*_eval1.txt\")\n",
    "filepaths.sort()\n",
    "\n",
    "names = []\n",
    "scores_doc_acc = []\n",
    "for p in range(len(filepaths)):\n",
    "    file = open(filepaths[p], \"r\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "\n",
    "    filename = \".\".join(filepaths[p].split(\"/\")[-1].split(\".\")[:-1])\n",
    "    #print(\"filename: \", filename)\n",
    "    names.append(filename)\n",
    "\n",
    "    for l in range(len(lines)):\n",
    "        line = lines[l].strip(\"\\n\")\n",
    "        line = line.replace(\"    \", \"\\t\")\n",
    "        line = line.strip()\n",
    "#         print(\"line:\", line)\n",
    "        line = \"\".join(line.split(\":\")[-1].strip())\n",
    "        scores_doc_acc.append(line)\n",
    "\n",
    "#     break\n",
    "print(len(names))\n",
    "print(len(scores_doc_acc))\n",
    "\n",
    "outtext = [\"name\\tscores_doc_acc\"]\n",
    "for n in range(len(names)):\n",
    "    print(names[n]+\"\\t\"+scores_doc_acc[n])\n",
    "    outtext.append(names[n]+\"\\t\"+scores_doc_acc[n])\n",
    "\n",
    "file = open(\"./data/scores/eval1.txt\", \"w\")\n",
    "file.write(\"\\n\".join(outtext))\n",
    "file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b552ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b603b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b881ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
