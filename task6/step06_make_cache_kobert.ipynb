{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f11145",
   "metadata": {},
   "source": [
    "# Make cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentencepiece\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import six\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0505025",
   "metadata": {},
   "source": [
    "## 4. write_instance_to_example_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a65b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance_ext(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, input_ids_0, input_mask_0, segment_ids_0, \n",
    "                 input_ids_1, input_mask_1, segment_ids_1, label_ids):\n",
    "        \n",
    "        self.input_ids_0 = input_ids_0\n",
    "        self.input_mask_0 = input_mask_0\n",
    "        self.segment_ids_0 = segment_ids_0\n",
    "        \n",
    "        self.input_ids_1 = input_ids_1\n",
    "        self.input_mask_1 = input_mask_1\n",
    "        self.segment_ids_1 = segment_ids_1\n",
    "        \n",
    "        self.label_ids = label_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instance_to_example_files(instances, \n",
    "                                    tokenizer, \n",
    "                                    vocab_words,\n",
    "                                    max_seq_length,\n",
    "                                    max_predictions_per_seq, \n",
    "                                    outfilename):\n",
    "    \n",
    "    features = []\n",
    "    pad_id = vocab_words.w_to_i[\"[PAD]\"]\n",
    "    \n",
    "    for (inst_index, instance) in enumerate(instances):\n",
    "        input_ids_0 = []\n",
    "        for l in range(len(instance.tokens_query)):\n",
    "            tokentmp = instance.tokens_query[l]\n",
    "            input_id = vocab_words.w_to_i[tokentmp]\n",
    "            input_ids_0.append(input_id)\n",
    "\n",
    "        # kobert\n",
    "        input_ids_1 = []\n",
    "        for l in range(len(instance.tokens_doc)):\n",
    "            tokentmp = instance.tokens_doc[l]\n",
    "            input_id = vocab_words.w_to_i[tokentmp]\n",
    "            input_ids_1.append(input_id)\n",
    "        \n",
    "        input_mask_0 = [1] * len(input_ids_0)\n",
    "        input_mask_1 = [1] * len(input_ids_1)\n",
    "        \n",
    "        segment_ids_0 = list(instance.segment_ids_query)\n",
    "        segment_ids_1 = list(instance.segment_ids_doc)\n",
    "        \n",
    "        label_ids = list(instance.label_ids) # label_id\n",
    "        \n",
    "        if segment_ids_0[-1]==0:\n",
    "            seg_id_0=1\n",
    "        else:\n",
    "            seg_id_0=0\n",
    "        \n",
    "        if segment_ids_1[-1]==0:\n",
    "            seg_id_1=1\n",
    "        else:\n",
    "            seg_id_1=0\n",
    "            \n",
    "        assert len(input_ids_0) <= max_seq_length\n",
    "        assert len(input_ids_1) <= max_seq_length\n",
    "\n",
    "        while len(input_ids_0) < max_seq_length:\n",
    "            input_ids_0.append(pad_id)\n",
    "            input_mask_0.append(0)\n",
    "            segment_ids_0.append(seg_id_0)\n",
    "        assert len(input_ids_0) == max_seq_length\n",
    "        assert len(input_mask_0) == max_seq_length\n",
    "        assert len(segment_ids_0) == max_seq_length\n",
    "        \n",
    "        while len(input_ids_1) < max_seq_length:\n",
    "            input_ids_1.append(pad_id)\n",
    "            input_mask_1.append(0)\n",
    "            segment_ids_1.append(seg_id_1)\n",
    "        assert len(input_ids_1) == max_seq_length\n",
    "        assert len(input_mask_1) == max_seq_length\n",
    "        assert len(segment_ids_1) == max_seq_length            \n",
    "            \n",
    "        while len(label_ids) < max_seq_length:\n",
    "            label_ids.append(0)        # label\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            TrainingInstance_ext(\n",
    "                input_ids_0 = input_ids_0,\n",
    "                input_mask_0 = input_mask_0, \n",
    "                segment_ids_0 = segment_ids_0,\n",
    "                \n",
    "                input_ids_1 = input_ids_1,\n",
    "                input_mask_1 = input_mask_1, \n",
    "                segment_ids_1 = segment_ids_1,\n",
    "                \n",
    "                label_ids = label_ids,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    if len(features)!=0:\n",
    "#         print(\"outfilename: \", outfilename)\n",
    "        with open(outfilename, 'wb') as output:\n",
    "            pickle.dump(features, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302adda0",
   "metadata": {},
   "source": [
    "## 2. create_instances_from_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance_ext_tmp(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, tokens_query, segment_ids_query, \n",
    "                 tokens_doc, segment_ids_doc, label_ids): \n",
    "        \n",
    "        self.tokens_query = tokens_query\n",
    "        self.segment_ids_query = segment_ids_query\n",
    "        \n",
    "        self.tokens_doc = tokens_doc\n",
    "        self.segment_ids_doc = segment_ids_doc\n",
    "        \n",
    "        self.label_ids = label_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66bb69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_seq(tokens_a, max_num_tokens):    \n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    return tokens_a[:max_num_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827234c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_contents(input_seq, tokenizer, do_lower_case, vocab_words):\n",
    "    subtokens = []\n",
    "    if do_lower_case==True:\n",
    "        input_seq = input_seq.lower()\n",
    "        input_seq = input_seq.repace(\"[mask]\", \"[MASK]\")\n",
    "        input_seq = input_seq.repace(\"[sep]\", \"[SEP]\")\n",
    "    token_ids_tmp = tokenizer.Encode(input_seq) # kobert\n",
    "    \n",
    "    for t in range(len(token_ids_tmp)):\n",
    "        token = vocab_words.i_to_w[token_ids_tmp[t]]\n",
    "        subtokens.append(token)\n",
    "    \n",
    "    return subtokens\n",
    "\n",
    "\n",
    "def read_documents(targetfile, tokenizer, do_lower_case, vocab_words):\n",
    "    f = open(targetfile, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    questions = []\n",
    "    labels = []\n",
    "    for l in range(len(lines)):\n",
    "        line = lines[l].strip(\"\\n\")\n",
    "        if line==\"\":\n",
    "            continue\n",
    "            \n",
    "        # info line\n",
    "        if \"[START_QUESTION]\"==line:\n",
    "            quest = []\n",
    "            label_q = []\n",
    "        elif \"[END_QUESTION]\"==line:\n",
    "            assert len(quest)==5\n",
    "            assert len(quest)==len(label_q)\n",
    "            \n",
    "            questions.append(quest)\n",
    "            labels.append(label_q)\n",
    "            \n",
    "        elif line!=0:\n",
    "            filename  = line.split(\"\\t\")[0]\n",
    "            doc_order = int(line.split(\"\\t\")[1])\n",
    "            date      = line.split(\"\\t\")[2]\n",
    "            content   = line.split(\"\\t\")[3]\n",
    "            \n",
    "            content_tkzd = tokenize_contents(content, tokenizer, do_lower_case, vocab_words)\n",
    "            \n",
    "            if doc_order==4:\n",
    "                label_tmp = 1\n",
    "            else:\n",
    "                label_tmp = 0\n",
    "                \n",
    "            quest.append(content_tkzd)\n",
    "            label_q.append(label_tmp)\n",
    "        \n",
    "    return questions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49dc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(input_file, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, rng, do_lower_case, vocab_words, mecab_sp_tokenizer, mode):\n",
    "    \n",
    "    instances = []\n",
    "    \n",
    "    truncate_len = int(512-2)\n",
    "    \n",
    "    ########### tokenize all documents ###########\n",
    "    questions, labels = read_documents(input_file, mecab_sp_tokenizer, do_lower_case, vocab_words)\n",
    "    \n",
    "    assert len(questions)==len(labels)\n",
    "        \n",
    "    for q in range(len(questions)):\n",
    "        tokens_query = truncate_seq(questions[q][0], truncate_len)\n",
    "        tokens_query[-1] = \"[SEP]\"\n",
    "\n",
    "        # query\n",
    "        tokens_query = [\"[CLS]\"] + tokens_query\n",
    "        seg_id = 0\n",
    "        segment_ids_query = [seg_id]*len(tokens_query)\n",
    "        \n",
    "        # candidates\n",
    "        for d in range(1, len(questions[q])):\n",
    "            tokens_doc = truncate_seq(questions[q][d], truncate_len)\n",
    "            tokens_doc[-1] = \"[SEP]\"\n",
    "\n",
    "            # [CLS]\n",
    "            tokens_doc = [\"[CLS]\"] + tokens_doc\n",
    "            labels_doc = labels[q][d]\n",
    "            seg_id = 1\n",
    "            segment_ids_doc = [seg_id]*len(tokens_doc)\n",
    "\n",
    "            instance = TrainingInstance_ext_tmp(\n",
    "                tokens_query = tokens_query,\n",
    "                segment_ids_query = segment_ids_query,\n",
    "\n",
    "                tokens_doc = tokens_doc,\n",
    "                segment_ids_doc = segment_ids_doc,\n",
    "\n",
    "                label_ids = [labels_doc], \n",
    "            )\n",
    "\n",
    "            instances.append(instance)\n",
    "    \n",
    "    return instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ebc3f",
   "metadata": {},
   "source": [
    "## 1. Create Training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(input_file, tokenizer, vocab_words, max_seq_length,\n",
    "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                              max_predictions_per_seq, rng, do_lower_case, mode):\n",
    "    \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "    instances = []\n",
    "    #for _ in range(dupe_factor):\n",
    "    instances.extend(\n",
    "        create_instances_from_document(\n",
    "            input_file, max_seq_length, short_seq_prob,\n",
    "            masked_lm_prob, max_predictions_per_seq, rng, do_lower_case, \n",
    "            vocab_words, tokenizer, mode))\n",
    "\n",
    "    #print(\"len(instances): \", len(instances))\n",
    "        \n",
    "    return instances\n",
    "\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529cf20",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    # required\n",
    "    input_files, \n",
    "    vocab_file, \n",
    "    outdir,\n",
    "    mode, \n",
    "    spmodel, \n",
    "    \n",
    "    # optional\n",
    "    do_lower_case=True, \n",
    "    max_seq_length=512, \n",
    "    max_predictions_per_seq = 20, \n",
    "    random_seed=12345, \n",
    "    dupe_factor = 1,\n",
    "    masked_lm_prob = 0.15,\n",
    "    short_seq_prob = 0.1,\n",
    "    ):\n",
    "    \n",
    "    randseed = random.randint(1, 1000)\n",
    "    print(\"randseed: \", randseed)\n",
    "    rng = random.Random(randseed)\n",
    "    \n",
    "    # vocab_words\n",
    "    class Vocab_words(object):\n",
    "        def __init__(self, vocab_file):\n",
    "            self.i_to_w = {}\n",
    "            self.w_to_i = {}\n",
    "            self.getvocab(vocab_file)\n",
    "\n",
    "        def getvocab(self, vocab_file):\n",
    "            f = open(vocab_file, 'r')\n",
    "            lines = f.readlines()\n",
    "            for l in range(len(lines)):\n",
    "                term = lines[l].strip(\"\\n\")\n",
    "                term = convert_to_unicode(term)\n",
    "                self.i_to_w[int(l)] = term\n",
    "                self.w_to_i[term] = int(l)\n",
    "        \n",
    "    vocab_words = Vocab_words(vocab_file)\n",
    "    \n",
    "    # sptokenizer\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.load(spmodel)\n",
    "    \n",
    "    print(\"len(input_files): \", len(input_files))\n",
    "    \n",
    "    for i in range(0, len(input_files), 1):\n",
    "        if i%100==0:\n",
    "            print(str(i)+\"/\"+str(len(input_files)))\n",
    "        instances = create_training_instances(\n",
    "                        input_files[i], tokenizer, vocab_words, max_seq_length,\n",
    "                        dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                        max_predictions_per_seq, rng, do_lower_case, mode)\n",
    "        filename = input_files[i].split(\"/\")[-1]\n",
    "        filename = filename.split(\".\")[0]+\".cache\"\n",
    "        \n",
    "        write_instance_to_example_files(instances=instances, \n",
    "                                    tokenizer=tokenizer, \n",
    "                                    vocab_words=vocab_words,\n",
    "                                    max_seq_length=max_seq_length,\n",
    "                                    max_predictions_per_seq=max_predictions_per_seq, \n",
    "                                    outfilename=outdir+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12926e5f",
   "metadata": {},
   "source": [
    "# Generate Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c556931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f31043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Generating Features...\")\n",
    "\n",
    "data_dirs = [\n",
    "    \"./data/05_samples/train\",\n",
    "    \"./data/05_samples/test\",\n",
    "]\n",
    "\n",
    "output_paths = [\n",
    "    \"kobert\"\n",
    "]\n",
    "\n",
    "vocab_paths = [\n",
    "    \"../otherberts/KoBERT/models\",    \n",
    "]\n",
    "\n",
    "do_lower_cases = [False]\n",
    "\n",
    "modes = [\"train\", \"test\"]\n",
    "\n",
    "for d in range(len(data_dirs)):\n",
    "    mode = modes[d]\n",
    "    for i in range(len(output_paths)):\n",
    "        print(\"data_dirs[d]: \", data_dirs[d])\n",
    "        \n",
    "        out_directory = \"./cache/\"+str(output_paths[i])+\"/\"+str(mode)\n",
    "        print(\"out_directory: \", out_directory)\n",
    "        if not os.path.exists(out_directory):\n",
    "            os.makedirs(out_directory)\n",
    "        \n",
    "        vocab_path=vocab_paths[i]\n",
    "        \n",
    "        input_files = glob.glob(data_dirs[d]+\"/*.txt\")\n",
    "        input_files.sort()\n",
    "        \n",
    "        main(\n",
    "            input_files = input_files, \n",
    "            outdir = out_directory,\n",
    "            mode=mode,\n",
    "            \n",
    "            #sentence piece\n",
    "            #https://skt-lsl-nlp-model.s3.amazonaws.com/KoBERT/tokenizers/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
    "            vocab_file = vocab_paths[i]+'/vocab.txt',\n",
    "            spmodel = vocab_paths[i]+'/spiece.model',\n",
    "\n",
    "            # optional\n",
    "            do_lower_case = do_lower_cases[i],\n",
    "            max_seq_length = 512,\n",
    "        )\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b1d073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
