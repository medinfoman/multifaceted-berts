{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f11145",
   "metadata": {},
   "source": [
    "# Make cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentencepiece\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import six\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "import glob\n",
    "\n",
    "from tokenization import BertTokenizer\n",
    "import tokenization as tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0505025",
   "metadata": {},
   "source": [
    "## 4. write_instance_to_example_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a65b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance_ext(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, input_ids_0, input_mask_0, segment_ids_0, \n",
    "                 input_ids_1, input_mask_1, segment_ids_1, label_ids):\n",
    "        \n",
    "        self.input_ids_0 = input_ids_0\n",
    "        self.input_mask_0 = input_mask_0\n",
    "        self.segment_ids_0 = segment_ids_0\n",
    "        \n",
    "        self.input_ids_1 = input_ids_1\n",
    "        self.input_mask_1 = input_mask_1\n",
    "        self.segment_ids_1 = segment_ids_1\n",
    "        \n",
    "        self.label_ids = label_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instance_to_example_files(instances, \n",
    "                                    tokenizer, \n",
    "                                    vocab_words,\n",
    "                                    max_seq_length,\n",
    "                                    max_predictions_per_seq, \n",
    "                                    outfilename):\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    pad_id = tokenizer.convert_tokens_to_ids([\"[PAD]\"])\n",
    "    pad_id = pad_id[0]\n",
    "    \n",
    "    for (inst_index, instance) in enumerate(instances):\n",
    "        input_ids_0 = tokenizer.convert_tokens_to_ids(instance.tokens_query)\n",
    "        input_ids_1 = tokenizer.convert_tokens_to_ids(instance.tokens_doc)\n",
    "        \n",
    "        input_mask_0 = [1] * len(input_ids_0)\n",
    "        input_mask_1 = [1] * len(input_ids_1)\n",
    "        \n",
    "        segment_ids_0 = list(instance.segment_ids_query)\n",
    "        segment_ids_1 = list(instance.segment_ids_doc)        \n",
    "        label_ids = list(instance.label_ids)\n",
    "        \n",
    "        if segment_ids_0[-1]==0:\n",
    "            seg_id_0=1\n",
    "        else:\n",
    "            seg_id_0=0\n",
    "        \n",
    "        if segment_ids_1[-1]==0:\n",
    "            seg_id_1=1\n",
    "        else:\n",
    "            seg_id_1=0\n",
    "            \n",
    "        assert len(input_ids_0) <= max_seq_length\n",
    "        assert len(input_ids_1) <= max_seq_length\n",
    "\n",
    "        while len(input_ids_0) < max_seq_length:\n",
    "            input_ids_0.append(pad_id)\n",
    "            input_mask_0.append(0)\n",
    "            segment_ids_0.append(seg_id_0)\n",
    "        assert len(input_ids_0) == max_seq_length\n",
    "        assert len(input_mask_0) == max_seq_length\n",
    "        assert len(segment_ids_0) == max_seq_length\n",
    "        \n",
    "        while len(input_ids_1) < max_seq_length:\n",
    "            input_ids_1.append(pad_id)\n",
    "            input_mask_1.append(0)\n",
    "            segment_ids_1.append(seg_id_1)\n",
    "        assert len(input_ids_1) == max_seq_length\n",
    "        assert len(input_mask_1) == max_seq_length\n",
    "        assert len(segment_ids_1) == max_seq_length\n",
    "            \n",
    "            \n",
    "        while len(label_ids) < max_seq_length:\n",
    "            label_ids.append(0)        # label\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            TrainingInstance_ext(\n",
    "                input_ids_0 = input_ids_0,\n",
    "                input_mask_0 = input_mask_0, \n",
    "                segment_ids_0 = segment_ids_0,\n",
    "                \n",
    "                input_ids_1 = input_ids_1,\n",
    "                input_mask_1 = input_mask_1, \n",
    "                segment_ids_1 = segment_ids_1,\n",
    "                \n",
    "                label_ids = label_ids,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    if len(features)!=0:\n",
    "#         print(\"outfilename: \", outfilename)\n",
    "        with open(outfilename, 'wb') as output:\n",
    "            pickle.dump(features, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302adda0",
   "metadata": {},
   "source": [
    "## 2. create_instances_from_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance_ext_tmp(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, tokens_query, segment_ids_query, \n",
    "                 tokens_doc, segment_ids_doc, label_ids): \n",
    "        \n",
    "        self.tokens_query = tokens_query\n",
    "        self.segment_ids_query = segment_ids_query\n",
    "        \n",
    "        self.tokens_doc = tokens_doc\n",
    "        self.segment_ids_doc = segment_ids_doc\n",
    "        \n",
    "        self.label_ids = label_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_seq(tokens_a, max_num_tokens):    \n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    return tokens_a[:max_num_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827234c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_contents(input_seq, tokenizer, do_lower_case):\n",
    "    subtokens = []\n",
    "    \n",
    "    if do_lower_case==True:\n",
    "        input_seq = input_seq.lower()\n",
    "    \n",
    "    tokens = input_seq.split(\" \")\n",
    "    for t in range(len(tokens)):\n",
    "        if tokens[t]==\"[mask]\" or tokens[t]==\"[sep]\" or \\\n",
    "           tokens[t]==\"[MASK]\" or tokens[t]==\"[MASK]\":\n",
    "            subtokens.append(tokens[t].upper()) # [mask] -> [MASK]\n",
    "        \n",
    "        else:\n",
    "            subtokens = subtokens + tokenizer.tokenize(tokens[t])\n",
    "    \n",
    "    return subtokens\n",
    "\n",
    "\n",
    "def read_documents(targetfile, tokenizer, do_lower_case):\n",
    "#     print(\"targetfile: \", targetfile)\n",
    "    f = open(targetfile, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    questions = []\n",
    "    labels = []\n",
    "    for l in range(len(lines)):\n",
    "        line = lines[l].strip(\"\\n\")\n",
    "        if line==\"\":\n",
    "            continue\n",
    "            \n",
    "        # info line\n",
    "        if \"[START_QUESTION]\"==line:\n",
    "            quest = []\n",
    "            label_q = []\n",
    "        elif \"[END_QUESTION]\"==line:\n",
    "            assert len(quest)==5\n",
    "            assert len(quest)==len(label_q)\n",
    "            \n",
    "            questions.append(quest)\n",
    "            labels.append(label_q)\n",
    "            \n",
    "        elif line!=0:\n",
    "            filename  = line.split(\"\\t\")[0]\n",
    "            doc_order = int(line.split(\"\\t\")[1])\n",
    "            date      = line.split(\"\\t\")[2]\n",
    "            content   = line.split(\"\\t\")[3]\n",
    "            \n",
    "            content_tkzd = tokenize_contents(content, tokenizer, do_lower_case)\n",
    "            \n",
    "            # 마지막 방문문서를 찾는 것이므로 4개 순서중 가장 큰 순서인 4를 선택\n",
    "            if doc_order==4:\n",
    "                label_tmp = 1\n",
    "            else:\n",
    "                label_tmp = 0\n",
    "                \n",
    "            quest.append(content_tkzd)\n",
    "            label_q.append(label_tmp)\n",
    "        \n",
    "    return questions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49dc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(input_file, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, rng, do_lower_case, vocab_words, mecab_sp_tokenizer, mode):\n",
    "    \n",
    "    instances = []\n",
    "    \n",
    "    truncate_len = int(512-2)\n",
    "    \n",
    "    ########### tokenize all documents ###########\n",
    "    questions, labels = read_documents(input_file, mecab_sp_tokenizer, do_lower_case)\n",
    "    \n",
    "    assert len(questions)==len(labels)\n",
    "        \n",
    "    for q in range(len(questions)):\n",
    "        tokens_query = truncate_seq(questions[q][0], truncate_len)\n",
    "        tokens_query[-1] = \"[SEP]\"\n",
    "\n",
    "        # query\n",
    "        tokens_query = [\"[CLS]\"] + tokens_query\n",
    "        seg_id = 0\n",
    "        segment_ids_query = [seg_id]*len(tokens_query)\n",
    "        \n",
    "        # candidates\n",
    "        # 쿼리문서와 그외 문서들 pairing\n",
    "        for d in range(1, len(questions[q])):\n",
    "            tokens_doc = truncate_seq(questions[q][d], truncate_len)\n",
    "            tokens_doc[-1] = \"[SEP]\"\n",
    "\n",
    "            # [CLS]\n",
    "            tokens_doc = [\"[CLS]\"] + tokens_doc\n",
    "            labels_doc = labels[q][d]\n",
    "            seg_id = 1\n",
    "            segment_ids_doc = [seg_id]*len(tokens_doc)\n",
    "\n",
    "            instance = TrainingInstance_ext_tmp(\n",
    "                tokens_query = tokens_query,\n",
    "                segment_ids_query = segment_ids_query,\n",
    "\n",
    "                tokens_doc = tokens_doc,\n",
    "                segment_ids_doc = segment_ids_doc,\n",
    "\n",
    "                label_ids = [labels_doc], \n",
    "            )\n",
    "\n",
    "            instances.append(instance)\n",
    "    \n",
    "    return instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ebc3f",
   "metadata": {},
   "source": [
    "## 1. Create Training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(input_file, tokenizer, vocab_words, max_seq_length,\n",
    "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                              max_predictions_per_seq, rng, do_lower_case, mode):\n",
    "    \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "    instances = []\n",
    "    #for _ in range(dupe_factor):\n",
    "    instances.extend(\n",
    "        create_instances_from_document(\n",
    "            input_file, max_seq_length, short_seq_prob,\n",
    "            masked_lm_prob, max_predictions_per_seq, rng, do_lower_case, \n",
    "            vocab_words, tokenizer, mode))\n",
    "        \n",
    "    return instances\n",
    "\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529cf20",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    # required\n",
    "    input_files, \n",
    "    vocab_file, \n",
    "    outdir,\n",
    "    mode, \n",
    "    \n",
    "    # optional\n",
    "    do_lower_case=True, \n",
    "    max_seq_length=512, \n",
    "    max_predictions_per_seq = 20, \n",
    "    random_seed=12345, \n",
    "    dupe_factor = 1,\n",
    "    masked_lm_prob = 0.15,\n",
    "    short_seq_prob = 0.1,\n",
    "    ):\n",
    "    \n",
    "    print(\"vocab_file: \", vocab_file)\n",
    "    tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case, max_len=max_seq_length)\n",
    "    vocab_words = list(tokenizer.vocab.keys())\n",
    "    print(\"len(vocab_words): \", len(vocab_words))\n",
    "    print(\"max_predictions_per_seq: \", max_predictions_per_seq)\n",
    "    \n",
    "    randseed = random.randint(1, 1000)\n",
    "    print(\"randseed: \", randseed)\n",
    "    rng = random.Random(randseed)\n",
    "    \n",
    "    print(\"len(input_files): \", len(input_files))\n",
    "    \n",
    "    for i in range(0, len(input_files), 1):\n",
    "        if i%100==0:\n",
    "            print(str(i)+\"/\"+str(len(input_files)))\n",
    "        \n",
    "        instances = create_training_instances(\n",
    "                        input_files[i], tokenizer, vocab_words, max_seq_length,\n",
    "                        dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                        max_predictions_per_seq, rng, do_lower_case, mode)\n",
    "        \n",
    "        filename = input_files[i].split(\"/\")[-1]\n",
    "        filename = filename.split(\".\")[0]+\".cache\"\n",
    "        \n",
    "        write_instance_to_example_files(instances=instances, \n",
    "                                    tokenizer=tokenizer, \n",
    "                                    vocab_words=vocab_words,\n",
    "                                    max_seq_length=max_seq_length,\n",
    "                                    max_predictions_per_seq=max_predictions_per_seq, \n",
    "                                    outfilename=outdir+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12926e5f",
   "metadata": {},
   "source": [
    "# Generate Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f31043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"Generating Features...\")\n",
    "\n",
    "data_dirs = [\n",
    "    \"./data/05_samples/train\",\n",
    "    \"./data/05_samples/test\",\n",
    "]\n",
    "\n",
    "output_paths = [\n",
    "        \"bertbase_cased\",\n",
    "        \"biobert\",\n",
    "        \"mbert_cased\",   \n",
    "]\n",
    "\n",
    "vocab_paths = [\n",
    "        \"../otherberts/bertbase_cased\",\n",
    "        \"../otherberts/bioBERT/biobert_v1.1_pubmed\",\n",
    "        \"../otherberts/mbert_cased\"\n",
    "]\n",
    "\n",
    "lowercase = [False, False, False]\n",
    "modes = [\"train\", \"test\"]\n",
    "\n",
    "for d in range(len(data_dirs)):\n",
    "    mode = modes[d]\n",
    "\n",
    "    for i in range(len(output_paths)):\n",
    "        print(\"data_dirs[d]: \", data_dirs[d])\n",
    "        \n",
    "        out_directory = \"./cache/\"+str(output_paths[i])+\"/\"+str(mode)\n",
    "        print(\"out_directory: \", out_directory)\n",
    "        if not os.path.exists(out_directory):\n",
    "            os.makedirs(out_directory)\n",
    "        \n",
    "        vocab_path=vocab_paths[i]\n",
    "        \n",
    "        input_files = glob.glob(data_dirs[d]+\"/*.txt\")\n",
    "        input_files.sort()\n",
    "        \n",
    "        main(\n",
    "            input_files = input_files, \n",
    "            outdir = out_directory,\n",
    "            vocab_file = vocab_paths[i]+'/vocab.txt',\n",
    "            mode = mode,\n",
    "            \n",
    "            # optional\n",
    "            do_lower_case = lowercase[i],\n",
    "            max_seq_length = 512,\n",
    "        )\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b1d073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
