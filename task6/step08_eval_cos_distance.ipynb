{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df74685f",
   "metadata": {},
   "source": [
    "# sts eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abad38af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkm/anaconda3/envs/multibert/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "from transformers import (WEIGHTS_NAME, AdamW, BertConfig,\n",
    "                          BertForTokenClassification, BertTokenizer,\n",
    "                          get_linear_schedule_with_warmup, \n",
    "                          BertPreTrainedModel, BertModel)\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import sequence_labeling\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "import codecs\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "    \n",
    "import glob\n",
    "import time\n",
    "\n",
    "import easydict\n",
    "\n",
    "import six\n",
    "import tqdm\n",
    "from tqdm import tqdm, trange\n",
    "import collections\n",
    "\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "import shutil\n",
    "\n",
    "import math\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gzip\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5bce7e",
   "metadata": {},
   "source": [
    "## data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e9522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance_ext(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, input_ids_0, input_mask_0, segment_ids_0, \n",
    "                 input_ids_1, input_mask_1, segment_ids_1, label_ids):\n",
    "        \n",
    "        self.input_ids_0 = input_ids_0\n",
    "        self.input_mask_0 = input_mask_0\n",
    "        self.segment_ids_0 = segment_ids_0\n",
    "        \n",
    "        self.input_ids_1 = input_ids_1\n",
    "        self.input_mask_1 = input_mask_1\n",
    "        self.segment_ids_1 = segment_ids_1\n",
    "        \n",
    "        self.label_ids = label_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c73862",
   "metadata": {},
   "source": [
    "## BERT with cos distance class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERT(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        #word_embedding_model = models.Transformer(model_name)\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        \n",
    "        self.ffnn_qury_0 = nn.Linear(config.hidden_size, config.hidden_size//2)\n",
    "        self.ffnn_qury_1 = nn.Linear(config.hidden_size//2, config.hidden_size)\n",
    "        \n",
    "        self.ffnn_cand_0 = nn.Linear(config.hidden_size, config.hidden_size//2)\n",
    "        self.ffnn_cand_1 = nn.Linear(config.hidden_size//2, config.hidden_size)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, sentence_features, label_ids):\n",
    "        s = 0\n",
    "        outputs = self.bert(\n",
    "            input_ids = sentence_features[s]['input_ids'],\n",
    "            attention_mask=sentence_features[s]['attention_mask'],\n",
    "            token_type_ids=sentence_features[s]['token_type_ids'],\n",
    "            position_ids=None, head_mask=None, inputs_embeds=None, output_attentions=None, output_hidden_states=None,return_dict=None,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        # batchsize x (1), dimension\n",
    "        cls_embeds_q = torch.zeros(sequence_output.size()[0], sequence_output.size()[2])\n",
    "        cls_embeds_q = cls_embeds_q.to(self.device)\n",
    "        for bat in range(sequence_output.size()[0]):\n",
    "            cls_embeds_q[bat] = sequence_output[bat][0]\n",
    "        \n",
    "        # ffnn for query\n",
    "        m2d_h1_q = self.ffnn_qury_0(cls_embeds_q)\n",
    "        m2d_h1_q = self.act1(m2d_h1_q) # relu (remove negatives)\n",
    "        m2d_h2_q = self.ffnn_qury_1(m2d_h1_q)\n",
    "        m2d_h2_q = self.act1(m2d_h2_q) # relu (remove negatives)\n",
    "        m2d_h2_q = m2d_h2_q / m2d_h2_q.norm(dim=-1)[:, None].to(self.device) # norm (for cosine sim)\n",
    "        \n",
    "        \n",
    "        # loop 1: sentence_features cands (as much batsize)\n",
    "        s = 1\n",
    "        outputs = self.bert(\n",
    "            input_ids = sentence_features[s]['input_ids'], \n",
    "            attention_mask=sentence_features[s]['attention_mask'],\n",
    "            token_type_ids=sentence_features[s]['token_type_ids'],\n",
    "            position_ids=None, head_mask=None, inputs_embeds=None, output_attentions=None, output_hidden_states=None,return_dict=None,\n",
    "        )\n",
    "        sequence_output = outputs[0]\n",
    "        # batchsize x (1), dimension\n",
    "        cls_embeds_c = torch.zeros(sequence_output.size()[0], sequence_output.size()[2])\n",
    "        cls_embeds_c = cls_embeds_c.to(self.device)\n",
    "        for bat in range(sequence_output.size()[0]):\n",
    "            cls_embeds_c[bat] = sequence_output[bat][0]\n",
    "        \n",
    "        # ffnn for candidate\n",
    "        m2d_h1_c = self.ffnn_cand_0(cls_embeds_c)\n",
    "        m2d_h1_c = self.act1(m2d_h1_c) # relu (remove negatives)\n",
    "        m2d_h2_c = self.ffnn_cand_1(m2d_h1_c)\n",
    "        m2d_h2_c = self.act1(m2d_h2_c) # relu (remove negatives)\n",
    "        m2d_h2_c = m2d_h2_c / m2d_h2_c.norm(dim=-1)[:, None].to(self.device) # norm (for cosine sim)\n",
    "        cos_sim_fn = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        cos_distance = 1-cos_sim_fn(m2d_h2_q, m2d_h2_c)\n",
    "        cos_distance = cos_distance.to(self.device)\n",
    "        labels = torch.zeros(sequence_output.size()[0])\n",
    "        labels = labels.to(self.device)\n",
    "        for bat in range(len(label_ids)):\n",
    "            labels[bat] = label_ids[bat][0]\n",
    "        loss = self.loss(labels, cos_distance)\n",
    "        return loss, cos_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f2d3e9",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46534df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args):\n",
    "    ############################################################################\n",
    "    ##              Multi-GPUs, Distributed settings\n",
    "    ############################################################################ \n",
    "    if args.server_ip and args.server_port:\n",
    "    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "        import ptvsd\n",
    "        print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "    \n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "    \n",
    "    print(\"n_gpu: \", n_gpu)\n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    ##              batch size, gradient_accumulation_steps\n",
    "    ############################################################################ \n",
    "    if args.gradient_accumulation_steps < 1:\n",
    "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                            args.gradient_accumulation_steps))\n",
    "    args.eval_batch_size = args.eval_batch_size // args.gradient_accumulation_steps\n",
    "    print(\"args.eval_batch_size: \", args.eval_batch_size)\n",
    "    \n",
    "    ###############################################################\n",
    "    #                Functions for Dataload (load cache data)\n",
    "    ###############################################################\n",
    "    def get_file_arrays(data_path):\n",
    "        data_path = data_path.split(\",\")\n",
    "        print(\"data_path: \", data_path)\n",
    "        \n",
    "        cache_files_all = []\n",
    "        for i in range(len(data_path)):\n",
    "            path = data_path[i].strip()\n",
    "#             print(\"data_path[i]: \", path)\n",
    "            files = glob.glob(path)\n",
    "#             print(\"len(files): \", len(files))\n",
    "            cache_files_all = cache_files_all + files\n",
    "        cache_files_all.sort()\n",
    "        \n",
    "        print(\"Got \" + str(len(cache_files_all)) + \" cache_files\")\n",
    "        \n",
    "        return cache_files_all\n",
    "    \n",
    "    def read_files(cache_files_pieces):\n",
    "        eval_features = []\n",
    "        for c in range(len(cache_files_pieces)):\n",
    "            with open(cache_files_pieces[c], 'rb') as input:\n",
    "                eval_features = eval_features + pickle.load(input)\n",
    "            print(\"cache_files_pieces[c]: \", cache_files_pieces[c])\n",
    "        print(\"len(eval_features): \", len(eval_features))        \n",
    "        \n",
    "        num_eval_examples = len(eval_features)\n",
    "        \n",
    "        all_input_ids_0 = torch.tensor([f.input_ids_0 for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask_0 = torch.tensor([f.input_mask_0 for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids_0 = torch.tensor([f.segment_ids_0 for f in eval_features], dtype=torch.long)\n",
    "        \n",
    "        all_input_ids_1 = torch.tensor([f.input_ids_1 for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask_1 = torch.tensor([f.input_mask_1 for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids_1 = torch.tensor([f.segment_ids_1 for f in eval_features], dtype=torch.long)\n",
    "        \n",
    "        all_label_ids = torch.tensor([f.label_ids for f in eval_features], dtype=torch.long)\n",
    "        \n",
    "        # squeeze\n",
    "        if all_input_ids_0.size()[0]>1:\n",
    "            all_input_ids_0 = torch.squeeze(all_input_ids_0)\n",
    "            all_input_mask_0 = torch.squeeze(all_input_mask_0)\n",
    "            all_segment_ids_0 = torch.squeeze(all_segment_ids_0)\n",
    "        \n",
    "        if all_input_ids_1.size()[0]>1:\n",
    "            all_input_ids_1 = torch.squeeze(all_input_ids_1)\n",
    "            all_input_mask_1 = torch.squeeze(all_input_mask_1)\n",
    "            all_segment_ids_1 = torch.squeeze(all_segment_ids_1)\n",
    "        \n",
    "        if all_label_ids.size()[0]>1:\n",
    "            all_label_ids = torch.squeeze(all_label_ids)\n",
    "        \n",
    "        eval_data = TensorDataset(\n",
    "            all_input_ids_0, all_input_mask_0, all_segment_ids_0, \n",
    "            all_input_ids_1, all_input_mask_1, all_segment_ids_1, \n",
    "            all_label_ids            \n",
    "        )\n",
    "\n",
    "        if args.local_rank == -1:\n",
    "            eval_sampler = SequentialSampler(eval_data)\n",
    "        else:\n",
    "            eval_sampler = DistributedSampler(eval_data)\n",
    "        \n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "        \n",
    "        return eval_dataloader, num_eval_examples\n",
    "    \n",
    "    def read_file(cache_file):\n",
    "        eval_features = []\n",
    "        with open(cache_file, 'rb') as input:\n",
    "            eval_features = eval_features + pickle.load(input)\n",
    "\n",
    "        num_eval_examples = len(eval_features)\n",
    "        \n",
    "        all_input_ids_0 = torch.tensor([f.input_ids_0 for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask_0 = torch.tensor([f.input_mask_0 for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids_0 = torch.tensor([f.segment_ids_0 for f in eval_features], dtype=torch.long)\n",
    "        \n",
    "        all_input_ids_1 = torch.tensor([f.input_ids_1 for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask_1 = torch.tensor([f.input_mask_1 for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids_1 = torch.tensor([f.segment_ids_1 for f in eval_features], dtype=torch.long)\n",
    "        \n",
    "        all_label_ids = torch.tensor([f.label_ids for f in eval_features], dtype=torch.long)\n",
    "        \n",
    "        # squeeze\n",
    "        if all_input_ids_0.size()[0]>1:\n",
    "            all_input_ids_0 = torch.squeeze(all_input_ids_0)\n",
    "            all_input_mask_0 = torch.squeeze(all_input_mask_0)\n",
    "            all_segment_ids_0 = torch.squeeze(all_segment_ids_0)\n",
    "        \n",
    "        if all_input_ids_1.size()[0]>1:\n",
    "            all_input_ids_1 = torch.squeeze(all_input_ids_1)\n",
    "            all_input_mask_1 = torch.squeeze(all_input_mask_1)\n",
    "            all_segment_ids_1 = torch.squeeze(all_segment_ids_1)\n",
    "        \n",
    "        if all_label_ids.size()[0]>1:\n",
    "            all_label_ids = torch.squeeze(all_label_ids)\n",
    "            \n",
    "        eval_data = TensorDataset(\n",
    "            all_input_ids_0, all_input_mask_0, all_segment_ids_0, \n",
    "            all_input_ids_1, all_input_mask_1, all_segment_ids_1, \n",
    "            all_label_ids            \n",
    "        )\n",
    "        \n",
    "        if args.local_rank == -1:\n",
    "            eval_sampler = SequentialSampler(eval_data)\n",
    "        else:\n",
    "            eval_sampler = DistributedSampler(eval_data)\n",
    "        \n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "        \n",
    "        return eval_dataloader, num_eval_examples\n",
    "    \n",
    "    ############################################################################\n",
    "    ##                    Load Training data\n",
    "    ############################################################################\n",
    "    label_list = [\"0\", \"1\"]\n",
    "    num_labels = len(label_list)\n",
    "    \n",
    "    \n",
    "    ###############################################################\n",
    "    #            Dataload (load cache data) - Train data\n",
    "    ###############################################################\n",
    "    cache_files = get_file_arrays(args.data_dir)\n",
    "    print(\"cache_files: \", cache_files)\n",
    "    \n",
    "\n",
    "    ############################################################################\n",
    "    ##                      Prepare model\n",
    "    ############################################################################\n",
    "    print(\"loading weights from checkpoint (\", args.checkpoint, \")\")\n",
    "    config = BertConfig.from_pretrained(args.checkpoint, num_labels=num_labels)\n",
    "    model = SBERT.from_pretrained(args.checkpoint,\n",
    "              from_tf = False,\n",
    "              config = config)\n",
    "    print(\"loaded weights from checkpoint (\", args.checkpoint, \")\")\n",
    "    \n",
    "    if args.local_rank == 0:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "    print(\"device: \", device)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    ############################################################################\n",
    "    ##              save_model\n",
    "    ############################################################################\n",
    "    def save_model(args, global_step):\n",
    "        if not os.path.exists(args.output_dir+\"/\"+str(global_step)):\n",
    "            os.makedirs(args.output_dir+\"/\"+str(global_step))\n",
    "\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "        model_to_save.save_pretrained(args.output_dir+\"/\"+str(global_step))\n",
    "\n",
    "    def make_sentence_feature(input_ids, input_mask, segment_ids):\n",
    "        sentencefeature = {}\n",
    "        sentencefeature['input_ids'] = input_ids\n",
    "        sentencefeature['attention_mask'] = input_mask\n",
    "        sentencefeature['token_type_ids'] = segment_ids\n",
    "        return sentencefeature\n",
    "    \n",
    "    ############################################################################\n",
    "    ##              Start training\n",
    "    ############################################################################\n",
    "    global_step = -1\n",
    "    nb_tr_steps = 0\n",
    "    tr_loss = 0\n",
    "    \n",
    "    args.save_file_limit\n",
    "    min_train_loss = 1000000\n",
    "    save_checkpoints = []\n",
    "    save_loss = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for c in range(len(cache_files)):\n",
    "        filename = cache_files[c].split(\"/\")[-1].split(\".\")[0]\n",
    "        eval_dataloader, num_eval_examples = read_file(cache_files[c])\n",
    "        outtext = []\n",
    "        \n",
    "        for step, batch in enumerate(tqdm(eval_dataloader, desc=\"Iteration\")):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids_0, input_mask_0, segment_ids_0, \\\n",
    "            input_ids_1, input_mask_1, segment_ids_1, label_ids = batch\n",
    "            \n",
    "            sentence_features = []\n",
    "            sentence_features.append(make_sentence_feature(input_ids_0, input_mask_0, segment_ids_0))\n",
    "            sentence_features.append(make_sentence_feature(input_ids_1, input_mask_1, segment_ids_1))            \n",
    "            \n",
    "            loss, props = model(sentence_features, label_ids)\n",
    "            \n",
    "            input_ids_0 = input_ids_0.detach().cpu()\n",
    "            input_ids_1 = input_ids_1.detach().cpu()\n",
    "            label_ids   = label_ids.detach().cpu()\n",
    "            loss        = loss.detach().cpu()\n",
    "            props       = props.detach().cpu()\n",
    "            \n",
    "            assert len(input_ids_0)==len(input_ids_1)\n",
    "            for b in range(len(input_ids_0)):\n",
    "                cosim = str(props[b].item())\n",
    "                label = str(label_ids[b][0].item()) # 0-th: gold standard\n",
    "                text0 = \",\".join([str(idx) for idx in input_ids_0[b].tolist()])\n",
    "                text1 = \",\".join([str(idx) for idx in input_ids_1[b].tolist()])\n",
    "                outtext.append(cosim+\"\\t\"+label+\"\\t\"+text0+\"\\t\"+text1)\n",
    "            \n",
    "            \n",
    "        test_mode = args.data_dir.split(\"/\")[-2]\n",
    "        if not os.path.exists(args.output_dir+\"/\"+str(test_mode)+\"_pred\"):\n",
    "            os.makedirs(args.output_dir+\"/\"+str(test_mode)+\"_pred\")\n",
    "\n",
    "        file = open(args.output_dir+\"/\"+str(test_mode)+\"_pred\"+\"/\"+str(filename)+\".txt\", \"w\")\n",
    "        file.write(\"\\n\".join(outtext))\n",
    "        file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473aac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    print(\"args.output_dir: \", args.output_dir)\n",
    "    \n",
    "    # check directories\n",
    "    if not args.do_train and not args.do_eval:\n",
    "        raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n",
    "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "            \n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    \n",
    "    if args.do_train:\n",
    "        train(args)\n",
    "    \n",
    "    if args.do_eval:\n",
    "        evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886954d0",
   "metadata": {},
   "source": [
    "# finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203319c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dirs = [\n",
    "    './cache/bertbase_cased/test/*.cache',\n",
    "    './cache/mbert_cased/test/*.cache',\n",
    "    './cache/biobert/test/*.cache',\n",
    "    './cache/kobert/test/*.cache',\n",
    "]\n",
    "\n",
    "checkpoints = [\n",
    "    \"./finetuned/ver9.1.4_521121_epoch2/4597\",\n",
    "    \"./finetuned/ver8.1.4_1142642_epoch2/4597\",\n",
    "    './finetuned/ver11.1.4_521079_epoch2/4597',\n",
    "    './finetuned/ver12.1.4_407013_epoch2/4597',\n",
    "]\n",
    "\n",
    "\n",
    "assert len(data_dirs)==len(checkpoints)\n",
    "\n",
    "for d in range(len(data_dirs)):\n",
    "    otherbert = data_dirs[d].split(\"/\")[2]\n",
    "    output_dir = \"./data/08_preds/\"+checkpoints[d].split(\"/\")[-2]\n",
    "                                   \n",
    "    print(\"output_dir: \", output_dir)\n",
    "                                   \n",
    "    args = easydict.EasyDict({\n",
    "        'data_dir':data_dirs[d],\n",
    "        'eval_batch_size':4, \n",
    "        'gradient_accumulation_steps':1,\n",
    "        'save_globalstep':1,\n",
    "        'checkpoint':checkpoints[d],\n",
    "        'output_dir':output_dir,\n",
    "        'save_step':1,\n",
    "        'num_files_on_memory':1,\n",
    "        \n",
    "        'do_train':False,\n",
    "        'do_eval':True,\n",
    "        'do_lower_case':True,\n",
    "        'fp16_allreduce':False,\n",
    "        'seed':42,\n",
    "        'no_cuda':False,\n",
    "        'use_adasum':False,\n",
    "        'learning_rate':3e-05,\n",
    "        'warmup_proportion':1.00,\n",
    "        'num_train_epochs':2,\n",
    "        'save_file_limit':1,\n",
    "        \n",
    "        'weight_decay':0.01,\n",
    "        'adam_epsilon':1e-8,\n",
    "        'gradient_predivide_factor':1.0,\n",
    "        'fp16':False,\n",
    "        'max_grad_norm':1.0,\n",
    "        'local_rank':-1,\n",
    "        'fp16_opt_level':'O1',\n",
    "        'loss_scale':0,\n",
    "        \n",
    "        'server_ip':None,\n",
    "        'server_port':None,\n",
    "        'cuda':True,\n",
    "    })\n",
    "    \n",
    "    \n",
    "    # configs\n",
    "    if \"multilingualbert\" == str(data_dirs[d].split(\"/\")[2]):\n",
    "        print(\"multilingualbert configs\")\n",
    "        # multilingual bert config\n",
    "        args[\"attention_probs_dropout_prob\"] = 0.1\n",
    "        args[\"directionality\"] = \"bidi\"\n",
    "        args[\"hidden_act\"] = \"gelu\"\n",
    "        args[\"hidden_dropout_prob\"] = 0.1\n",
    "        args[\"hidden_size\"] = 768\n",
    "        args[\"initializer_range\"] = 0.02\n",
    "        args[\"intermediate_size\"] = 3072\n",
    "        args[\"max_position_embeddings\"] = 512\n",
    "        args[\"num_attention_heads\"] =  12\n",
    "        args[\"num_hidden_layers\"] = 12\n",
    "        args[\"pooler_fc_size\"] = 768\n",
    "        args[\"pooler_num_attention_heads\"] = 12\n",
    "        args[\"pooler_num_fc_layers\"] = 3\n",
    "        args[\"pooler_size_per_head\"] = 128\n",
    "        args[\"pooler_type\"] = \"first_token_transform\"\n",
    "        args[\"type_vocab_size\"] = 2\n",
    "        args[\"vocab_size\"] = 119547\n",
    "        args[\"cls_id\"] = 101\n",
    "        args[\"sep_id\"] = 102\n",
    "    # configs\n",
    "    elif \"biobert\" == str(data_dirs[d].split(\"/\")[2]):\n",
    "        # BioBERT\n",
    "        print(\"biobert configs\")\n",
    "        args[\"attention_probs_dropout_prob\"]= 0.1\n",
    "        args[\"hidden_act\"] = \"gelu\"\n",
    "        args[\"hidden_dropout_prob\"] = 0.1\n",
    "        args[\"hidden_size\"] = 768\n",
    "        args[\"initializer_range\"] = 0.02\n",
    "        args[\"intermediate_size\"] = 3072\n",
    "        args[\"max_position_embeddings\"] = 512\n",
    "        args[\"num_attention_heads\"] = 12\n",
    "        args[\"num_hidden_layers\"] = 12\n",
    "        args[\"type_vocab_size\"] = 2\n",
    "        args[\"vocab_size\"] = 28996\n",
    "        args[\"cls_id\"] = 101\n",
    "        args[\"sep_id\"] = 102\n",
    "        \n",
    "    elif \"kobert\" == str(data_dirs[d].split(\"/\")[2]):\n",
    "        print(\"kobert configs\")\n",
    "        args[\"attention_probs_dropout_prob\"]= 0.1\n",
    "        args[\"gradient_checkpointing\"]= False\n",
    "        args[\"hidden_act\"]= \"gelu\"\n",
    "        args[\"hidden_dropout_prob\"]= 0.1\n",
    "        args[\"hidden_size\"]= 768\n",
    "        args[\"initializer_range\"]= 0.02\n",
    "        args[\"intermediate_size\"]= 3072\n",
    "        args[\"layer_norm_eps\"]= 1e-12\n",
    "        args[\"max_position_embeddings\"]= 512\n",
    "        args[\"model_type\"]= \"bert\"\n",
    "        args[\"num_attention_heads\"]= 12\n",
    "        args[\"num_hidden_layers\"]= 12\n",
    "        args[\"pad_token_id\"]= 1\n",
    "        args[\"type_vocab_size\"]= 2\n",
    "        args[\"vocab_size\"]= 8002\n",
    "        args[\"author\"]= \"Heewon Jeon(madjakarta@gmail.com)\"\n",
    "        args[\"kobert_version\"]= 1.0\n",
    "        args[\"cls_id\"] = 2\n",
    "        args[\"sep_id\"] = 3\n",
    "        \n",
    "    elif \"bertbase\" == str(data_dirs[d].split(\"/\")[2]):\n",
    "        args['vocab_size']=30522 # bert-base-uncased\n",
    "        args['hidden_size']=768\n",
    "        args['num_hidden_layers']=12\n",
    "        args['num_attention_heads']=12\n",
    "        args['hidden_act']='gelu'\n",
    "        args['intermediate_size']=3072\n",
    "        args['hidden_dropout_prob']=0.1\n",
    "        args['attention_probs_dropout_prob']=0.1\n",
    "        args['max_position_embeddings']=512\n",
    "        args['type_vocab_size']=2\n",
    "        args['initializer_range']=0.02\n",
    "        args['layer_norm_eps']=1e-12\n",
    "        args['gradient_checkpointing']=None\n",
    "        args['position_embedding_type']=None\n",
    "        args['use_cache']=None\n",
    "        args['classifier_dropout']=None\n",
    "        args[\"cls_id\"] = 101\n",
    "        args[\"sep_id\"] = 102\n",
    "        \n",
    "    \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0896ab6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
