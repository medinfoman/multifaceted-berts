{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f11145",
   "metadata": {},
   "source": [
    "# Task 3. predict department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentencepiece\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import six\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0505025",
   "metadata": {},
   "source": [
    "## 4. write_instance_to_example_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a65b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, labels):\n",
    "        self.input_ids=input_ids\n",
    "        self.input_mask=input_mask\n",
    "        self.segment_ids=segment_ids\n",
    "        self.labels=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance 를 exaple cinfe로 작성\n",
    "def write_instance_to_example_files(instances, \n",
    "                                    mecab_sp_tokenizer, \n",
    "                                    vocab_words,\n",
    "                                    max_seq_length,\n",
    "                                    outfilename):\n",
    "    features = []\n",
    "    \n",
    "    pad_id = vocab_words.w_to_i[\"[PAD]\"]\n",
    "    \n",
    "    for (inst_index, instance) in enumerate(instances):\n",
    "        input_ids = []\n",
    "        for l in range(len(instance.tokens)):\n",
    "            tokentmp = instance.tokens[l]\n",
    "            input_id = vocab_words.w_to_i[tokentmp]\n",
    "            input_ids.append(input_id)\n",
    "                    \n",
    "        input_mask = [1] * len(input_ids)\n",
    "        segment_ids = list(instance.segment_ids)\n",
    "        assert len(input_ids) <= max_seq_length\n",
    "\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(pad_id)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        \n",
    "        labels = [-1]*max_seq_length\n",
    "        labels[0] = instance.labels\n",
    "        \n",
    "        features.append(\n",
    "            TrainingInstance(\n",
    "                input_ids=input_ids, \n",
    "                input_mask=input_mask, \n",
    "                segment_ids=segment_ids, \n",
    "                labels=labels\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    if len(features)!=0:\n",
    "#         print(\"outfilename: \", outfilename)\n",
    "        with open(outfilename, 'wb') as output:\n",
    "            pickle.dump(features, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "\n",
    "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "        assert len(trunc_tokens) >= 1\n",
    "\n",
    "        # We want to sometimes truncate from the front and sometimes from the\n",
    "        # back to add more randomness and avoid biases.\n",
    "        if rng.random() < 0.5:\n",
    "            del trunc_tokens[0]\n",
    "        else:\n",
    "            trunc_tokens.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba7df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_seq_single(tokens_a, max_num_tokens, rng):\n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "            \n",
    "        trunc_tokens = tokens_a\n",
    "        assert len(trunc_tokens) >= 1\n",
    "\n",
    "        # We want to sometimes truncate from the front and sometimes from the\n",
    "        # back to add more randomness and avoid biases.\n",
    "        if rng.random() < 0.5:\n",
    "            del trunc_tokens[0]\n",
    "        else:\n",
    "            trunc_tokens.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302adda0",
   "metadata": {},
   "source": [
    "## 2. create_instances_from_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance_tmp(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, tokens, segment_ids, labels):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.labels=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49dc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(\n",
    "    samples, max_seq_length, rng, vocab_words, mecab_sp_tokenizer):\n",
    "    \n",
    "    \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
    "    instances = []\n",
    "    \n",
    "    max_num_tokens = max_seq_length-2 # [CLS], [SEP]\n",
    "    \n",
    "    for i in range(len(samples)):\n",
    "        record = samples[i]\n",
    "        \n",
    "        labels = record[1]\n",
    "        labels = int(labels)\n",
    "        \n",
    "        tokens_a = record[2]\n",
    "        \n",
    "        truncate_seq_single(tokens_a, max_num_tokens, rng)\n",
    "        \n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "        \n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "        \n",
    "        instance = TrainingInstance_tmp(\n",
    "            tokens=tokens,\n",
    "            segment_ids=segment_ids,\n",
    "            labels=labels)\n",
    "        instances.append(instance)\n",
    "        \n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ebc3f",
   "metadata": {},
   "source": [
    "## 1. Create Training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(samples, mecab_sp_tokenizer, vocab_words, max_seq_length, rng):\n",
    "    \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "    \n",
    "    instances = []\n",
    "    instances.extend(\n",
    "        create_instances_from_document(samples, max_seq_length, rng, vocab_words, mecab_sp_tokenizer))\n",
    "            \n",
    "    print(\"len(instances): \", len(instances))\n",
    "    rng.shuffle(instances)\n",
    "    \n",
    "    return instances\n",
    "\n",
    "    \n",
    "\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aeda22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(targetfile, mecab_sp_tokenizer, version, startline, endline, vocab_words):\n",
    "    f = open(targetfile, \"r\")\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    lines = lines[startline:endline]\n",
    "    \n",
    "    stacks = []\n",
    "    for l in range(len(lines)):\n",
    "        if l%100==0:\n",
    "            print(l, \"/\", len(lines))\n",
    "        \n",
    "        try:\n",
    "            line = lines[l].strip(\"\\n\")\n",
    "            line = line.split(\"\\t\")\n",
    "            if len(line)<3:\n",
    "                continue\n",
    "        \n",
    "            label_text = line[0] # data label text\n",
    "            label_int  = line[1] # data label int\n",
    "            content    = line[2] # text\n",
    "            \n",
    "        except:\n",
    "            print(\"line: \", line)\n",
    "            \n",
    "        tokens = []\n",
    "        token_ids = mecab_sp_tokenizer.Encode(content) # kobert\n",
    "        for t in range(len(token_ids)):\n",
    "            token = vocab_words.i_to_w[token_ids[t]]\n",
    "            tokens.append(token)\n",
    "\n",
    "        \n",
    "        record = []\n",
    "        record.append(label_text)\n",
    "        record.append(label_int)\n",
    "        record.append(tokens)\n",
    "        \n",
    "#         print(\"record: \", record)\n",
    "        \n",
    "        stacks.append(record)\n",
    "    return stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529cf20",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f408db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab_words(object):\n",
    "    def __init__(self, vocab_file):\n",
    "        self.i_to_w = {}\n",
    "        self.w_to_i = {}\n",
    "        self.getvocab(vocab_file)\n",
    "#         print(self.w_to_i)\n",
    "\n",
    "    def getvocab(self, vocab_file):\n",
    "        f = open(vocab_file, 'r')\n",
    "        lines = f.readlines()\n",
    "        for l in range(len(lines)):\n",
    "            self.i_to_w[l] = lines[l].strip(\"\\n\")\n",
    "            self.w_to_i[lines[l].strip(\"\\n\")] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    # required\n",
    "    input_file, \n",
    "    vocab_file,\n",
    "    version,\n",
    "    outdir,\n",
    "    spmodel, \n",
    "\n",
    "    # optional\n",
    "    do_lower_case=True, \n",
    "    max_seq_length=512, \n",
    "    random_seed=12345, \n",
    "    ):\n",
    "    \n",
    "    randseed = random.randint(1, 1000)\n",
    "    print(\"randseed: \", randseed)\n",
    "    rng = random.Random(randseed)\n",
    " \n",
    "    # vocab_words\n",
    "    class Vocab_words(object):\n",
    "        def __init__(self, vocab_file):\n",
    "            self.i_to_w = {}\n",
    "            self.w_to_i = {}\n",
    "            self.getvocab(vocab_file)\n",
    "    #         print(self.w_to_i)\n",
    "\n",
    "        def getvocab(self, vocab_file):\n",
    "            f = open(vocab_file, 'r')\n",
    "            lines = f.readlines()\n",
    "            for l in range(len(lines)):\n",
    "                term = lines[l].strip(\"\\n\")\n",
    "                term = convert_to_unicode(term)\n",
    "                self.i_to_w[int(l)] = term\n",
    "                self.w_to_i[term] = int(l)\n",
    "        \n",
    "    vocab_words = Vocab_words(vocab_file)\n",
    "    \n",
    "    # sptokenizer\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.load(spmodel)\n",
    "    \n",
    "    \n",
    "    f = open(input_file, \"r\")\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    print(\"generating...\")\n",
    "    print(\"len(lines): \", len(lines))\n",
    "    jump = 1000\n",
    "    for d in range(0, len(lines), jump):\n",
    "        s = d\n",
    "        e = d+jump\n",
    "        if e>len(lines):\n",
    "            e = len(lines)\n",
    "        print(s, \"~\", e, \"/\", len(lines))\n",
    "        samples = read_dataset(input_file, tokenizer, version, startline=s, endline=e, vocab_words=vocab_words)\n",
    "        \n",
    "        rng.shuffle(samples)\n",
    "        \n",
    "        filename = \"\".join([\"0\"]*(8-len(str(s))))+str(s)+\".cache\"\n",
    "        \n",
    "        instances = create_training_instances(samples, tokenizer, vocab_words, max_seq_length, rng)\n",
    "        \n",
    "        write_instance_to_example_files(\n",
    "                        instances=instances, \n",
    "                        mecab_sp_tokenizer=tokenizer,\n",
    "                        vocab_words=vocab_words,\n",
    "                        max_seq_length=max_seq_length,\n",
    "                        outfilename=outdir+\"/\"+filename)\n",
    "#         print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12926e5f",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5eb88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./cache\"):\n",
    "    os.makedirs(\"./cache\")\n",
    "    \n",
    "print(\"Generating Features...\")\n",
    "\n",
    "versions = [\n",
    "            \"kobert\"\n",
    "]\n",
    "\n",
    "datapath = [\"./data/train.txt\", \"./data/test.txt\"]\n",
    "\n",
    "vocab_paths = [\n",
    "    \"../otherberts/KoBERT/models\",    \n",
    "]\n",
    "\n",
    "do_lower_cases = [False]\n",
    "\n",
    "assert len(versions)==len(vocab_paths)\n",
    "assert len(versions)==len(do_lower_cases)\n",
    "\n",
    "\n",
    "# folder loop\n",
    "for t in range(len(versions)):\n",
    "    print(\"versions: \", versions[t])\n",
    "    out_directory = \"./cache/\"+str(versions[t])+\"/\"    \n",
    "    if not os.path.exists(out_directory):\n",
    "        os.makedirs(out_directory)\n",
    "\n",
    "    # sample loop\n",
    "    for s in range(len(datapath)):\n",
    "        print(\"datapath[s]: \", datapath[s])\n",
    "        datapathname = datapath[s].split(\"/\")[-1].split(\".\")[0]\n",
    "        out_directory = \"./cache/\"+str(versions[t])+\"/\"+str(datapathname)\n",
    "\n",
    "        if not os.path.exists(out_directory):\n",
    "            os.makedirs(out_directory)\n",
    "\n",
    "        main(\n",
    "            # required\n",
    "            input_file = datapath[s], \n",
    "            outdir = out_directory,\n",
    "            \n",
    "            version = versions[t],\n",
    "\n",
    "            #sentence piece\n",
    "            #https://skt-lsl-nlp-model.s3.amazonaws.com/KoBERT/tokenizers/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
    "            vocab_file = vocab_paths[t]+'/vocab.txt',\n",
    "            spmodel = vocab_paths[t]+'/spiece.model',\n",
    "\n",
    "            # optional\n",
    "            do_lower_case = do_lower_cases[t],\n",
    "            max_seq_length = 512,\n",
    "        )\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce63cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb211fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a2717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d25fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
