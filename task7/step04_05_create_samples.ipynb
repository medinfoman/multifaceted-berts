{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c93de7",
   "metadata": {},
   "source": [
    "# 04-05. Make samples for knowledge inference task\n",
    "\n",
    "\n",
    "# 04. Make candidate entities & document links\n",
    "\n",
    "- get top10 entities within a department\n",
    "- Each entity has one file to link documents which contain the entity.\n",
    "- example of output\n",
    "- filename: C0020538,Hypertensive disease,[dsyn],3588_valid.txt\n",
    "- contents:\n",
    "<pre>\n",
    "Patient_fakeid.txt/docid    sentenceid/begin_of_entity/end_of_entity/semantic type/entity name\n",
    "00000120_data.txt/00000000\t00000004/00000002/00000005/[dsyn]/Hypertensive disease\n",
    "00000330_data.txt/00000004\t00000048/00000002/00000005/[dsyn]/Hypertensive disease\n",
    "00000330_data.txt/00000005\t00000061/00000002/00000005/[dsyn]/Hypertensive disease\n",
    "00000330_data.txt/00000009\t00000150/00000000/00000003/[dsyn]/Hypertensive disease\n",
    "...\n",
    "</pre>\n",
    "\n",
    "\n",
    "## a. Get Dictionaries for entity locations, corpus locations\n",
    "\n",
    "#### Entity_loc_map\n",
    "- Entity -map- file name/document number/sentence number/begin/end\n",
    "    - key: cui_id\n",
    "    - value: [File name/document number/sentence number/begin/end/semantics/prefered_term]\n",
    "\n",
    "#### Corpus_map\n",
    "- File name + document number -map- document content\n",
    "    - key: File name + document number\n",
    "    - value: [\n",
    "line (sentence number, document number, original sentence number, section, content)]\n",
    "    - original sentence number: while labeling metamap, some sentences were split into sub sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# 00001 -> 1\n",
    "def recover_to_int(strnumber):\n",
    "    for z in range(len(strnumber)):\n",
    "        if strnumber[z]!=0:\n",
    "            return int(strnumber[z:])\n",
    "    return 0\n",
    "\n",
    "# entitie map\n",
    "def make_ent_map(targetdir, filename, ent_map):\n",
    "    \n",
    "    ent_mode = False\n",
    "    file = open(targetdir, \"r\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    \n",
    "    for l in range(len(lines)):\n",
    "        line = lines[l].strip()\n",
    "#         print(\"line: \", line)\n",
    "        \n",
    "        if \"#### entities\" in line:\n",
    "            ent_mode=True\n",
    "        \n",
    "        if len(line.split(\"\\t\"))<3:\n",
    "            continue\n",
    "        \n",
    "        if ent_mode==True:\n",
    "            if line.lower()==\"none\":\n",
    "                break\n",
    "                \n",
    "            key = line.split(\"\\t\")[0]\n",
    "            cui = line.split(\"\\t\")[1]\n",
    "            sem = line.split(\"\\t\")[2]\n",
    "            term = line.split(\"\\t\")[3]\n",
    "            \n",
    "            doc_id = key.split(\"/\")[0]\n",
    "            sent_id = key.split(\"/\")[1]\n",
    "            begin = key.split(\"/\")[2]\n",
    "            end = key.split(\"/\")[3]\n",
    "            \n",
    "            if cui in ent_map:\n",
    "                valtmp = ent_map[cui]\n",
    "                valtmp.append(filename+\"/\"+doc_id+\"/\"+sent_id+\"/\"+\\\n",
    "                                            begin+\"/\"+end+\"/\"+sem+\"/\"+term)\n",
    "                ent_map[cui] = valtmp\n",
    "            \n",
    "            else:\n",
    "                ent_map[cui] = [filename+\"/\"+doc_id+\"/\"+sent_id+\"/\"+\\\n",
    "                                            begin+\"/\"+end+\"/\"+sem+\"/\"+term]\n",
    "    \n",
    "    return ent_map\n",
    "    \n",
    "\n",
    "\n",
    "def make_corpus_map(targetdir, filename, corpus_map):\n",
    "    \n",
    "    ent_mode = False\n",
    "    file = open(targetdir, \"r\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    \n",
    "    for l in range(len(lines)):\n",
    "        line = lines[l].strip()\n",
    "        #print(\"line: \", line)\n",
    "        \n",
    "        if \"#### entities\" in line:\n",
    "            break\n",
    "        \n",
    "        doc_id = line.split(\"\\t\")[0]\n",
    "        sent_id = line.split(\"\\t\")[1]\n",
    "\n",
    "        dockey = str(filename)+\"/\"+str(doc_id)\n",
    "        if dockey in corpus_map:\n",
    "            corpustmp = corpus_map[dockey]\n",
    "            corpustmp.append(line)\n",
    "            corpus_map[dockey] = corpustmp            \n",
    "        else:\n",
    "            corpus_map[dockey] = [line]\n",
    "\n",
    "    return corpus_map\n",
    "\n",
    "\n",
    "def get_map_data(department=\"감염내과\", mode=\"train\"):\n",
    "    start_time = time.time()\n",
    "    ent_map = {}\n",
    "    corpus_map = {}\n",
    "    \n",
    "    cui_to_term = {}\n",
    "    cui_to_sem = {}\n",
    "    ent_count = {}\n",
    "    \n",
    "    files_input = glob.glob(\"./data/03_entities_task7_fixed/\"+str(mode)+\"/\"+str(department)+\"/*_data.txt\")\n",
    "    files_input.sort()\n",
    "    \n",
    "    for f in range(0, len(files_input)):\n",
    "        filename = files_input[f].split(\"/\")[-1]\n",
    "        ent_map = make_ent_map(files_input[f], filename, ent_map)\n",
    "        corpus_map = make_corpus_map(files_input[f], filename, corpus_map)\n",
    "    \n",
    "    return ent_map, corpus_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581ea66",
   "metadata": {},
   "source": [
    "##  b. Get top-10 entities within each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451cf337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_entities(top=10, department=\"감염내과\", mode=\"train\"):\n",
    "    read_path = \"./data/ent_dist/\"+str(mode)+\"_\"+str(department)+\"task7_dist.txt\"\n",
    "#     print(\"read_path: \", read_path)\n",
    "    file = open(read_path, \"r\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    \n",
    "    target_entities = []\n",
    "    for l in range(1, len(lines)):\n",
    "        if (l)==top+1:\n",
    "            break\n",
    "        line = lines[l].strip()\n",
    "        target_entities.append(line)\n",
    "    \n",
    "    return target_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3623c671",
   "metadata": {},
   "source": [
    "## c. Create samples\n",
    "\n",
    "### Determination of candidate documents for target entity\n",
    "- target_loc_map = {}\n",
    "- Iteration of (target entities)\n",
    "    - In entity_loc_map, obtain multiple values “file name/document number/sentence number/begin/end” according to target entity as key\n",
    "    - Since we decided to [MASK] only the entity that appears first in the document, we went through the following process.\n",
    "    - Sort by “file name/document number/sentence number/begin/end”\n",
    "    - Obtain only one smallest “sentence number/begin/end” for each “file name/document number”\n",
    "    - (target_loc_map[\"file name/document number\"] = smallest \"sentence number/begin/end\")\n",
    "\n",
    "### Obtain candidate documents for target entity\n",
    "- Iteration of (target_loc_map)\n",
    "    - By using 'Corpus_map', obtain value documents (input key: “file name/sentence number”)\n",
    "    - Corpus_map([\"File name/sentence number\"]) == Document content\n",
    "    - Replace the relevant entity in the document content with the [MASK] token\n",
    "    - The document goes through the conversion process below.\n",
    "        - Before conversion: (sentence number, document number, original sentence number, section, content)\n",
    "        - After conversion: Document consisting of \"section (enter) content\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f6807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_ent_candidate_docs(target_ent, ent_map):\n",
    "    cui = target_ent.split(\"\\t\")[0]\n",
    "    term = target_ent.split(\"\\t\")[1]\n",
    "    sem = target_ent.split(\"\\t\")[2]\n",
    "    count = target_ent.split(\"\\t\")[3]\n",
    "    \n",
    "    # Obtain only the first entity per document\n",
    "    firstparts = {}    \n",
    "    if cui not in ent_map:\n",
    "        return firstparts\n",
    "    \n",
    "    entinfo = ent_map[cui]\n",
    "    \n",
    "    # Since we decided to [MASK] only the entity that appears first in the document, \n",
    "    # we went through the following process:\n",
    "    # Sort by “file name/document number/sentence number/begin/end”\n",
    "    entinfo.sort()\n",
    "    \n",
    "    for e in range(len(entinfo)):\n",
    "        line = entinfo[e].strip()\n",
    "        \n",
    "        filename = line.split(\"/\")[0]\n",
    "        docid = line.split(\"/\")[1]\n",
    "        sentid = line.split(\"/\")[2]\n",
    "        begin = line.split(\"/\")[3]\n",
    "        end = line.split(\"/\")[4]\n",
    "        sem = line.split(\"/\")[5]\n",
    "        term = line.split(\"/\")[6]\n",
    "        \n",
    "        document_id = str(filename)+\"/\"+str(docid)\n",
    "        \n",
    "        if document_id not in firstparts:\n",
    "            firstparts[document_id]=sentid+\"/\"+begin+\"/\"+end+\"/\"+sem+\"/\"+term\n",
    "     \n",
    "    return firstparts\n",
    "\n",
    "def write_target_ents(target_ents, ent_map, mode=\"train\", department=\"감염내과\"):\n",
    "    for t in range(len(target_ents)):\n",
    "        candidates          = target_ent_candidate_docs(target_ents[t], ent_map)\n",
    "        outdir = \"./data/04_ents_location/\"+str(mode)+\"/\"+str(department)\n",
    "        if not os.path.exists(outdir):\n",
    "            os.makedirs(outdir)\n",
    "            \n",
    "        outtext = []\n",
    "        for key in candidates:\n",
    "            outtext.append(key+\"\\t\"+candidates[key])\n",
    "\n",
    "        outfilename = target_ents[t].replace(\"\\t\", \",\")\n",
    "        \n",
    "        print(outdir+\"/\"+str(outfilename)+\".txt\")\n",
    "        \n",
    "        file = open(outdir+\"/\"+str(outfilename)+\".txt\", \"w\")\n",
    "        file.write(\"\\n\".join(outtext))\n",
    "        file.close()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96ca595",
   "metadata": {},
   "source": [
    "## d. length filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization import BertTokenizer\n",
    "import tokenization as tokenization\n",
    "\n",
    "def orgarnize_document(target_doc):\n",
    "    date = \"\"\n",
    "    doctype = \"\"\n",
    "    section2content = {}\n",
    "    for t in range(len(target_doc)):\n",
    "        if t==0:\n",
    "            date = target_doc[t].split(\"\\t\")[4]\n",
    "            continue\n",
    "            \n",
    "        section = target_doc[t].split(\"\\t\")[3]\n",
    "        content = target_doc[t].split(\"\\t\")[4]\n",
    "        \n",
    "        # DOCTYPE [ 감염내과 _ 외래경과 ] SECTION [ Subjective [ Symptoms ] <- 소견 ]\n",
    "        doctype = section.split(\"] SECTION [\")[0]\n",
    "        doctype = doctype.replace(\"DOCTYPE [\", \"\").strip()\n",
    "        \n",
    "        section = section.split(\"] SECTION [\")[1].strip()\n",
    "        section = section[:-1].strip()\n",
    "        \n",
    "        if section in section2content:\n",
    "            tmpcontent = section2content[section]\n",
    "            tmpcontent.append(content)\n",
    "            section2content[section] = tmpcontent\n",
    "        else:\n",
    "            section2content[section] = [content]\n",
    "    \n",
    "    # 문서 재조립\n",
    "    or_document = []\n",
    "    or_document.append(date)\n",
    "    or_document.append(doctype)\n",
    "    \n",
    "    for section in section2content:\n",
    "        or_document.append(section)\n",
    "        contents = section2content[section]\n",
    "        or_document = or_document + contents\n",
    "    \n",
    "    return or_document\n",
    "\n",
    "\n",
    "# Ensure samples that are within the maximum input length of the BERT\n",
    "def check_token_length(documents):\n",
    "    max_len = 512 - 2 # [CLS], [SEP]\n",
    "    vocab_paths = [\n",
    "        \"../otherberts/bertbase_cased/vocab.txt\",\n",
    "        \"../otherberts/mbert_cased/vocab.txt\"\n",
    "    ]\n",
    "    do_lower = [False, False]\n",
    "    max_lengths = [512, 512]\n",
    "    \n",
    "    assert len(vocab_paths)==len(do_lower)\n",
    "    assert len(vocab_paths)==len(max_lengths)\n",
    "    \n",
    "    for v in range(len(vocab_paths)):\n",
    "        tokenizer = BertTokenizer(vocab_file=vocab_paths[v], do_lower_case=do_lower[v], max_len=max_lengths[v])\n",
    "        vocab_words = list(tokenizer.vocab.keys())\n",
    "        \n",
    "        new_doc = []\n",
    "        for d in range(len(documents)):\n",
    "            content = documents[d]\n",
    "            if do_lower[v]==True:\n",
    "                content = content.lower()\n",
    "            content = tokenizer.tokenize(content)\n",
    "            new_doc = new_doc + content\n",
    "\n",
    "        if len(new_doc)>max_len:\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "\n",
    "# Ensure samples that are within the maximum input length of the BERT\n",
    "def corpus_length_filter(corpus_map, department, mode):\n",
    "    path = \"./data/04_ents_location/\"+str(mode)+\"/\"+str(department)+\"/*.txt\"\n",
    "    entfiles = glob.glob(path)\n",
    "    entfiles.sort()\n",
    "    #print(\"entfiles: \", entfiles)\n",
    "    \n",
    "    for e in range(len(entfiles)):\n",
    "        ent = entfiles[e].split(\"/\")[-1] \n",
    "        ent = ent.replace(\".txt\", \"\") # label\n",
    "\n",
    "        file = open(entfiles[e], \"r\")\n",
    "        lines = file.readlines()\n",
    "        file.close()\n",
    "        \n",
    "        # candidates_locs -> valid_candidate\n",
    "        valid_candidate = []\n",
    "        for l in range(len(lines)):\n",
    "            line = lines[l].strip()\n",
    "            filename_docidx = line.split(\"\\t\")[0]\n",
    "            filename = filename_docidx.split(\"/\")[0]\n",
    "            docidx = recover_to_int(filename_docidx.split(\"/\")[1])\n",
    "            key = str(filename)+\"/\"+str(docidx)\n",
    "            \n",
    "            target_doc = corpus_map[key]\n",
    "            # (before)\n",
    "            # sectionname sentence1\n",
    "            # sectionname sentence2\n",
    "            # sectionname sentence3 ...\n",
    "            # ->\n",
    "            # (after)\n",
    "            # sectionname sentence1 \\n sentence2 \\n sentence3 ...            \n",
    "            target_doc = orgarnize_document(target_doc)\n",
    "            \n",
    "            # check length of docs\n",
    "            legnthover = check_token_length(target_doc)\n",
    "            if legnthover==True:\n",
    "                continue\n",
    "            else:\n",
    "                valid_candidate.append(line)\n",
    "                \n",
    "        outdir = \"/\".join(entfiles[e].split(\"/\")[:-1])\n",
    "        ent = entfiles[e].split(\"/\")[-1]\n",
    "        ent = ent.replace(\".txt\", \"\")\n",
    "        \n",
    "        print(outdir+\"/\"+str(ent)+\"_valid.txt\")\n",
    "        file = open(outdir+\"/\"+str(ent)+\"_valid.txt\", \"w\")\n",
    "        file.write(\"\\n\".join(valid_candidate))\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6496b2b",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "- Function to implement a~d\n",
    "- a. Get Dictionaries for entity locations, corpus locations\n",
    "- b. Get top-10 entities within each department\n",
    "- c. Create samples\n",
    "- d. length filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 과정을 한번에 돌리는 함수\n",
    "def run_bundle_get_entity_locations(department=\"감염내과\", mode=\"train\"):\n",
    "    # a. Get Dictionaries for entity locations, corpus locations\n",
    "    ent_map, corpus_map = get_map_data(department=department, mode=mode)    \n",
    "    \n",
    "    # b. Get top-10 entities within each department\n",
    "    target_ents         = get_target_entities(top=10, department=department, mode=\"train\")\n",
    "    \n",
    "    # c. Create samples\n",
    "    write_target_ents(target_ents, ent_map, mode=mode, department=department)    \n",
    "    \n",
    "    # d. length filter\n",
    "    corpus_length_filter(corpus_map, department, mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8c9856",
   "metadata": {},
   "source": [
    "# Run pipelines\n",
    "- example of output\n",
    "- filename: C0020538,Hypertensive disease,[dsyn],3588_valid.txt\n",
    "- contents:\n",
    "<pre>\n",
    "Patient_fakeid.txt/docid    sentenceid/begin_of_entity/end_of_entity/semantic type/entity name\n",
    "00000120_data.txt/00000000\t00000004/00000002/00000005/[dsyn]/Hypertensive disease\n",
    "00000330_data.txt/00000004\t00000048/00000002/00000005/[dsyn]/Hypertensive disease\n",
    "00000330_data.txt/00000005\t00000061/00000002/00000005/[dsyn]/Hypertensive disease\n",
    "00000330_data.txt/00000009\t00000150/00000000/00000003/[dsyn]/Hypertensive disease\n",
    "...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59b108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mode=\"train\"\n",
    "run_bundle_get_entity_locations(department=\"감염내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"내분비대사내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"류마티스내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"소화기내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"순환기내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"신장내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"알레르기내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"호흡기내과\", mode=mode)\n",
    "\n",
    "mode=\"test\"\n",
    "run_bundle_get_entity_locations(department=\"감염내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"내분비대사내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"류마티스내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"소화기내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"순환기내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"신장내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"알레르기내과\", mode=mode)\n",
    "run_bundle_get_entity_locations(department=\"호흡기내과\", mode=mode)\n",
    "\n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dce3fa",
   "metadata": {},
   "source": [
    "## Entity types used in top-10 of departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe3f50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def get_labels():\n",
    "    target_entities_all = []\n",
    "    \n",
    "    path = \"./data/04_ents_location/train/*\"\n",
    "    departments = glob.glob(path)\n",
    "    \n",
    "    for d in range(len(departments)):\n",
    "        entfiles = glob.glob(departments[d]+\"/*_valid.txt\")\n",
    "        \n",
    "        for e in range(len(entfiles)):\n",
    "            ent = entfiles[e].split(\"/\")[-1]\n",
    "            ent = ent.split(\",\")[:-1] # C0020538,Hypertensive disease,[dsyn],2066_valid -> [C0020538,Hypertensive disease,[dsyn]]\n",
    "            ent = \",\".join(ent)\n",
    "            if ent not in target_entities_all:\n",
    "                target_entities_all.append(ent)\n",
    "    \n",
    "    return target_entities_all\n",
    "\n",
    "labels = get_labels()\n",
    "print(\"len(labels): \", len(labels))\n",
    "# print(labels)\n",
    "\n",
    "\n",
    "file = open(\"./data/labels.txt\", \"w\")\n",
    "file.write(\"\\n\".join(labels))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1663a",
   "metadata": {},
   "source": [
    "<hr></hr>\n",
    "\n",
    "# 05 sampling\n",
    "\n",
    "- Get the links of documents to be used in actual task\n",
    "- example of output\n",
    "- filename: C0020538,Hypertensive disease,[dsyn],3588_rand30.txt\n",
    "- contents:\n",
    "<pre>\n",
    "Patient_fakeid.txt/docid    sentenceid/begin_of_entity/end_of_entity    CUI/entity name/semantic type    department\n",
    "00003108_data.txt/00000001\t00000035/00000002/00000014\tC0020538/Hypertensive disease/[dsyn]\t신장내과\n",
    "00003820_data.txt/00000000\t00000004/00000003/00000006\tC0020538/Hypertensive disease/[dsyn]\t신장내과\n",
    "00012070_data.txt/00000002\t00000040/00000001/00000004\tC0020538/Hypertensive disease/[dsyn]\t신장내과\n",
    "00025519_data.txt/00000004\t00000115/00000000/00000003\tC0020538/Hypertensive disease/[dsyn]\t신장내과\n",
    "...\n",
    "</pre>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a50de",
   "metadata": {},
   "source": [
    "# Random sample\n",
    "- Select k random documents mapped to target entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77324c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_sample(department=\"감염내과\", mode = \"train\"):\n",
    "    if mode==\"train\":\n",
    "        max_samples = 30\n",
    "    elif mode==\"test\":\n",
    "        max_samples = 10\n",
    "        \n",
    "    \n",
    "    path = \"./data/04_ents_location/\"+str(mode)+\"/\"+str(department)+\"/*_valid.txt\"\n",
    "    entfiles = glob.glob(path)\n",
    "\n",
    "    for e in range(len(entfiles)):\n",
    "        ent = entfiles[e].split(\"/\")[-1]\n",
    "        ent = ent.replace(\"_valid.txt\", \"\") # label\n",
    "        ent_info = \"/\".join(ent.split(\",\")[:-1])\n",
    "        print(\"ent_info: \", ent_info)\n",
    "        \n",
    "        file = open(entfiles[e], \"r\")\n",
    "        lines = file.readlines()\n",
    "        file.close()\n",
    "        \n",
    "        candidates_locs = []\n",
    "        for l in range(len(lines)):\n",
    "            line = lines[l].strip()\n",
    "            candidates_locs.append(line)\n",
    "\n",
    "        random.shuffle(candidates_locs)\n",
    "\n",
    "        candidates_locs = candidates_locs[:max_samples]\n",
    "        candidates_locs.sort()\n",
    "\n",
    "        \n",
    "        for c in range(len(candidates_locs)):    \n",
    "            sample_info = candidates_locs[c].split(\"\\t\")[1]\n",
    "            sample_info = \"/\".join(sample_info.split(\"/\")[:3])\n",
    "            \n",
    "            candidates_locs[c] = candidates_locs[c].split(\"\\t\")[0]+\"\\t\"+sample_info+\"\\t\"+ent_info+\"\\t\"+str(department)\n",
    "        \n",
    "        \n",
    "        outdir = \"./data/04_ents_location/\"+str(mode)+\"/\"+str(department)\n",
    "\n",
    "        file = open(outdir+\"/\"+str(ent)+\"_rand\"+str(max_samples)+\".txt\", \"w\")\n",
    "        file.write(\"\\n\".join(candidates_locs))\n",
    "        file.close()\n",
    "        \n",
    "        \n",
    "\n",
    "mode = \"train\"\n",
    "random_sample(department=\"감염내과\", mode = mode)\n",
    "random_sample(department=\"내분비대사내과\", mode = mode)\n",
    "random_sample(department=\"류마티스내과\", mode = mode)\n",
    "random_sample(department=\"소화기내과\", mode = mode)\n",
    "random_sample(department=\"순환기내과\", mode = mode)\n",
    "random_sample(department=\"신장내과\", mode = mode)\n",
    "random_sample(department=\"알레르기내과\", mode = mode)\n",
    "random_sample(department=\"호흡기내과\", mode = mode)\n",
    "\n",
    "mode = \"test\"\n",
    "random_sample(department=\"감염내과\", mode = mode)\n",
    "random_sample(department=\"내분비대사내과\", mode = mode)\n",
    "random_sample(department=\"류마티스내과\", mode = mode)\n",
    "random_sample(department=\"소화기내과\", mode = mode)\n",
    "random_sample(department=\"순환기내과\", mode = mode)\n",
    "random_sample(department=\"신장내과\", mode = mode)\n",
    "random_sample(department=\"알레르기내과\", mode = mode)\n",
    "random_sample(department=\"호흡기내과\", mode = mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30d1067",
   "metadata": {},
   "source": [
    "# 05 get text\n",
    "- Obtain the original text and create a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d0d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from datamanager_task7 import get_map_data, recover_to_int, orgarnize_document\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Receive a document and map it to line number and content\n",
    "def mask_target_entity(target_doc, target_sentidx, begin, end):\n",
    "    masked_sample = []\n",
    "    for t in range(len(target_doc)):\n",
    "        docidx   = recover_to_int(target_doc[t].split(\"\\t\")[0])\n",
    "        sentidx  = recover_to_int(target_doc[t].split(\"\\t\")[1])\n",
    "        sentidx2 = recover_to_int(target_doc[t].split(\"\\t\")[2])\n",
    "        section  = target_doc[t].split(\"\\t\")[3]\n",
    "        content  = target_doc[t].split(\"\\t\")[4]\n",
    "        \n",
    "        # mask\n",
    "        if target_sentidx==sentidx:\n",
    "            content  = content[:begin] + \"[MASK]\" + content[end:]\n",
    "            \n",
    "        masked_sample.append(\"\\t\".join(target_doc[t].split(\"\\t\")[0:4]) +\"\\t\"+ content)\n",
    "        \n",
    "    return masked_sample\n",
    "\n",
    "# Obtain the original text and create a sample\n",
    "def get_text(corpus_map, department, mode):\n",
    "    if mode==\"train\":\n",
    "        max_samples=30\n",
    "    elif mode==\"test\":\n",
    "        max_samples=10\n",
    "    \n",
    "    path = \"./data/04_ents_location/\"+str(mode)+\"/\"+str(department)+\"/*_rand\"+str(max_samples)+\".txt\"\n",
    "    entfiles = glob.glob(path)\n",
    "    \n",
    "    sampled = []\n",
    "    for e in range(len(entfiles)):\n",
    "        ent = entfiles[e].split(\"/\")[-1]\n",
    "        ent = ent.replace(\"_rand30.txt\", \"\") # label\n",
    "    \n",
    "        file = open(entfiles[e], \"r\")\n",
    "        lines = file.readlines()\n",
    "        file.close()\n",
    "        \n",
    "        # line: Location information and entity information of the document mapped to the entity\n",
    "        for l in range(len(lines)):\n",
    "            ######################################\n",
    "            line     = lines[l].strip()\n",
    "            filename_docidx = line.split(\"\\t\")[0]\n",
    "            filename = filename_docidx.split(\"/\")[0]\n",
    "            docidx   = recover_to_int(filename_docidx.split(\"/\")[1])\n",
    "            key      = str(filename)+\"/\"+str(docidx)\n",
    "            \n",
    "            # Obtain the target document\n",
    "            target_doc = corpus_map[key]\n",
    "            ######################################\n",
    "            \n",
    "\n",
    "            ######################################\n",
    "            # location information\n",
    "            locinfo = line.split(\"\\t\")[1]\n",
    "            sentidx = recover_to_int(locinfo.split(\"/\")[0])\n",
    "            begin   = recover_to_int(locinfo.split(\"/\")[1])\n",
    "            end     = recover_to_int(locinfo.split(\"/\")[2])\n",
    "            \n",
    "            # Replace entities in the document with [MASK] tokens\n",
    "            target_doc = mask_target_entity(target_doc, sentidx, begin, end)\n",
    "            \n",
    "            \n",
    "            arranged_doc = orgarnize_document(target_doc)\n",
    "            ######################################\n",
    "\n",
    "            \n",
    "            ######################################\n",
    "            # entity info\n",
    "            entinfo = line.split(\"\\t\")[2]\n",
    "            cuiid   = entinfo.split(\"/\")[0]\n",
    "            term    = entinfo.split(\"/\")[1]\n",
    "            sem     = entinfo.split(\"/\")[2]\n",
    "            label = str(cuiid)+\",\"+str(term)+\",\"+str(sem) # C0020538,Hypertensive disease,[dsyn]\n",
    "\n",
    "            sampled.append(line+\"\\t\"+\" \".join(arranged_doc))\n",
    "            ######################################\n",
    "        \n",
    "    outdir = \"./data/05_samples/\"+str(mode)\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    file = open(outdir+\"/\"+str(department)+\".txt\", \"w\")\n",
    "    file.write(\"\\n\".join(sampled))\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "# run\n",
    "def get_text_run(department, mode):\n",
    "    print(department, mode)\n",
    "    ent_map, corpus_map = get_map_data(department=department, mode=mode)\n",
    "    get_text(corpus_map, department=department, mode=mode)\n",
    "\n",
    "    \n",
    "mode = \"train\"\n",
    "get_text_run(department=\"감염내과\", mode=mode)\n",
    "get_text_run(department=\"내분비대사내과\", mode=mode)\n",
    "get_text_run(department=\"류마티스내과\", mode=mode)\n",
    "get_text_run(department=\"소화기내과\", mode=mode)\n",
    "get_text_run(department=\"순환기내과\", mode=mode)\n",
    "get_text_run(department=\"신장내과\", mode=mode)\n",
    "get_text_run(department=\"알레르기내과\", mode=mode)\n",
    "get_text_run(department=\"호흡기내과\", mode=mode)\n",
    "\n",
    "\n",
    "mode = \"test\"\n",
    "get_text_run(department=\"감염내과\", mode=mode)\n",
    "get_text_run(department=\"내분비대사내과\", mode=mode)\n",
    "get_text_run(department=\"류마티스내과\", mode=mode)\n",
    "get_text_run(department=\"소화기내과\", mode=mode)\n",
    "get_text_run(department=\"순환기내과\", mode=mode)\n",
    "get_text_run(department=\"신장내과\", mode=mode)\n",
    "get_text_run(department=\"알레르기내과\", mode=mode)\n",
    "get_text_run(department=\"호흡기내과\", mode=mode)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17419b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all samples, mix them, and print them\n",
    "import random\n",
    "def all_gather(mode, is_shuffle=True):\n",
    "    path = \"./data/05_samples/\"+str(mode)+\"/*.txt\"\n",
    "    entfiles = glob.glob(path)\n",
    "    # print(\"entfiles: \", entfiles)\n",
    "    \n",
    "    sampled = []\n",
    "    for e in range(len(entfiles)):\n",
    "        ent = entfiles[e].split(\"/\")[-1]\n",
    "        ent = ent.replace(\"_rand30.txt\", \"\") # label\n",
    "    \n",
    "        file = open(entfiles[e], \"r\")\n",
    "        lines = file.readlines()\n",
    "        file.close()\n",
    "        sampled = sampled + [line.strip() for line in lines]\n",
    "    \n",
    "    if is_shuffle==True:\n",
    "        random.shuffle(sampled)\n",
    "    \n",
    "    file = open(\"./data/05_samples/\"+str(mode)+\".txt\", \"w\")\n",
    "    file.write(\"\\n\".join(sampled))\n",
    "    file.close()\n",
    "\n",
    "all_gather(mode=\"train\", is_shuffle=False)\n",
    "all_gather(mode=\"test\", is_shuffle=False)\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f4d58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
