{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd19d8c",
   "metadata": {},
   "source": [
    "# Labeling Entities using Metamap\n",
    "\n",
    "## Note\n",
    "- The codes extract disease names from documents using MetaMap\n",
    "- We used the following python wrapper\n",
    "    - https://github.com/AnthonyMRios/pymetamap\n",
    "- Download the code and locate the pymetamap folder in task 7 folder\n",
    "- Be sure to <b>run the MetaMap (MetaMap2020) server before running this code</b>\n",
    "- Set your mmpath='metamap installation location'\n",
    "\n",
    "## Install metmap on your system\n",
    "- To run the metamap server, you should install java & Metamap thesaurus the first\n",
    "- Before download Metamap thesaurus, signup & get your UMLS Metathesaurus License from National Library of Medicine.\n",
    "    - https://uts.nlm.nih.gov/uts/signup-login\n",
    "- To download Metamap thesaurus visit \n",
    "    - https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/run-locally/MainDownload.html\n",
    "- How to install Metamap: \n",
    "    - https://lhncbc.nlm.nih.gov/ii/tools/MetaMap/documentation/Installation.html\n",
    "- After installing them, start metamap by using following command in Ubuntu prompt:\n",
    "    \n",
    "## Start metamap server\n",
    "<pre>\n",
    "cd /path/installed/mmap2020/public_mm\n",
    "./bin/skrmedpostctl start\n",
    "./bin/wsdserverctl start\n",
    "#./bin/skrmedpostctl stop\n",
    "#./bin/wsdserverctl stop\n",
    "./bin/metamap\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf01b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymetamap import MetaMap\n",
    "\n",
    "# mmpath = '/home/kkm/anaconda_home/mmap2020/public_mm/bin/metamap'\n",
    "mmpath = '/path/installed/mmap2020/public_mm/bin/metamap'\n",
    "mm = MetaMap.get_instance(mmpath)\n",
    "semantic_filter = ['[dsyn]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c7396",
   "metadata": {},
   "source": [
    "## Test your input works\n",
    "- Get entity locations in character-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ['Heart Attack', 'John had a huge heart attack']\n",
    "concepts,error = mm.extract_concepts(sents,[1,2])\n",
    "for concept in concepts:\n",
    "    print(concept)\n",
    "    print(concept.pos_info)\n",
    "    \n",
    "    target_sent = int(concept.index)-1\n",
    "    pos_s = int(concept.pos_info.split(\"/\")[0])-1\n",
    "    pos_e = pos_s + int(concept.pos_info.split(\"/\")[1])\n",
    "    \n",
    "    # extraction result\n",
    "    print(sents[target_sent])\n",
    "    print(sents[target_sent][pos_s:pos_e])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341c1a8",
   "metadata": {},
   "source": [
    "### apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af913a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from task7reader import read_patient_documents\n",
    "import time\n",
    "\n",
    "# Function to get the position of entities\n",
    "# If there are multiple startidx sentences split them\n",
    "def call_mmp(sents, sents_ids_for_query, pos_score, pos_cui, pos_sem, pos_term, startidx, found_concepts):\n",
    "    # found_concepts: return concepts. use original Metamap output format\n",
    "    found_concepts = []\n",
    "    \n",
    "    # Temporarily replace the string causing the error with another string\n",
    "    sents_for_search = []\n",
    "    for s in range(len(sents)):\n",
    "        # \"/\" -> \",\"\n",
    "        tmp_sent = sents[s]\n",
    "        tmp_sent = tmp_sent.replace(\"/\", \",\")\n",
    "        tmp_sent = tmp_sent.replace(\"|\", \",\")\n",
    "        tmp_sent = tmp_sent.replace(\"\\\\\", \",\")\n",
    "        tmp_sent = tmp_sent.replace(\"≥\", \",\")\n",
    "        tmp_sent = tmp_sent.replace(\"-\", \",\")\n",
    "        tmp_sent = tmp_sent.replace(\")\", \",\")\n",
    "        tmp_sent = tmp_sent.replace(\"(\", \",\")\n",
    "\n",
    "        sents_for_search.append(tmp_sent)\n",
    "    \n",
    "    \n",
    "    # search\n",
    "    # Don't use original 'sents', because of text matching errors\n",
    "    # User 'sents_for_search'\n",
    "    concepts, error = mm.extract_concepts(sents_for_search, sents_ids_for_query)\n",
    "    \n",
    "    ########################\n",
    "    # Parse results (=cocepts) to get start, end location within each sentence\n",
    "    ########################\n",
    "    # Check the location of the acquired concept\n",
    "    # Obtain the position with the highest score\n",
    "    concept_count = 0\n",
    "    for concept in concepts:\n",
    "        concept_count = concept_count + 1\n",
    "        # exclude conceptAA\n",
    "        # conceptAA class does not have semtypes, so an error occurs.\n",
    "        try:\n",
    "            semantic = concept.semtypes\n",
    "            \n",
    "            found_concepts.append(str(concept))\n",
    "            \n",
    "        except:\n",
    "            print(\"conceptAA has no semantic\")\n",
    "            continue\n",
    "        \n",
    "        # positions of concepts == poses\n",
    "        # Covers the following cases\n",
    "        # pos_info='[5/3],[12/3],[62/3]'\n",
    "        # pos_info='122/2;[94/2],[107/2]'\n",
    "        poses = concept.pos_info.replace(\"[\", \"\")\n",
    "        poses = poses.replace(\"]\", \"\")\n",
    "        poses = poses.replace(\";\", \",\")\n",
    "        poses = poses.split(\",\")\n",
    "\n",
    "        for p in range(len(poses)):\n",
    "            target_line_num = int(concept.index)\n",
    "            \n",
    "            # set the sentence number considering offset (startidx)\n",
    "            # Reason for existing the offset is the a documents are split into \n",
    "            # small set of sentences to query the metamap.\n",
    "            # therefore there are some gaps between \n",
    "            # and search results sentence number (==original sentence number == concept.index)\n",
    "            # and sents_for_search sentence index (==target_line_num-startidx)\n",
    "            target_line = sents_for_search[target_line_num-startidx]\n",
    "            \n",
    "            if concept.semtypes not in semantic_filter:\n",
    "                continue\n",
    "            \n",
    "            # position of text\n",
    "            pos_s = int(poses[p].split(\"/\")[0])-1\n",
    "            pos_e = pos_s + int(poses[p].split(\"/\")[1])\n",
    "            \n",
    "            # bugfix\n",
    "            if pos_s==-1:\n",
    "                continue\n",
    "            \n",
    "            cui = str(concept.cui)\n",
    "            sem = str(semantic)\n",
    "            score = str(concept.score)\n",
    "            preferred_name = str(concept.preferred_name)\n",
    "            \n",
    "            zeros_d = \"\".join(['0']*(8-len(str(target_line_num))))\n",
    "            zeros_s = \"\".join(['0']*(8-len(str(pos_s))))\n",
    "            zeros_e = \"\".join(['0']*(8-len(str(pos_e))))\n",
    "            \n",
    "            # key = str(target_line_num)+\"/\"+str(pos_s)+\"/\"+str(pos_e)\n",
    "            key = zeros_d+str(target_line_num)+\"/\"+zeros_s+str(pos_s)+\"/\"+zeros_e+str(pos_e)\n",
    "            \n",
    "            if key not in pos_score:\n",
    "                pos_score[key] = score\n",
    "                pos_cui[key] = cui\n",
    "                pos_sem[key] = sem\n",
    "                pos_term[key] = preferred_name\n",
    "            else:\n",
    "                # get the pos with highest score\n",
    "                if score>pos_score[key]:\n",
    "                    pos_cui[key] = cui\n",
    "                    pos_sem[key] = sem\n",
    "                    pos_term[key] = preferred_name\n",
    "    \n",
    "    return pos_score, pos_cui, pos_sem, pos_term, found_concepts\n",
    "    \n",
    "    \n",
    "# before querying the sentence to metamap, \n",
    "# A function which make map sentence ids to segment id, vice versa \n",
    "def make_sents_ids_for_query(segment_ids):\n",
    "    sents_ids_for_query = [i for i in range(len(segment_ids))]\n",
    "    \n",
    "    # Creating a matrix to recover segid and sentid\n",
    "    sent_ids = [i for i in range(len(segment_ids))]\n",
    "    seg_to_sent = {}\n",
    "    sent_to_seg = {}\n",
    "    for i in range(len(segment_ids)):\n",
    "        seg_to_sent[segment_ids[i]] = i\n",
    "        sent_to_seg[i] = segment_ids[i]\n",
    "        \n",
    "    return seg_to_sent, sent_to_seg, sents_ids_for_query\n",
    "\n",
    "\n",
    "# result text parser 1\n",
    "# 00001 -> 1\n",
    "def recover_to_int(strnumber):\n",
    "    for z in range(len(strnumber)):\n",
    "        if strnumber[z]!=0:\n",
    "            return int(strnumber[z:])\n",
    "    return 0\n",
    "\n",
    "# result text parser 2\n",
    "def sort_entities(pos_cui, pos_sem, pos_term, reverse=False):\n",
    "    # 엔터티 정렬\n",
    "    pos_cui = dict(sorted(pos_cui.items(), reverse=reverse))\n",
    "    pos_sem = dict(sorted(pos_sem.items(), reverse=reverse))\n",
    "    pos_term = dict(sorted(pos_term.items(), reverse=reverse))\n",
    "    return pos_cui, pos_sem, pos_term\n",
    "\n",
    "# result text parser 3\n",
    "# Make sure there is no overlapping positions between entities\n",
    "# Select the longest entity.\n",
    "def make_entity_no_dup_area(pos_cui, pos_sem, pos_term):\n",
    "    del_targets = []\n",
    "\n",
    "    for key in pos_cui:\n",
    "        linenum = recover_to_int(key.split(\"/\")[0])\n",
    "        start = recover_to_int(key.split(\"/\")[1])\n",
    "        end = recover_to_int(key.split(\"/\")[2])\n",
    "        \n",
    "        len_ent = end-start+1\n",
    "        \n",
    "        for key_compare in pos_cui:\n",
    "            if key==key_compare:\n",
    "                continue\n",
    "                \n",
    "            compare_l = recover_to_int(key_compare.split(\"/\")[0])\n",
    "            compare_s = recover_to_int(key_compare.split(\"/\")[1])\n",
    "            compare_e = recover_to_int(key_compare.split(\"/\")[2])\n",
    "\n",
    "            len_ent2 = compare_e-compare_s+1\n",
    "            \n",
    "            # overlapped\n",
    "            if compare_l==linenum:\n",
    "                if (compare_s<= start and start<=compare_e) or (compare_s<= end and end<=compare_e) or \\\n",
    "                (start<= compare_s and compare_s<=end) or (start<= compare_e and compare_e<=end):\n",
    "                    \n",
    "                    if len_ent2<len_ent:\n",
    "                        if key_compare not in del_targets:\n",
    "                            del_targets.append(key_compare)\n",
    "                    \n",
    "                    elif len_ent2>=len_ent:\n",
    "                        if key not in del_targets:\n",
    "                            del_targets.append(key)\n",
    "    \n",
    "    \n",
    "    # Delete unnecessary entities\n",
    "    for d in range(len(del_targets)):\n",
    "        targetkey = del_targets[d]\n",
    "        del pos_cui[targetkey]\n",
    "        del pos_sem[targetkey]\n",
    "        del pos_term[targetkey]\n",
    "    \n",
    "    return pos_cui, pos_sem, pos_term\n",
    "\n",
    "\n",
    "# send queries (documents) to MetaMap\n",
    "def doc_iter(documents, output_directory, output_dir_entall):\n",
    "    interval  = 1000\n",
    "    pos_score = {}\n",
    "    pos_cui   = {}\n",
    "    pos_sem   = {}\n",
    "    pos_term  = {}\n",
    "    found_concepts = []\n",
    "\n",
    "    # segment_ids: Index of the original sentence\n",
    "    # sent_ids: Index of the sentence after segmentation\n",
    "    # sentid_to_sec: Index of the original sentence-section\n",
    "    # sentid_to_docid: Index of the original sentence - docid\n",
    "    sents, segment_ids, sentid_to_sec, sentid_to_docid = organize_text(documents)\n",
    "    \n",
    "    # sent_id for search\n",
    "    seg_to_sent, sent_to_seg, sents_ids_for_query = make_sents_ids_for_query(segment_ids)\n",
    "    \n",
    "    # mmp\n",
    "    # There is a bug that prevents search when all documents of one patient are placed in one matrix and searched at once.\n",
    "    # Divide queries into up to 100 queries.\n",
    "    for i in range(0, len(sents), interval):\n",
    "        sents_split = sents[i:i+interval]\n",
    "        sents_ids_for_query_split = sents_ids_for_query[i:i+interval]\n",
    "        pos_score, pos_cui, pos_sem, pos_term, found_concepts = call_mmp(sents_split, sents_ids_for_query_split, \n",
    "                                                         pos_score, pos_cui, pos_sem, pos_term, i, found_concepts)\n",
    "                \n",
    "    # sort entities\n",
    "    pos_cui, pos_sem, pos_term = sort_entities(pos_cui, pos_sem, pos_term, reverse=False)\n",
    "    \n",
    "    # If the same semantics or terms appear in succession, they should be treated as one entity and bundled.\n",
    "    pos_cui, pos_sem, pos_term = post_process_entities(sents, pos_cui, pos_sem, pos_term)\n",
    "    \n",
    "    # If sections overlap between entities, select the longest matching entity\n",
    "    pos_cui, pos_sem, pos_term = make_entity_no_dup_area(pos_cui, pos_sem, pos_term)\n",
    "    \n",
    "    outtext = []\n",
    "    \n",
    "    for s in range(len(sents)):\n",
    "        restored_sent_id = segment_ids[s]\n",
    "        doc_id      = sentid_to_docid[restored_sent_id]\n",
    "        sectionname = sentid_to_sec[restored_sent_id]\n",
    "        \n",
    "        line = str(s)+\"\\t\"+str(doc_id)+\"\\t\"+str(restored_sent_id)+\"\\t\"+str(sectionname)+\"\\t\"+str(sents[s])\n",
    "        outtext.append(line)\n",
    "    \n",
    "    output_entities(output_directory, outtext, pos_cui, pos_sem, pos_term)\n",
    "    \n",
    "    \n",
    "    file = open(output_dir_entall, \"w\")\n",
    "    file.write(\"\\n\".join(found_concepts))\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "# Prepare the text in a format suitable for mmp search\n",
    "def organize_text(documents):\n",
    "    sent_count = 0\n",
    "    \n",
    "    sents = []\n",
    "    sent_ids = []\n",
    "    sentid_to_sec   = {}\n",
    "    sentid_to_docid = {}\n",
    "    date = \"\"\n",
    "    sent_id = 0\n",
    "    doc_id = 0\n",
    "    for doc in documents:\n",
    "        for sec in doc:\n",
    "            if sec==\"date\":\n",
    "                doc[sec] = [doc[sec]]\n",
    "            \n",
    "            for sent in doc[sec]:\n",
    "                split_sents, segment_ids = make_short_sents(text=sent, seg_id=sent_id, linechar_limit=100)                \n",
    "                sents = sents + split_sents\n",
    "                sentid_to_sec[sent_id]   = sec\n",
    "                sentid_to_docid[sent_id] = doc_id\n",
    "                sent_ids = sent_ids + segment_ids\n",
    "                sent_id = sent_id + 1\n",
    "        \n",
    "        doc_id = doc_id + 1\n",
    "    \n",
    "    return sents, sent_ids, sentid_to_sec, sentid_to_docid\n",
    "    \n",
    "\n",
    "# There is a bug in MetaMap that prevents analysis altogether when a single sentence exceeds about 200 words.\n",
    "# The original text needs to be segmented into multiple sentences.\n",
    "def make_short_sents(text, seg_id=0, linechar_limit=100):\n",
    "    split_sents = []\n",
    "    segment_ids = []\n",
    "    tokens = text.split(\" \")\n",
    "    sent_tmp = []\n",
    "    for t in range(len(tokens)):\n",
    "        if (len(\" \".join(sent_tmp))+1+len(tokens[t]))>linechar_limit:\n",
    "            split_sents.append(\" \".join(sent_tmp))\n",
    "            sent_tmp = []\n",
    "        sent_tmp.append(tokens[t])\n",
    "        \n",
    "    if len(sent_tmp)>0:\n",
    "        split_sents.append(\" \".join(sent_tmp))\n",
    "        sent_tmp = []\n",
    "    \n",
    "    segment_ids = [seg_id]*len(split_sents)\n",
    "    \n",
    "    return split_sents, segment_ids\n",
    "\n",
    "\n",
    "\n",
    "def output_entities(output_directory, outtext, pos_cui, pos_sem, pos_term):\n",
    "    outtext.append(\"\\n\")\n",
    "    outtext.append(\"#### entities (문장번호/begin/end) ####\")\n",
    "    if len(pos_cui)>0:\n",
    "        for key in pos_cui:\n",
    "            outtext.append(str(key)+\"\\t\"+str(pos_cui[key])+ \"\\t\"+str(pos_sem[key])+\"\\t\"+str(pos_term[key]))\n",
    "    else:\n",
    "        outtext.append(\"None\")\n",
    "    \n",
    "    file = open(output_directory, \"w\")\n",
    "    file.write(\"\\n\".join(outtext)+\"\\n\\n\")\n",
    "    file.close()\n",
    "        \n",
    "\n",
    "\n",
    "# If the same semantics or terms appear in succession, they should be treated as one entity.\n",
    "# e.g)\n",
    "# key:  0/4/5  /  C2603358  /  [clna]  /  R prime\n",
    "# key:  0/10/15  /  C5203670  /  [dsyn]  /  COVID-19\n",
    "# key:  0/17/19  /  C5203670  /  [dsyn]  /  COVID-19\n",
    "import re\n",
    "def post_process_entities(sents, pos_cui, pos_sem, pos_term):\n",
    "    pos_cui_p = {}\n",
    "    pos_sem_p = {}\n",
    "    pos_term_p = {}\n",
    "    \n",
    "    last_linenum = \"\"\n",
    "    last_sem = \"\"\n",
    "    last_term = \"\"\n",
    "    last_end = \"\"\n",
    "    \n",
    "    begin_cands = []\n",
    "    end_cands = []\n",
    "    \n",
    "    count = 0\n",
    "    for key in pos_cui:\n",
    "        if count==0:\n",
    "            last_linenum = recover_to_int(key.split(\"/\")[0])\n",
    "            begin_cands.append(recover_to_int(key.split(\"/\")[1]))\n",
    "            end_cands.append(recover_to_int(key.split(\"/\")[2]))\n",
    "            last_end = recover_to_int(key.split(\"/\")[2])\n",
    "            \n",
    "            last_cui = pos_cui[key]\n",
    "            last_sem = pos_sem[key]\n",
    "            last_term = pos_term[key]\n",
    "            count = count + 1\n",
    "            continue\n",
    "        \n",
    "        begin_current = recover_to_int(key.split(\"/\")[1])\n",
    "        end_current   = recover_to_int(key.split(\"/\")[2])\n",
    "        \n",
    "        # connect entities only when the length between entities is within 5\n",
    "        ent_distance  = begin_current - last_end\n",
    "        linenum       = recover_to_int(key.split(\"/\")[0])        \n",
    "        text = sents[linenum][begin_current:end_current]\n",
    "                    \n",
    "        if recover_to_int(key.split(\"/\")[0])==last_linenum and last_cui==pos_cui[key] and last_sem==pos_sem[key] \\\n",
    "           and last_term==pos_term[key] and ent_distance<5:\n",
    "            begin_cands.append(begin_current)\n",
    "            end_cands.append(end_current)\n",
    "            \n",
    "        # When conditions change, previously collected entities are merged into one entity.\n",
    "        else:\n",
    "            new_start = min(begin_cands)\n",
    "            new_end = max(end_cands)\n",
    "            \n",
    "            zeros_d = \"\".join(['0']*(8-len(str(last_linenum))))\n",
    "            zeros_s = \"\".join(['0']*(8-len(str(new_start))))\n",
    "            zeros_e = \"\".join(['0']*(8-len(str(new_end))))\n",
    "            \n",
    "            newkey = zeros_d+str(last_linenum)+\"/\"+zeros_s+str(new_start)+\"/\"+zeros_e+str(new_end)\n",
    "            \n",
    "            pos_cui_p[newkey] = last_cui\n",
    "            pos_sem_p[newkey] = last_sem\n",
    "            pos_term_p[newkey] = last_term\n",
    "            \n",
    "            # prepare for next\n",
    "            begin_cands = []\n",
    "            end_cands = []\n",
    "            begin_cands.append(begin_current)\n",
    "            end_cands.append(end_current)        \n",
    "    \n",
    "        last_linenum = recover_to_int(key.split(\"/\")[0])\n",
    "        last_end = recover_to_int(key.split(\"/\")[2])\n",
    "        \n",
    "        last_cui = pos_cui[key]\n",
    "        last_sem = pos_sem[key]\n",
    "        last_term = pos_term[key]\n",
    "        \n",
    "        count = count + 1\n",
    "    \n",
    "    if len(begin_cands)>0:\n",
    "        new_start = min(begin_cands)\n",
    "        new_end = max(end_cands)\n",
    "            \n",
    "        zeros_d = \"\".join(['0']*(8-len(str(last_linenum))))\n",
    "        zeros_s = \"\".join(['0']*(8-len(str(new_start))))\n",
    "        zeros_e = \"\".join(['0']*(8-len(str(new_end))))\n",
    "        newkey = zeros_d+str(last_linenum)+\"/\"+zeros_s+str(new_start)+\"/\"+zeros_e+str(new_end)\n",
    "        \n",
    "        pos_cui_p[newkey] = last_cui\n",
    "        pos_sem_p[newkey] = last_sem\n",
    "        pos_term_p[newkey] = last_term\n",
    "\n",
    "    return pos_cui_p, pos_sem_p, pos_term_p\n",
    "\n",
    "\n",
    "def file_loop(datapath_input):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    department = datapath_input.split(\"/\")[-1]\n",
    "    \n",
    "    groups_inputs = glob.glob(datapath_input+\"/*\")\n",
    "    groups_inputs.sort()\n",
    "\n",
    "    for g in range(0, len(groups_inputs)):\n",
    "        files_input = glob.glob(groups_inputs[g]+\"/*.txt\")\n",
    "        files_input.sort()\n",
    "        group = groups_inputs[g].split(\"/\")[-1]\n",
    "\n",
    "        for f in range(0, len(files_input)):\n",
    "            filename = files_input[f].split(\"/\")[-1]\n",
    "            output_path = \"./data/02_entities/\"+str(department)+\"/\"+str(group)\n",
    "            output_path_file = output_path+\"/\"+filename[:-len(\".txt\")]+\"_data.txt\"\n",
    "            output_path_ents = output_path+\"/\"+filename[:-len(\".txt\")]+\"_ents.txt\"\n",
    "            print(output_path, \" / \", f, \"/\",len(files_input),\"/\",filename, \"...\", g,\"/\",len(groups_inputs))\n",
    "            if not os.path.exists(output_path):\n",
    "                os.makedirs(output_path)\n",
    "            \n",
    "            documents = read_patient_documents(files_input[f])\n",
    "            doc_iter(documents = documents, \n",
    "                     output_directory = output_path_file, \n",
    "                     output_dir_entall = output_path_ents)\n",
    "            \n",
    "    end = time.time()\n",
    "    print(\"run time: \", str(end-start_time))\n",
    "    \n",
    "    file = open(\"runtime.txt\", \"a\")\n",
    "    file.write(str(datapath_input)+\"\\t\"+str(end-start_time)+\"\\n\")\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "##################################################\n",
    "## Finetuning data\n",
    "##################################################\n",
    "def file_loop_targeted(department, mode=\"train\"):\n",
    "    print(\"department: \", department, \", mode: \", str(mode))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    def get_heldout_paths(heldoutlist, department):\n",
    "        file = open(heldoutlist, \"r\")\n",
    "        lines = file.readlines()\n",
    "        file.close()\n",
    "\n",
    "        input_paths = []\n",
    "        for l in range(len(lines)):\n",
    "            pt_path = \"./data/visits_2011to2020/\"+str(department)+\"/\"+\"/\".join(lines[l].strip().split(\"/\")[-2:])\n",
    "            \n",
    "            input_paths.append(pt_path)\n",
    "\n",
    "        print(\"len(input_paths): \", len(input_paths))\n",
    "\n",
    "        return input_paths\n",
    "\n",
    "    \n",
    "    if mode==\"train\":\n",
    "        heldoutlist = \"../preprocessing/01_data4finetune/pts_SNUH_visit_2011to2020_heldout_train.txt\"\n",
    "    elif mode==\"test\":\n",
    "        heldoutlist = \"../preprocessing/01_data4finetune/pts_SNUH_visit_2011to2020_heldout_test.txt\"\n",
    "    else:\n",
    "        print(\"need to define mode\")\n",
    "        return\n",
    "        \n",
    "    datapath_input = get_heldout_paths(heldoutlist=heldoutlist, department=department)\n",
    "    \n",
    "    # run\n",
    "    for f in range(0, len(datapath_input)):\n",
    "        filename = datapath_input[f].split(\"/\")[-1]\n",
    "        output_path = \"./data/02_entities_task7/\"+str(mode)+\"/\"+str(department)\n",
    "        output_path_file = output_path+\"/\"+filename[:-len(\".txt\")]+\"_data.txt\"\n",
    "        output_path_ents = output_path+\"/\"+filename[:-len(\".txt\")]+\"_ents.txt\"\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "        \n",
    "        if os.path.isfile(datapath_input[f]):\n",
    "            \"\"\"File exists\"\"\"\n",
    "        else:\n",
    "            \"\"\"File doesnot exist\"\"\"\n",
    "            continue\n",
    "            \n",
    "        print(output_path, \" / \", f, \"/\",len(datapath_input),\"/\",filename)\n",
    "        documents = read_patient_documents(datapath_input[f])\n",
    "        doc_iter(documents = documents, \n",
    "                 output_directory = output_path_file, \n",
    "                 output_dir_entall = output_path_ents)\n",
    "                \n",
    "    end = time.time()\n",
    "    print(\"run time: \", str(end-start_time))\n",
    "    \n",
    "    file = open(\"runtime.txt\", \"a\")\n",
    "    file.write(str(department)+\"\\t\"+str(mode)+\"\\t\"+str(end-start_time)+\"\\n\")\n",
    "    file.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ebe61d",
   "metadata": {},
   "source": [
    "# Run code\n",
    "- Output\n",
    "    - ./data/02_entities_task7/train/department/patient_fakeid.txt\n",
    "    - ./data/02_entities_task7/test/department/patient_fakeid.txt    \n",
    "\n",
    "\n",
    "- Departments:\n",
    "    - 감염내과 (Infectious Diseases)\n",
    "    - 내분비대사내과 (Endocrinology and Metabolism)\n",
    "    - 류마티스내과 (Rheumatology)\n",
    "    - 소화기내과 (Gastroenterology)\n",
    "    - 순환기내과 (Cardiology)\n",
    "    - 신장내과 (Nephrology)\n",
    "    - 알레르기내과 (Allergy and Immunology)\n",
    "    - 호흡기내과 (Pulmonology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a517256",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loop_targeted(department=\"감염내과\", mode=\"train\")\n",
    "file_loop_targeted(department=\"내분비대사내과\", mode=\"train\")\n",
    "file_loop_targeted(department=\"류마티스내과\", mode=\"train\")\n",
    "file_loop_targeted(department=\"소화기내과\", mode=\"train\")\n",
    "file_loop_targeted(department=\"순환기내과\", mode=\"train\")\n",
    "file_loop_targeted(department=\"신장내과\", mode=\"train\")\n",
    "file_loop_targeted(department=\"알레르기내과\", mode=\"train\")\n",
    "file_loop_targeted(department=\"호흡기내과\", mode=\"train\")\n",
    "\n",
    "\n",
    "file_loop_targeted(department=\"감염내과\", mode=\"test\")\n",
    "file_loop_targeted(department=\"내분비대사내과\", mode=\"test\")\n",
    "file_loop_targeted(department=\"류마티스내과\", mode=\"test\")\n",
    "file_loop_targeted(department=\"소화기내과\", mode=\"test\")\n",
    "file_loop_targeted(department=\"순환기내과\", mode=\"test\")\n",
    "file_loop_targeted(department=\"신장내과\", mode=\"test\")\n",
    "file_loop_targeted(department=\"알레르기내과\", mode=\"test\")\n",
    "file_loop_targeted(department=\"호흡기내과\", mode=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148820a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
