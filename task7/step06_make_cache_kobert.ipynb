{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f11145",
   "metadata": {},
   "source": [
    "# Make cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentencepiece\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import six\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tokenization import BertTokenizer\n",
    "import tokenization as tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0505025",
   "metadata": {},
   "source": [
    "## 4. write_instance_to_example_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a65b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance_ext(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids, mask_location):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        self.mask_location = mask_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instance_to_example_files(instances, \n",
    "                                    tokenizer, \n",
    "                                    vocab_words,\n",
    "                                    max_seq_length,\n",
    "                                    max_predictions_per_seq, \n",
    "                                    outfilename):\n",
    "    \n",
    "    features = []\n",
    "        \n",
    "    pad_id = vocab_words.w_to_i[\"[PAD]\"]\n",
    "    \n",
    "    for (inst_index, instance) in enumerate(instances):\n",
    "        input_ids = []\n",
    "        for l in range(len(instance.tokens)):\n",
    "            tokentmp = instance.tokens[l]\n",
    "            input_id = vocab_words.w_to_i[tokentmp]\n",
    "            input_ids.append(input_id)\n",
    "    \n",
    "        input_mask = [1] * len(input_ids)\n",
    "        segment_ids = list(instance.segment_ids)\n",
    "        \n",
    "        label_ids = list(instance.label_ids) \n",
    "        mask_location = list(instance.mask_location) \n",
    "     \n",
    "        if segment_ids[-1]==0:\n",
    "            seg_id=1\n",
    "        else:\n",
    "            seg_id=0\n",
    "        \n",
    "        assert len(input_ids) <= max_seq_length\n",
    "\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(pad_id)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(seg_id)\n",
    "            label_ids.append(-1)\n",
    "            mask_location.append(0)\n",
    "            \n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(mask_location) == max_seq_length\n",
    "        \n",
    "        features.append(\n",
    "            TrainingInstance_ext(\n",
    "                input_ids = input_ids,\n",
    "                input_mask = input_mask, \n",
    "                segment_ids = segment_ids,\n",
    "                label_ids = label_ids,\n",
    "                mask_location = mask_location,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    if len(features)!=0:\n",
    "#         print(\"outfilename: \", outfilename)\n",
    "        with open(outfilename, 'wb') as output:\n",
    "            pickle.dump(features, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302adda0",
   "metadata": {},
   "source": [
    "## 2. create_instances_from_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance_ext_tmp(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, tokens, segment_ids, label_ids, mask_location):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        self.mask_location = mask_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827234c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_samples(targetfile, tokenizer, do_lower_case, label_map, vocab_words):\n",
    "    labels = []\n",
    "    samples = []\n",
    "    \n",
    "    f = open(targetfile, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    for l in range(len(lines)):\n",
    "        line = lines[l].strip(\"\\n\")\n",
    "        filename_docidx = line.split(\"\\t\")[0]\n",
    "        locinfo         = line.split(\"\\t\")[1]\n",
    "        entinfo         = line.split(\"\\t\")[2]\n",
    "        department      = line.split(\"\\t\")[3]\n",
    "        content         = line.split(\"\\t\")[4]\n",
    "        \n",
    "        sample_tmp = []\n",
    "        contents = content.split(\"[MASK]\")\n",
    "        for c in range(len(contents)):\n",
    "            content_tmp = contents[c]\n",
    "            if do_lower_case==True:\n",
    "                content_tmp = content_tmp.lower()\n",
    "            \n",
    "            token_ids_tmp = tokenizer.Encode(content_tmp) # kobert\n",
    "            content_tmp = []\n",
    "            for t in range(len(token_ids_tmp)):\n",
    "                token = vocab_words.i_to_w[token_ids_tmp[t]]\n",
    "                content_tmp.append(token)\n",
    "            \n",
    "            sample_tmp = sample_tmp + content_tmp +[\"[MASK]\"]\n",
    "        \n",
    "        sample_tmp = sample_tmp[:-1] # 마지막에 붙은 [MASK] 제거\n",
    "        samples.append(sample_tmp)\n",
    "        \n",
    "        label = label_map[entinfo]\n",
    "        labels.append(label)        \n",
    "    \n",
    "    return samples, labels\n",
    "\n",
    "def read_labels(label_path=\"./data/labels.txt\"):\n",
    "    label_map = {}\n",
    "    file = open(label_path, \"r\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    \n",
    "    for l in range(len(lines)):\n",
    "        line = lines[l].strip()\n",
    "        label_map[line.replace(\",\", \"/\")] = l\n",
    "    \n",
    "    return label_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49dc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_seq_sides(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    if len(tokens_a)+len(tokens_b) <= max_num_tokens:\n",
    "        return tokens_a, tokens_b\n",
    "    \n",
    "    while True:\n",
    "        if len(tokens_a)>len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "        \n",
    "        if len(tokens_a)+len(tokens_b)<=max_num_tokens:\n",
    "            return tokens_a, tokens_b\n",
    "\n",
    "def create_instances_from_document(input_file, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, rng, do_lower_case, vocab_words, tokenizer, mode):\n",
    "    \n",
    "    instances = []\n",
    "    ########### tokenize all documents ###########\n",
    "    label_map = read_labels(label_path=\"./data/labels.txt\")\n",
    "    samples, labels = read_samples(input_file, tokenizer, do_lower_case, label_map, vocab_words)\n",
    "    \n",
    "    for p in range(len(samples)):\n",
    "        tokens = samples[p]\n",
    "        label = labels[p]\n",
    "        \n",
    "        mask_idx = 0\n",
    "        for t in range(len(tokens)):\n",
    "            if tokens[t]==\"[MASK]\" or tokens[t]==\"[mask]\":\n",
    "                mask_idx = t\n",
    "                break\n",
    "        \n",
    "        tokens_left  = tokens[:mask_idx]\n",
    "        tokens_right = tokens[mask_idx+1:]\n",
    "        tokens_left, tokens_right = truncate_seq_sides(tokens_left, tokens_right, max_seq_length-3, rng) # [CLS], [MASK], [SEP]\n",
    "        tokens = []\n",
    "        tokens = tokens_left+[\"[MASK]\"]+tokens_right\n",
    "        \n",
    "        tokens = [\"[CLS]\"]+tokens+[\"[SEP]\"]\n",
    "        segment_ids = [0]*len(tokens)\n",
    "        mask_location = [0]*len(tokens)\n",
    "        label_ids  = [-1]*len(tokens)\n",
    "        \n",
    "        if len(tokens)>max_seq_length:\n",
    "            print(\"len(tokens): \", len(tokens))\n",
    "            print(\"tokens: \", tokens)\n",
    "            print(\"label_ids: \", label_ids)\n",
    "        \n",
    "        for t in range(len(tokens)):\n",
    "            if tokens[t]==\"[MASK]\" or tokens[t]==\"[mask]\":\n",
    "                mask_location[t]=1\n",
    "                label_ids[t]=label\n",
    "                mask_idx = t\n",
    "                break\n",
    "        \n",
    "        assert sum(mask_location)==1\n",
    "        assert len(tokens) <= max_seq_length\n",
    "        assert len(tokens)==len(segment_ids)\n",
    "        assert len(segment_ids)==len(label_ids)\n",
    "\n",
    "        instance = TrainingInstance_ext_tmp(\n",
    "                tokens = tokens,\n",
    "                segment_ids = segment_ids,\n",
    "                label_ids = label_ids, \n",
    "                mask_location = mask_location,\n",
    "            )\n",
    "        instances.append(instance)\n",
    "        \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ebc3f",
   "metadata": {},
   "source": [
    "## 1. Create Training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(input_file, tokenizer, vocab_words, max_seq_length,\n",
    "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                              max_predictions_per_seq, rng, do_lower_case, mode):\n",
    "    \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "    instances = []\n",
    "    #for _ in range(dupe_factor):\n",
    "    instances.extend(\n",
    "        create_instances_from_document(\n",
    "            input_file, max_seq_length, short_seq_prob,\n",
    "            masked_lm_prob, max_predictions_per_seq, rng, do_lower_case, \n",
    "            vocab_words, tokenizer, mode))\n",
    "        \n",
    "    return instances\n",
    "\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529cf20",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    # required\n",
    "    input_files, \n",
    "    vocab_file, \n",
    "    outdir,\n",
    "    mode, \n",
    "    spmodel,\n",
    "    \n",
    "    # optional\n",
    "    do_lower_case=True, \n",
    "    max_seq_length=512, \n",
    "    max_predictions_per_seq = 20, \n",
    "    random_seed=12345, \n",
    "    dupe_factor = 1,\n",
    "    masked_lm_prob = 0.15,\n",
    "    short_seq_prob = 0.1,\n",
    "    ):\n",
    "    \n",
    "    \n",
    "    # vocab_words\n",
    "    class Vocab_words(object):\n",
    "        def __init__(self, vocab_file):\n",
    "            self.i_to_w = {}\n",
    "            self.w_to_i = {}\n",
    "            self.getvocab(vocab_file)\n",
    "    #         print(self.w_to_i)\n",
    "\n",
    "        def getvocab(self, vocab_file):\n",
    "            f = open(vocab_file, 'r')\n",
    "            lines = f.readlines()\n",
    "            for l in range(len(lines)):\n",
    "                term = lines[l].strip(\"\\n\")\n",
    "                term = convert_to_unicode(term)\n",
    "                self.i_to_w[int(l)] = term\n",
    "                self.w_to_i[term] = int(l)\n",
    "        \n",
    "    vocab_words = Vocab_words(vocab_file)\n",
    "    \n",
    "    # sptokenizer\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.load(spmodel)\n",
    "    \n",
    "    print(\"max_predictions_per_seq: \", max_predictions_per_seq)\n",
    "    \n",
    "    randseed = random.randint(1, 1000)\n",
    "    print(\"randseed: \", randseed)\n",
    "    rng = random.Random(randseed)\n",
    "    \n",
    "    print(\"len(input_files): \", len(input_files))\n",
    "    \n",
    "    for i in range(0, len(input_files), 1):\n",
    "        if i%100==0:\n",
    "            print(str(i)+\"/\"+str(len(input_files)))\n",
    "        instances = create_training_instances(\n",
    "                        input_files[i], tokenizer, vocab_words, max_seq_length,\n",
    "                        dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                        max_predictions_per_seq, rng, do_lower_case, mode)\n",
    "        \n",
    "        filename = input_files[i].split(\"/\")[-1]\n",
    "        filename = filename.split(\".\")[0]+\".cache\"\n",
    "        \n",
    "        write_instance_to_example_files(instances=instances, \n",
    "                                    tokenizer=tokenizer, \n",
    "                                    vocab_words=vocab_words,\n",
    "                                    max_seq_length=max_seq_length,\n",
    "                                    max_predictions_per_seq=max_predictions_per_seq, \n",
    "                                    outfilename=outdir+\"/\"+filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12926e5f",
   "metadata": {},
   "source": [
    "# Generate Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c556931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f31043",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Generating Features...\")\n",
    "\n",
    "data_dirs = [\n",
    "    \"./data/05_samples/train\",\n",
    "    \"./data/05_samples/test\",\n",
    "]\n",
    "\n",
    "output_paths = [\n",
    "    \"kobert\"\n",
    "]\n",
    "vocab_paths = [\n",
    "    \"../otherberts/KoBERT/models\"\n",
    "]\n",
    "\n",
    "lowercase = [False]\n",
    "\n",
    "for d in range(len(data_dirs)):\n",
    "    mode = data_dirs[d].split(\"/\")[-1]\n",
    "    \n",
    "    for i in range(len(output_paths)):\n",
    "        print(\"data_dirs[d]: \", data_dirs[d])\n",
    "        \n",
    "        out_directory = \"./cache/\"+str(output_paths[i])+\"/\"+str(mode)\n",
    "        print(\"out_directory: \", out_directory)\n",
    "        if not os.path.exists(out_directory):\n",
    "            os.makedirs(out_directory)\n",
    "        \n",
    "        vocab_path=vocab_paths[i]\n",
    "        \n",
    "        input_files = glob.glob(data_dirs[d]+\".txt\")\n",
    "        input_files.sort()\n",
    "        \n",
    "        main(\n",
    "            input_files = input_files, \n",
    "            outdir = out_directory,\n",
    "            mode = mode,\n",
    "            \n",
    "            #sentence piece\n",
    "            #https://skt-lsl-nlp-model.s3.amazonaws.com/KoBERT/tokenizers/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
    "            vocab_file = vocab_paths[i]+'/vocab.txt',\n",
    "            spmodel = vocab_paths[i]+'/spiece.model',\n",
    "            \n",
    "            # optional\n",
    "            do_lower_case = lowercase[i],\n",
    "            max_seq_length = 512, \n",
    "        )\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b1d073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
