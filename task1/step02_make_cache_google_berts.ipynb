{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f11145",
   "metadata": {},
   "source": [
    "# Make cache (BERTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import six\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "import glob\n",
    "\n",
    "from tokenization import BertTokenizer\n",
    "import tokenization as tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0505025",
   "metadata": {},
   "source": [
    "## 4. write_instance_to_example_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a65b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, is_entailed):\n",
    "        self.input_ids=input_ids\n",
    "        self.input_mask=input_mask\n",
    "        self.segment_ids=segment_ids\n",
    "        self.is_entailed=is_entailed        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance 를 exaple cinfe로 작성\n",
    "def write_instance_to_example_files(instances, \n",
    "                                    tokenizer, \n",
    "                                    vocab_words,\n",
    "                                    max_seq_length,\n",
    "                                    outfilename):\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for (inst_index, instance) in enumerate(instances):\n",
    "        \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)        \n",
    "        input_mask = [1] * len(input_ids)\n",
    "        segment_ids = list(instance.segment_ids)\n",
    "        assert len(input_ids) <= max_seq_length\n",
    "\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        \n",
    "        is_entailed = [0]*max_seq_length\n",
    "        is_entailed[0] = 1 if instance.is_entailed else 0\n",
    "        \n",
    "        features.append(\n",
    "            TrainingInstance(\n",
    "                input_ids=input_ids, \n",
    "                input_mask=input_mask, \n",
    "                segment_ids=segment_ids, \n",
    "                is_entailed=is_entailed\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    if len(features)!=0:\n",
    "#         print(\"outfilename: \", outfilename)\n",
    "        with open(outfilename, 'wb') as output:\n",
    "            pickle.dump(features, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "\n",
    "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "        assert len(trunc_tokens) >= 1\n",
    "\n",
    "        if rng.random() < 0.5:\n",
    "            del trunc_tokens[0]\n",
    "        else:\n",
    "            trunc_tokens.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302adda0",
   "metadata": {},
   "source": [
    "## 2. create_instances_from_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance_tmp(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, tokens, segment_ids, is_entailed):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_entailed = is_entailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49dc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(\n",
    "    samples, max_seq_length, rng, vocab_words, tokenizer, mode):\n",
    "    \n",
    "    \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
    "    instances = []\n",
    "    \n",
    "    max_seq_length = max_seq_length-3 # [CLS], [SEP], [SEP]\n",
    "    \n",
    "    for i in range(len(samples)):\n",
    "        record = samples[i]\n",
    "        \n",
    "        label = record[3]\n",
    "        is_entailed = True if label==\"1\" else False \n",
    "        \n",
    "        tokens_a = record[4]\n",
    "        tokens_b = record[5]\n",
    "        \n",
    "        if mode==\"train\":\n",
    "            truncate_seq_pair(tokens_a, tokens_b, max_seq_length, rng)\n",
    "        elif mode==\"test\":\n",
    "            tokens_a = tokens_a[:max_seq_length//2]\n",
    "            tokens_b = tokens_b[:max_seq_length//2]\n",
    "\n",
    "        \n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "        \n",
    "        instance = TrainingInstance_tmp(\n",
    "            tokens=tokens,\n",
    "            segment_ids=segment_ids,\n",
    "            is_entailed=is_entailed)\n",
    "        instances.append(instance)\n",
    "        \n",
    "        \n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ebc3f",
   "metadata": {},
   "source": [
    "## 1. Create Training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(samples, tokenizer, vocab_words, max_seq_length, rng, mode):\n",
    "    \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "    \n",
    "    instances = []\n",
    "    instances.extend(\n",
    "        create_instances_from_document(samples, max_seq_length, rng, vocab_words, tokenizer, mode))\n",
    "    \n",
    "    rng.shuffle(instances)\n",
    "    \n",
    "    return instances\n",
    "\n",
    "\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aeda22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(targetfile, tokenizer, version, startline, endline, do_lower_case):\n",
    "    f = open(targetfile, \"r\")\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    lines = lines[startline:endline]\n",
    "    \n",
    "    stacks = []\n",
    "    for l in range(len(lines)):\n",
    "        if l%100==0:\n",
    "            print(l, \"/\", len(lines))\n",
    "        \n",
    "        try:\n",
    "            line = lines[l].split(\"\\t\")\n",
    "            if len(line)<4:\n",
    "                continue\n",
    "        \n",
    "            cate = line[0]   # data category\n",
    "            ptnum1 = line[1] # ptnum1\n",
    "            ptnum2 = line[2] # ptnum2\n",
    "            label = line[3]  # label\n",
    "            \n",
    "        except:\n",
    "            print(\"line: \", line)\n",
    "            \n",
    "        if do_lower_case==True:\n",
    "            content1 = tokenizer.tokenize(line[4].lower())\n",
    "            content2 = tokenizer.tokenize(line[5].lower())\n",
    "        \n",
    "        else:\n",
    "            content1 = tokenizer.tokenize(line[4])\n",
    "            content2 = tokenizer.tokenize(line[5])\n",
    "            \n",
    "        record = []\n",
    "        record.append(cate)\n",
    "        record.append(ptnum1)\n",
    "        record.append(ptnum2)\n",
    "        record.append(label)\n",
    "        record.append(content1)\n",
    "        record.append(content2)\n",
    "        \n",
    "        stacks.append(record)\n",
    "    return stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529cf20",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    # required\n",
    "    input_file, \n",
    "    vocab_file,\n",
    "    version,\n",
    "    outdir,\n",
    "    mode,\n",
    "    \n",
    "    # optional\n",
    "    do_lower_case=True, \n",
    "    max_seq_length=512, \n",
    "    random_seed=12345, \n",
    "    ):\n",
    "    \n",
    "    print(\"vocab_file: \", vocab_file)\n",
    "    tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case, max_len=max_seq_length)\n",
    "    vocab_words = list(tokenizer.vocab.keys())\n",
    "    print(\"len(vocab_words): \", len(vocab_words))\n",
    "\n",
    "    \n",
    "    randseed = random.randint(1, 1000)\n",
    "    print(\"randseed: \", randseed)\n",
    "    rng = random.Random(randseed)\n",
    "    \n",
    "    f = open(input_file, \"r\")\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    print(\"generating...\")\n",
    "    print(\"len(lines): \", len(lines))\n",
    "    jump = 1000\n",
    "    for d in range(0, len(lines), jump):\n",
    "        s = d\n",
    "        e = d+jump\n",
    "        if e>len(lines):\n",
    "            e = len(lines)\n",
    "        print(s, \"~\", e, \"/\", len(lines))\n",
    "        samples = read_dataset(input_file, tokenizer, version, startline=s, endline=e, do_lower_case=do_lower_case)\n",
    "        \n",
    "        \n",
    "        filename = \"\".join([\"0\"]*(8-len(str(s))))+str(s)+\".cache\"\n",
    "        \n",
    "        instances = create_training_instances(samples, tokenizer, vocab_words, max_seq_length, rng, mode)\n",
    "        \n",
    "        write_instance_to_example_files(\n",
    "                        instances=instances, \n",
    "                        tokenizer=tokenizer, \n",
    "                        vocab_words=vocab_words,\n",
    "                        max_seq_length=max_seq_length,\n",
    "                        outfilename=outdir+\"/\"+filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12926e5f",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951fc4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "if not os.path.exists(\"./cache\"):\n",
    "    os.makedirs(\"./cache\")\n",
    "    \n",
    "print(\"Generating Features...\")\n",
    "\n",
    "versions = [\n",
    "    \"bertbase_cased\",\n",
    "    \"mbert_cased\", \n",
    "    \"biobert\"\n",
    "]\n",
    "\n",
    "datapath = [\"./data/train.txt\", \"./data/test.txt\"]\n",
    "\n",
    "vocab_paths = [\n",
    "        \"../otherberts/bertbase_cased\",\n",
    "        \"../otherberts/mbert_cased\",\n",
    "        \"../otherberts/bioBERT/biobert_v1.1_pubmed\",    \n",
    "]\n",
    "\n",
    "do_lower_cases = [False, False, False]\n",
    "\n",
    "assert len(versions)==len(vocab_paths)\n",
    "assert len(versions)==len(do_lower_cases)\n",
    "\n",
    "# folder loop\n",
    "for t in range(len(versions)):\n",
    "    print(\"versions: \", versions[t])\n",
    "    out_directory = \"./cache/\"+str(versions[t])+\"/\"    \n",
    "    if not os.path.exists(out_directory):\n",
    "        os.makedirs(out_directory)\n",
    "\n",
    "    # sample loop\n",
    "    for s in range(len(datapath)):\n",
    "        print(\"datapath[s]: \", datapath[s])\n",
    "        datapathname = datapath[s].split(\"/\")[-1].split(\".\")[0]\n",
    "        out_directory = \"./cache/\"+str(versions[t])+\"/\"+str(datapathname)\n",
    "        if not os.path.exists(out_directory):\n",
    "            os.makedirs(out_directory)\n",
    "        \n",
    "        if \"train\" in datapath[s]:\n",
    "            mode=\"train\"\n",
    "        else:\n",
    "            mode=\"test\"\n",
    "            \n",
    "        main(\n",
    "            # required\n",
    "            input_file = datapath[s], \n",
    "            outdir = out_directory,\n",
    "            \n",
    "            version = versions[t],\n",
    "\n",
    "            vocab_file = vocab_paths[t]+'/vocab.txt',\n",
    "            mode=mode,\n",
    "            \n",
    "            # optional\n",
    "            do_lower_case = do_lower_cases[t],\n",
    "            max_seq_length = 512, # must matching max_position_embeddings in json\n",
    "        )\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5eb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f4adcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
