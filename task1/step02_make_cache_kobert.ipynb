{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f11145",
   "metadata": {},
   "source": [
    "## KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentencepiece\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import six\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "import glob\n",
    "\n",
    "# kobert tokenizer\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0505025",
   "metadata": {},
   "source": [
    "## 4. write_instance_to_example_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a65b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, is_entailed):\n",
    "        self.input_ids=input_ids\n",
    "        self.input_mask=input_mask\n",
    "        self.segment_ids=segment_ids\n",
    "        self.is_entailed=is_entailed        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instance_to_example_files(instances, \n",
    "                                    tokenizer, \n",
    "                                    vocab_words,\n",
    "                                    max_seq_length,\n",
    "                                    outfilename):\n",
    "    pad_id = vocab_words.w_to_i[\"[PAD]\"]\n",
    "    features = []\n",
    "    \n",
    "    for (inst_index, instance) in enumerate(instances):\n",
    "        input_ids = []\n",
    "        for l in range(len(instance.tokens)):\n",
    "            tokentmp = instance.tokens[l]\n",
    "            input_id = vocab_words.w_to_i[tokentmp]\n",
    "            input_ids.append(input_id)\n",
    "            \n",
    "        input_mask = [1] * len(input_ids)\n",
    "        segment_ids = list(instance.segment_ids)\n",
    "        assert len(input_ids) <= max_seq_length\n",
    "\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(pad_id)\n",
    "            input_mask.append(0)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        \n",
    "        is_entailed = [0]*max_seq_length\n",
    "        is_entailed[0] = 1 if instance.is_entailed else 0\n",
    "        \n",
    "        features.append(\n",
    "            TrainingInstance(\n",
    "                input_ids=input_ids, \n",
    "                input_mask=input_mask, \n",
    "                segment_ids=segment_ids, \n",
    "                is_entailed=is_entailed\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    if len(features)!=0:\n",
    "        with open(outfilename, 'wb') as output:\n",
    "            pickle.dump(features, output, pickle.HIGHEST_PROTOCOL)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "\n",
    "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "        assert len(trunc_tokens) >= 1\n",
    "        \n",
    "        if rng.random() < 0.5:\n",
    "            del trunc_tokens[0]\n",
    "        else:\n",
    "            trunc_tokens.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302adda0",
   "metadata": {},
   "source": [
    "## 2. create_instances_from_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance_tmp(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, tokens, segment_ids, is_entailed):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.is_entailed = is_entailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c0e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_change(segment_flag):\n",
    "    if segment_flag==0:\n",
    "        segment_flag = 1\n",
    "    else:\n",
    "        segment_flag = 0\n",
    "    return segment_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49dc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(\n",
    "    samples, max_seq_length, rng, vocab_words, tokenizer, mode):\n",
    "    \n",
    "    \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
    "    instances = []\n",
    "    \n",
    "    max_seq_length = max_seq_length-3 # [CLS], [SEP], [SEP]\n",
    "    \n",
    "    for i in range(len(samples)):\n",
    "        record = samples[i]\n",
    "        \n",
    "        label = record[3]\n",
    "        is_entailed = True if label==\"1\" else False \n",
    "        \n",
    "        tokens_a = record[4]\n",
    "        tokens_b = record[5]\n",
    "        \n",
    "        if mode==\"train\":\n",
    "            truncate_seq_pair(tokens_a, tokens_b, max_seq_length, rng)\n",
    "        elif mode==\"test\":\n",
    "            tokens_a = tokens_a[:max_seq_length//2]\n",
    "            tokens_b = tokens_b[:max_seq_length//2]\n",
    "            \n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(0)\n",
    "\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "        \n",
    "        instance = TrainingInstance_tmp(\n",
    "            tokens=tokens,\n",
    "            segment_ids=segment_ids,\n",
    "            is_entailed=is_entailed)\n",
    "        instances.append(instance)\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ebc3f",
   "metadata": {},
   "source": [
    "## 1. Create Training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(samples, tokenizer, vocab_words, max_seq_length, rng, mode):\n",
    "    \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "    \n",
    "    instances = []\n",
    "    instances.extend(\n",
    "        create_instances_from_document(samples, max_seq_length, rng, vocab_words, tokenizer, mode))\n",
    "            \n",
    "    print(\"len(instances): \", len(instances))\n",
    "    rng.shuffle(instances)\n",
    "    \n",
    "    return instances\n",
    "\n",
    "    \n",
    "\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e6a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(record, tokenizer, vocab_words):\n",
    "    text = []\n",
    "    for a in range(len(record)):\n",
    "        line = record[a]\n",
    "        tokens = []\n",
    "        token_ids = tokenizer.Encode(line) # kobert\n",
    "        for t in range(len(token_ids)):\n",
    "            token = vocab_words.i_to_w[token_ids[t]]\n",
    "            \n",
    "            tokens.append(token)\n",
    "            \n",
    "        text = text + tokens\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(targetfile, tokenizer, version, startline, endline, do_lower_case, vocab_words):\n",
    "    f = open(targetfile, \"r\")\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    lines = lines[startline:endline]\n",
    "    \n",
    "    stacks = []\n",
    "    for l in range(len(lines)):\n",
    "        if l%100==0:\n",
    "            print(l, \"/\", len(lines))\n",
    "        \n",
    "        try:\n",
    "            line = lines[l].split(\"\\t\")\n",
    "            if len(line)<4:\n",
    "                continue\n",
    "        \n",
    "            cate = line[0]   # data category\n",
    "            ptnum1 = line[1] # ptnum1\n",
    "            ptnum2 = line[2] # ptnum2\n",
    "            label = line[3]  # label\n",
    "            \n",
    "        except:\n",
    "            print(\"line: \", line)\n",
    "            \n",
    "        if do_lower_case==True:\n",
    "            content1 = get_tokens([line[4].lower()], tokenizer, vocab_words)\n",
    "            content2 = get_tokens([line[5].lower()], tokenizer, vocab_words) \n",
    "            \n",
    "        else:\n",
    "            content1 = get_tokens([line[4]], tokenizer, vocab_words)\n",
    "            content2 = get_tokens([line[5]], tokenizer, vocab_words)\n",
    "            \n",
    "        \n",
    "        record = []\n",
    "        record.append(cate)\n",
    "        record.append(ptnum1)\n",
    "        record.append(ptnum2)\n",
    "        record.append(label)\n",
    "        record.append(content1)\n",
    "        record.append(content2)\n",
    "        \n",
    "        stacks.append(record)\n",
    "    return stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529cf20",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    # required\n",
    "    input_file, \n",
    "    vocab_file,\n",
    "    version,\n",
    "    outdir,\n",
    "    mode,\n",
    "    spmodel, \n",
    "    \n",
    "    # optional\n",
    "    do_lower_case=True, \n",
    "    max_seq_length=512, \n",
    "    random_seed=12345, \n",
    "    ):\n",
    "    \n",
    "    randseed = random.randint(1, 1000)\n",
    "    print(\"randseed: \", randseed)\n",
    "    rng = random.Random(randseed)\n",
    "\n",
    "    # vocab_words\n",
    "    class Vocab_words(object):\n",
    "        def __init__(self, vocab_file):\n",
    "            self.i_to_w = {}\n",
    "            self.w_to_i = {}\n",
    "            self.getvocab(vocab_file)\n",
    "    #         print(self.w_to_i)\n",
    "\n",
    "        def getvocab(self, vocab_file):\n",
    "            f = open(vocab_file, 'r')\n",
    "            lines = f.readlines()\n",
    "            for l in range(len(lines)):\n",
    "                term = lines[l].strip(\"\\n\")\n",
    "                term = convert_to_unicode(term)\n",
    "                self.i_to_w[int(l)] = term\n",
    "                self.w_to_i[term] = int(l)\n",
    "        \n",
    "    vocab_words = Vocab_words(vocab_file)\n",
    "    \n",
    "    # sptokenizer\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.load(spmodel)\n",
    "    \n",
    "    \n",
    "    f = open(input_file, \"r\")\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    print(\"generating...\")\n",
    "    print(\"len(lines): \", len(lines))\n",
    "    jump = 1000\n",
    "    for d in range(0, len(lines), jump):\n",
    "        s = d\n",
    "        e = d+jump\n",
    "        if e>len(lines):\n",
    "            e = len(lines)\n",
    "        print(s, \"~\", e, \"/\", len(lines))\n",
    "        samples = read_dataset(input_file, tokenizer, version, startline=s, endline=e, \n",
    "                               do_lower_case=do_lower_case, vocab_words=vocab_words)\n",
    "        \n",
    "        \n",
    "        filename = \"\".join([\"0\"]*(8-len(str(s))))+str(s)+\".cache\"\n",
    "        \n",
    "        instances = create_training_instances(samples, tokenizer, vocab_words, max_seq_length, rng, mode)\n",
    "        \n",
    "        write_instance_to_example_files(\n",
    "                        instances=instances, \n",
    "                        tokenizer=tokenizer, \n",
    "                        vocab_words=vocab_words,\n",
    "                        max_seq_length=max_seq_length,\n",
    "                        outfilename=outdir+\"/\"+filename)\n",
    "#         print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12926e5f",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951fc4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./cache\"):\n",
    "    os.makedirs(\"./cache\")\n",
    "    \n",
    "versions = [\n",
    "    \"kobert\"\n",
    "]\n",
    "\n",
    "datapath = [\"./data/train.txt\", \"./data/test.txt\"]\n",
    "\n",
    "vocab_paths = [\n",
    "    \"../otherberts/KoBERT/models\",    \n",
    "]\n",
    "\n",
    "do_lower_cases = [False]\n",
    "\n",
    "assert len(versions)==len(vocab_paths)\n",
    "assert len(versions)==len(do_lower_cases)\n",
    "\n",
    "# folder loop\n",
    "for t in range(len(versions)):\n",
    "    print(\"versions: \", versions[t])\n",
    "    out_directory = \"./cache/\"+str(versions[t])+\"/\"    \n",
    "    if not os.path.exists(out_directory):\n",
    "        os.makedirs(out_directory)\n",
    "\n",
    "    # sample loop\n",
    "    for s in range(len(datapath)):\n",
    "        print(\"datapath[s]: \", datapath[s])\n",
    "        datapathname = datapath[s].split(\"/\")[-1].split(\".\")[0]\n",
    "        out_directory = \"./cache/\"+str(versions[t])+\"/\"+str(datapathname)\n",
    "        if not os.path.exists(out_directory):\n",
    "            os.makedirs(out_directory)\n",
    "        \n",
    "        if \"train\" in datapath[s]:\n",
    "            mode=\"train\"\n",
    "        else:\n",
    "            mode=\"test\"\n",
    "            \n",
    "        main(\n",
    "            # required\n",
    "            input_file = datapath[s], \n",
    "            outdir = out_directory,\n",
    "            \n",
    "            version = versions[t],\n",
    "\n",
    "            #sentence piece\n",
    "            #https://skt-lsl-nlp-model.s3.amazonaws.com/KoBERT/tokenizers/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
    "            vocab_file = vocab_paths[t]+'/vocab.txt',\n",
    "            spmodel = vocab_paths[t]+'/spiece.model',\n",
    "\n",
    "            mode=mode,\n",
    "            # optional\n",
    "            do_lower_case = do_lower_cases[t],\n",
    "            max_seq_length = 512, \n",
    "        )\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a0e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ec4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454fa514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f886b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
