{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f11145",
   "metadata": {},
   "source": [
    "# Make cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959bbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentencepiece\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "import six\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tokenization import BertTokenizer\n",
    "import tokenization as tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0505025",
   "metadata": {},
   "source": [
    "## 4. write_instance_to_example_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a65b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance_ext(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids, start_pos, end_pos, doc_ids, sent_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        self.start_pos = start_pos \n",
    "        self.end_pos = end_pos\n",
    "        self.doc_ids = doc_ids\n",
    "        self.sent_ids = sent_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instance_to_example_files(instances, \n",
    "                                    tokenizer, \n",
    "                                    vocab_words,\n",
    "                                    max_seq_length,\n",
    "                                    max_predictions_per_seq, \n",
    "                                    outfilename):\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    pad_id = tokenizer.convert_tokens_to_ids([\"[PAD]\"])\n",
    "    pad_id = pad_id[0]\n",
    "    \n",
    "    for (inst_index, instance) in enumerate(instances):\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "    \n",
    "        input_mask = [1] * len(input_ids)\n",
    "        segment_ids = list(instance.segment_ids)\n",
    "        \n",
    "        label_ids = list(instance.label_ids) # label_id\n",
    "        start_pos = list(instance.start_pos) # label_id\n",
    "        end_pos = list(instance.end_pos) # label_id\n",
    "\n",
    "        doc_ids = list(instance.doc_ids)\n",
    "        sent_ids = list(instance.sent_ids)\n",
    "        \n",
    "        # segment_id\n",
    "        if segment_ids[-1]==0:\n",
    "            seg_id=1\n",
    "        else:\n",
    "            seg_id=0\n",
    "        \n",
    "        assert len(input_ids) <= max_seq_length\n",
    "\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(pad_id)\n",
    "            input_mask.append(0)       # attention mask\n",
    "            segment_ids.append(seg_id) # segment_id\n",
    "            label_ids.append(0)        # label\n",
    "            start_pos.append(0)        # label\n",
    "            end_pos.append(0)          # label\n",
    "            doc_ids.append(-1)\n",
    "            sent_ids.append(-1)\n",
    "            \n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(start_pos) == max_seq_length\n",
    "        assert len(end_pos) == max_seq_length\n",
    "\n",
    "\n",
    "        features.append(\n",
    "            TrainingInstance_ext(\n",
    "                input_ids = input_ids,\n",
    "                input_mask = input_mask, \n",
    "                segment_ids = segment_ids,\n",
    "                label_ids = label_ids,\n",
    "                start_pos = start_pos,\n",
    "                end_pos = end_pos, \n",
    "                doc_ids = doc_ids,\n",
    "                sent_ids = sent_ids\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    if len(features)!=0:\n",
    "        with open(outfilename, 'wb') as output:\n",
    "            pickle.dump(features, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302adda0",
   "metadata": {},
   "source": [
    "## 2. create_instances_from_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a06ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingInstance_ext_tmp(object):\n",
    "    \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "    def __init__(self, tokens, segment_ids, label_ids, start_pos, end_pos, doc_ids, sent_ids):#, sep_loc):\n",
    "        self.tokens = tokens\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n",
    "        self.start_pos = start_pos \n",
    "        self.end_pos = end_pos\n",
    "        self.doc_ids = doc_ids\n",
    "        self.sent_ids = sent_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_seq_sides(tokens_a, doc_id_a, sent_id_a, doctype_a, section_a, label_a, \n",
    "                 tokens_b, doc_id_b, sent_id_b, doctype_b, section_b, label_b,\n",
    "                 max_num_tokens, rng):\n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "    if (len(tokens_a)+len(tokens_b))<=max_num_tokens:\n",
    "        return tokens_a, doc_id_a, sent_id_a, doctype_a, section_a, label_a, tokens_b, doc_id_b, sent_id_b, doctype_b, section_b, label_b\n",
    "    \n",
    "    if len(tokens_a)==0 and len(tokens_b)==0:\n",
    "        return tokens_a, doc_id_a, sent_id_a, doctype_a, section_a, label_a, tokens_b, doc_id_b, sent_id_b, doctype_b, section_b, label_b\n",
    "    \n",
    "    while True:\n",
    "        if (len(tokens_a)>len(tokens_b) and len(tokens_a)>0) or len(tokens_b)==0:\n",
    "            # check empty\n",
    "            if tokens_a:\n",
    "                del tokens_a[0]\n",
    "                del doc_id_a[0]\n",
    "                del sent_id_a[0]\n",
    "                del doctype_a[0]\n",
    "                del section_a[0]\n",
    "                del label_a[0]\n",
    "        \n",
    "        else:\n",
    "            # check empty\n",
    "            if tokens_b:\n",
    "                tokens_b.pop()\n",
    "                doc_id_b.pop()\n",
    "                sent_id_b.pop()\n",
    "                doctype_b.pop()\n",
    "                section_b.pop()\n",
    "                label_b.pop()\n",
    "            \n",
    "        if (len(tokens_a)+len(tokens_b))<=max_num_tokens:\n",
    "            return tokens_a, doc_id_a, sent_id_a, doctype_a, section_a, label_a, tokens_b, doc_id_b, sent_id_b, doctype_b, section_b, label_b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827234c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(targetfile, tokenizer, do_lower_case):\n",
    "    f = open(targetfile, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    doc_ids = []\n",
    "    sent_ids = []\n",
    "    doctypes = []\n",
    "    sections = []\n",
    "    labels = []\n",
    "    tokens_lines = []\n",
    "    \n",
    "    for l in range(len(lines)):\n",
    "        line = lines[l].strip(\"\\n\")\n",
    "        doc_id = line.split(\"\\t\")[0]\n",
    "        sent_id = int(line.split(\"\\t\")[1])\n",
    "        doctype = line.split(\"\\t\")[2]\n",
    "        section = line.split(\"\\t\")[3]\n",
    "        label   = int(line.split(\"\\t\")[4])\n",
    "        content = line.split(\"\\t\")[5]\n",
    "        \n",
    "        if do_lower_case==True:\n",
    "            content= content.lower()\n",
    "            \n",
    "        tokenstmp = tokenizer.tokenize(content)\n",
    "        \n",
    "        doc_id = str(doc_id)\n",
    "        not_zero_flag = False\n",
    "        for d in range(len(doc_id)):\n",
    "            if doc_id!=0:\n",
    "                doc_id = int(doc_id[d:])\n",
    "                not_zero_flag = True\n",
    "                break\n",
    "        if not_zero_flag==False:\n",
    "            doc_id = 0\n",
    "        \n",
    "        # 수집\n",
    "        doc_ids.append(doc_id)\n",
    "        sent_ids.append(sent_id)\n",
    "        doctypes.append(doctype)\n",
    "        sections.append(section)\n",
    "        labels.append(label)\n",
    "        tokens_lines.append(tokenstmp)\n",
    "    \n",
    "    return doc_ids, sent_ids, doctypes, sections, labels, tokens_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49dc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instances_from_document(input_file, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, rng, do_lower_case, vocab_words, mecab_sp_tokenizer, mode):\n",
    "    \n",
    "    instances = []\n",
    "    \n",
    "    ########### tokenize all documents ###########\n",
    "    doc_ids, sent_ids, doctypes, sections, labels, tokens_lines = read_documents(input_file, mecab_sp_tokenizer, do_lower_case)\n",
    "    \n",
    "    ########### 모든 정보를 concat ###########\n",
    "    doc_id_concat = []\n",
    "    sent_id_concat = []\n",
    "    doctype_concat = []\n",
    "    section_concat = []\n",
    "    label_concat = []\n",
    "    token_concat = []\n",
    "    \n",
    "    cut_interval = [0]\n",
    "    doc_ids_last = \"\"\n",
    "    \n",
    "    for t in range(len(tokens_lines)):\n",
    "        if doc_ids[t]!=doc_ids_last and t!=0:\n",
    "            cut_interval.append(len(token_concat))\n",
    "        \n",
    "        token_truncated = tokens_lines[t][:]\n",
    "        \n",
    "        token_concat = token_concat + token_truncated+[\"[SEP]\"]\n",
    "        doc_id_concat = doc_id_concat + [doc_ids[t]]*(len(token_truncated)+1)\n",
    "        sent_id_concat = sent_id_concat + [sent_ids[t]]*(len(token_truncated)+1)\n",
    "        doctype_concat = doctype_concat + [doctypes[t]]*(len(token_truncated)+1)\n",
    "        section_concat = section_concat + [sections[t]]*(len(token_truncated)+1)\n",
    "        label_concat = label_concat + [labels[t]]*(len(token_truncated)+1)\n",
    "        \n",
    "        doc_ids_last = doc_ids[t]\n",
    "        \n",
    "    cut_interval.append(len(token_concat))\n",
    "        \n",
    "    assert len(token_concat)==len(doc_id_concat)\n",
    "    assert len(token_concat)==len(sent_id_concat)\n",
    "    assert len(token_concat)==len(doctype_concat)\n",
    "    assert len(token_concat)==len(section_concat)\n",
    "    assert len(token_concat)==len(label_concat)\n",
    "    \n",
    "    if mode==\"train\":\n",
    "        rnd_start = 0\n",
    "        rnd_end = len(cut_interval)-2\n",
    "        if rnd_end<0:\n",
    "            point=0\n",
    "        else:\n",
    "            point = random.randint(rnd_start, rnd_end)\n",
    "    else:\n",
    "        point=0\n",
    "    \n",
    "    \n",
    "    ########### sampling ###########\n",
    "    for t in range(point, len(cut_interval)-1):\n",
    "        start = cut_interval[t]\n",
    "        end = cut_interval[t+1]\n",
    "        \n",
    "        # Find the start and end positions of the label\n",
    "        # Afterwards, cut the left and right 512 tokens based on the found location.\n",
    "        label_start_idx = 0\n",
    "        label_end_idx = 0\n",
    "        for l in range(len(label_concat[start:end])):\n",
    "            if label_concat[start:end][l]==1:\n",
    "                label_start_idx = l\n",
    "                break\n",
    "        for l in range(len(label_concat[start:end])-1, -1, -1):\n",
    "            if label_concat[start:end][l]==1:\n",
    "                label_end_idx = l\n",
    "                label_end_idx = label_end_idx + 1 # 미만 범위를 추출해야 하기 때문\n",
    "                break\n",
    "        \n",
    "        \n",
    "        # If the length of assessment exceeds 512,\n",
    "        # Force truncate to 100~200 tokens\n",
    "        assessmentlen = label_end_idx - label_start_idx\n",
    "        span = random.randint(100, 200)\n",
    "        if assessmentlen>=512:\n",
    "            label_end_idx = min(label_end_idx, label_start_idx + span)\n",
    "        \n",
    "        # truncate assessment + [SEP]\n",
    "        tokens_asmt = token_concat[start:end][label_start_idx:label_end_idx]\n",
    "        if tokens_asmt[-1]!=\"[SEP]\":\n",
    "            tokens_asmt[-1]=\"[SEP]\"\n",
    "        \n",
    "        \n",
    "        # set 512 tokens in total on the left and right side of the assessment section.\n",
    "        # left\n",
    "        tokens_a = token_concat[start:end][:label_start_idx]\n",
    "        doc_id_a = doc_id_concat[start:end][:label_start_idx]\n",
    "        sent_id_a = sent_id_concat[start:end][:label_start_idx]\n",
    "        doctype_a = doctype_concat[start:end][:label_start_idx]\n",
    "        section_a = section_concat[start:end][:label_start_idx]\n",
    "        label_a = label_concat[start:end][:label_start_idx]\n",
    "\n",
    "        # right\n",
    "        tokens_b = token_concat[start:end][label_end_idx:]\n",
    "        doc_id_b = doc_id_concat[start:end][label_end_idx:]\n",
    "        sent_id_b = sent_id_concat[start:end][label_end_idx:]\n",
    "        doctype_b = doctype_concat[start:end][label_end_idx:]\n",
    "        section_b = section_concat[start:end][label_end_idx:]\n",
    "        label_b = label_concat[start:end][label_end_idx:]\n",
    "        \n",
    "        # truncate left assessemnt right\n",
    "        max_trunlen = max_seq_length - (label_end_idx - label_start_idx) -1 # [CLS]\n",
    "        \n",
    "        tokens_a, doc_id_a, sent_id_a, doctype_a, section_a, label_a,\\\n",
    "        tokens_b, doc_id_b, sent_id_b, doctype_b, section_b, label_b = truncate_seq_sides(\n",
    "                 tokens_a, doc_id_a, sent_id_a, doctype_a, section_a, label_a, \n",
    "                 tokens_b, doc_id_b, sent_id_b, doctype_b, section_b, label_b,\n",
    "                 max_trunlen, rng)\n",
    "        \n",
    "        # after truncating\n",
    "        # token_left del sep\n",
    "        if len(tokens_a)>0:\n",
    "            if tokens_a[0]==\"[SEP]\":\n",
    "                del tokens_a[0]\n",
    "                del doc_id_a[0]\n",
    "                del sent_id_a[0]\n",
    "                del label_a[0]\n",
    "            \n",
    "        # token rihgt add sep\n",
    "        if len(tokens_b)>0:\n",
    "            if tokens_b[-1]!=\"[SEP]\":\n",
    "                tokens_b[-1] = \"[SEP]\"\n",
    "                #doc_id_b[-1] = \n",
    "                #sent_id_b[-1] = \n",
    "                #label_b[-1] = \n",
    "                \n",
    "\n",
    "        tokens = [\"[CLS]\"] + tokens_a + tokens_asmt + tokens_b\n",
    "        doc_ids = [-1] + doc_id_a + doc_id_concat[start:end][label_start_idx:label_end_idx] + doc_id_b\n",
    "        sent_ids = [-1] + sent_id_a + sent_id_concat[start:end][label_start_idx:label_end_idx] + sent_id_b\n",
    "        label_ids = [0] + label_a + label_concat[start:end][label_start_idx:label_end_idx] + label_b\n",
    "        \n",
    "        \n",
    "        # get position arrays\n",
    "        label_start_idx = 0\n",
    "        label_end_idx = 0\n",
    "        start_pos = []\n",
    "        end_pos = []\n",
    "        for l in range(len(label_ids)):\n",
    "            if label_ids[l]==1:\n",
    "                label_start_idx = l\n",
    "                break\n",
    "        for l in range(len(label_ids)-1, -1, -1):\n",
    "            if label_ids[l]==1:\n",
    "                label_end_idx = l\n",
    "                label_end_idx = label_end_idx + 1 # 미만범위를 출력해야 하기 떄문\n",
    "                break\n",
    "                \n",
    "        start_pos = [0]*len(label_ids)\n",
    "        start_pos[label_start_idx] = 1\n",
    "        end_pos = [0]*len(label_ids)\n",
    "        end_pos[label_end_idx-1] = 1 # 미만범위를 추출하느라 +1을 했었으므로, 여기에서는 인덱스를 맞춰서 -1 함\n",
    "        \n",
    "        # segment_ids\n",
    "        seg_id = 0\n",
    "        segment_ids = [0] # [CLS]\n",
    "        last_sent_id = sent_ids[0]\n",
    "        for s in range(1, len(sent_ids)): # [CLS] 제외한 루프\n",
    "            if last_sent_id!=sent_ids[s]:\n",
    "                seg_id = 1 if seg_id==0 else 0\n",
    "            segment_ids.append(seg_id)\n",
    "            last_sent_id = sent_ids[s]\n",
    "\n",
    "        assert len(tokens)==len(segment_ids)\n",
    "        assert len(segment_ids)==len(label_ids)\n",
    "        assert len(label_ids)==len(doc_ids)\n",
    "        assert len(doc_ids)==len(sent_ids)\n",
    "\n",
    "        instance = TrainingInstance_ext_tmp(\n",
    "                tokens = tokens,\n",
    "                segment_ids = segment_ids,\n",
    "                label_ids = label_ids, \n",
    "                start_pos = start_pos,\n",
    "                end_pos = end_pos,\n",
    "                doc_ids = doc_ids, \n",
    "                sent_ids = sent_ids, \n",
    "                #sep_loc = sep_loc\n",
    "            )\n",
    "        instances.append(instance)\n",
    "        \n",
    "        # In case of train, learning the entire data takes too long\n",
    "        # Learn by repeatedly extracting only some data as much as dup_factor\n",
    "        if mode==\"train\":\n",
    "            return instances\n",
    "        \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ebc3f",
   "metadata": {},
   "source": [
    "## 1. Create Training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(input_file, tokenizer, vocab_words, max_seq_length,\n",
    "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                              max_predictions_per_seq, rng, do_lower_case, mode):\n",
    "    \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "    instances = []\n",
    "    for _ in range(dupe_factor):\n",
    "        instances.extend(\n",
    "            create_instances_from_document(\n",
    "                input_file, max_seq_length, short_seq_prob,\n",
    "                masked_lm_prob, max_predictions_per_seq, rng, do_lower_case, \n",
    "                vocab_words, tokenizer, mode))\n",
    "\n",
    "    #print(\"len(instances): \", len(instances))\n",
    "        \n",
    "    return instances\n",
    "\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f529cf20",
   "metadata": {},
   "source": [
    "# main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    # required\n",
    "    input_files, \n",
    "    vocab_file, \n",
    "    outdir,\n",
    "    mode,\n",
    "    \n",
    "    # optional\n",
    "    do_lower_case=True, \n",
    "    max_seq_length=512, \n",
    "    max_predictions_per_seq = 20, \n",
    "    random_seed=12345, \n",
    "    dupe_factor = 1,\n",
    "    masked_lm_prob = 0.15,\n",
    "    short_seq_prob = 0.1,\n",
    "    ):\n",
    "    \n",
    "    # bertbase\n",
    "    print(\"vocab_file: \", vocab_file)\n",
    "    tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case, max_len=max_seq_length)\n",
    "    vocab_words = list(tokenizer.vocab.keys())\n",
    "    print(\"len(vocab_words): \", len(vocab_words))\n",
    "\n",
    "    print(\"max_predictions_per_seq: \", max_predictions_per_seq)\n",
    "    \n",
    "    randseed = random.randint(1, 1000)\n",
    "    print(\"randseed: \", randseed)\n",
    "    rng = random.Random(randseed)\n",
    "    \n",
    "    print(\"len(input_files): \", len(input_files))\n",
    "    \n",
    "    for i in range(0, len(input_files), 1):\n",
    "        if i%100==0:\n",
    "            print(str(i)+\"/\"+str(len(input_files)))\n",
    "        instances = create_training_instances(\n",
    "                        input_files[i], tokenizer, vocab_words, max_seq_length,\n",
    "                        dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                        max_predictions_per_seq, rng, do_lower_case, mode)\n",
    "        \n",
    "        filename = input_files[i].split(\"/\")[-1]\n",
    "        filename = filename.split(\".\")[0]+\".cache\"\n",
    "        \n",
    "        write_instance_to_example_files(instances=instances, \n",
    "                                    tokenizer=tokenizer, \n",
    "                                    vocab_words=vocab_words,\n",
    "                                    max_seq_length=max_seq_length,\n",
    "                                    max_predictions_per_seq=max_predictions_per_seq, \n",
    "                                    outfilename=outdir+\"/\"+filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12926e5f",
   "metadata": {},
   "source": [
    "# Generate Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f31043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"Generating Features...\")\n",
    "\n",
    "data_dirs = [\n",
    "    \"./data/05_sampled/train\",\n",
    "    \"./data/05_sampled/test\"\n",
    "]\n",
    "\n",
    "output_paths = [\n",
    "    \"bertbase_cased\",\n",
    "    \"mbert_cased\",\n",
    "    \"biobert\"\n",
    "]\n",
    "\n",
    "vocab_paths = [\n",
    "    \"../otherberts/bertbase_cased\",\n",
    "    \"../otherberts/mbert_cased\",\n",
    "    \"../otherberts/bioBERT/biobert_v1.1_pubmed\",\n",
    "]\n",
    "\n",
    "lowercase = [False, False, False]\n",
    "\n",
    "\n",
    "modes = [\"train\", \"test\"]\n",
    "dupe_factors = [4, 1]\n",
    "\n",
    "\n",
    "assert len(output_paths)==len(vocab_paths)\n",
    "assert len(vocab_paths)==len(lowercase)\n",
    "\n",
    "for d in range(len(data_dirs)):\n",
    "    mode = modes[d]\n",
    "    \n",
    "    for i in range(len(output_paths)):\n",
    "        print(\"data_dirs[d]: \", data_dirs[d])\n",
    "        \n",
    "        out_directory = \"./cache/\"+str(output_paths[i])+\"/\"+str(mode)\n",
    "        print(\"out_directory: \", out_directory)\n",
    "        if not os.path.exists(out_directory):\n",
    "            os.makedirs(out_directory)\n",
    "        \n",
    "        vocab_path=vocab_paths[i]\n",
    "        \n",
    "        input_files = glob.glob(data_dirs[d]+\"/*.txt\")\n",
    "        input_files.sort()\n",
    "        \n",
    "        main(\n",
    "            input_files = input_files, \n",
    "            outdir = out_directory,\n",
    "            vocab_file = vocab_paths[i]+'/vocab.txt', # 한글, 영문 모두 포함한 사전        \n",
    "            mode = mode,\n",
    "            \n",
    "            # optional\n",
    "            do_lower_case = lowercase[i], \n",
    "            max_seq_length = 512, \n",
    "            dupe_factor = dupe_factors[d], \n",
    "        )\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b1d073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36eac62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c908a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
