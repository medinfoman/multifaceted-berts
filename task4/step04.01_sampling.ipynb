{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3a0d77",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "## sampling rules\n",
    "- A sample contain a patient records, and is a concat of n documents within a patient records.\n",
    "- Select 2 documents for each patient\n",
    "- Select the AP of the document, look at the left and right directions of the Assessment section, and add the non-Assessment sections to compose the new document.\n",
    "- If the length is too long, truncate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf368926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c1313",
   "metadata": {},
   "source": [
    "## section names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc354447",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_types_paths = [\"./data/02_type_to_file_links/sections_visits_2011to2020_task4.txt\"]\n",
    "\n",
    "section_types = {}\n",
    "for i in range(len(section_types_paths)):\n",
    "    section_types_path = section_types_paths[i]\n",
    "    file = open(section_types_path, \"r\")\n",
    "    lines = file.readlines()\n",
    "    for l in range(len(lines)):\n",
    "        if lines[l]==\"\\n\":\n",
    "            continue\n",
    "        doctype = lines[l].split(\"\\t\")[0].strip(\"\\n\")\n",
    "        section = lines[l].split(\"\\t\")[1].strip(\"\\n\")    \n",
    "        key = doctype+\"\\t\"+section\n",
    "\n",
    "        val = lines[l].split(\"\\t\")[2].strip(\"\\n\")\n",
    "        \n",
    "        # Definition of val\n",
    "        # assessment sections are 1\n",
    "        # For sections other than assessment indicate 0\n",
    "        if \"assessment\" in val.lower():\n",
    "            section_types[key] = \"assessment\"\n",
    "        else:\n",
    "            section_types[key] = \"else\"\n",
    "                \n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496558da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_documents(path, doc_samples, rng):\n",
    "    file = open(path, \"r\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    \n",
    "    # Each line is the location information of the original document and sentence.\n",
    "    # Obtain only documents where both AP and SO exist\n",
    "    valid_docs = []\n",
    "    for l in range(len(lines)):\n",
    "        line = lines[l].strip(\"\\n\")\n",
    "        \n",
    "        # AP, SO 를 섹션 이름을 사용하여 판정함\n",
    "        data = line.split(\"\\t\")[1:]\n",
    "        flag_target = False\n",
    "        flag_else = False\n",
    "        for d in range(len(data)):\n",
    "            docinfo = data[d].split(\"/[DOC]\")[1]\n",
    "            doctype = docinfo.split(\"/[SEC]\")[0]\n",
    "            section = docinfo.split(\"/[SEC]\")[1]\n",
    "#             print(\"doctype: \", doctype)\n",
    "#             print(\"section: \", section)\n",
    "            key = doctype+\"\\t\"+section\n",
    "            if len(doctype)==0 and len(section)==0:\n",
    "                continue\n",
    "            #print(\"key: \", key)\n",
    "            if section_types[key]==\"assessment\":\n",
    "                flag_target=True\n",
    "            elif section_types[key]==\"else\":\n",
    "                flag_else = True\n",
    "        \n",
    "        # 두 개 모두 들어있는 문서들만 취급\n",
    "        if flag_target==True and flag_else==True:\n",
    "            valid_docs.append(line)\n",
    "            \n",
    "            \n",
    "    #print(\"valid_docs: \", valid_docs)\n",
    "    random.shuffle(valid_docs)\n",
    "    #print(\"valid_docs: \", valid_docs)\n",
    "    valid_docs = valid_docs[:doc_samples]\n",
    "    \n",
    "    valid_docs.sort()\n",
    "    \n",
    "    return valid_docs\n",
    "\n",
    "def read_text(path):\n",
    "    # read data\n",
    "    filename = filepaths[i].split(\"/\")[-1]\n",
    "    filename_int = \"\"\n",
    "    #print(\"filename: \", filename)\n",
    "    for j in range(len(filename)):\n",
    "        if filename[j]!=0:\n",
    "            filename_int = int(filename[j:-len(\".txt\")])\n",
    "            break\n",
    "    \n",
    "    groupname = str(((filename_int//docperfile)+1)*docperfile)\n",
    "    \n",
    "    target_path = \"../task2/data/\"+str(category)+\"/\"+str(groupname)+\"/\"+str(filename)\n",
    "    \n",
    "    file = open(target_path, \"r\")\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    \n",
    "    # date, doctype, section, contents\n",
    "    linee = [line.strip(\"\\n\").split(\"\\t\")[-1] for line in lines]\n",
    "    \n",
    "    return linee\n",
    "\n",
    "# Obtain sentences at the indexed location from the document\n",
    "def extract_sample(got_valid_docs, textlines):\n",
    "    doc_id_lines = []\n",
    "    sent_id_lines = []\n",
    "    doctype_lines = []\n",
    "    section_lines = []    \n",
    "    labels = []\n",
    "    extractedlines = []\n",
    "    \n",
    "    for v in range(len(got_valid_docs)):\n",
    "        docinfo = got_valid_docs[v]\n",
    "        soap_span = docinfo.split(\"\\t\")\n",
    "        doc_id = soap_span[0]\n",
    "        print(\"doc_id: \", doc_id)\n",
    "        \n",
    "        # Sentence number, start/end/label...\n",
    "        for s in range(1, len(soap_span)):\n",
    "            #print(\"soap_span[s]: \", soap_span[s])\n",
    "            startline = int(soap_span[s].split(\"/\")[0])\n",
    "            endline   = int(soap_span[s].split(\"/\")[1])+1\n",
    "            \n",
    "            docinfo = soap_span[s].split(\"/[DOC]\")[1]\n",
    "            doctype = docinfo.split(\"/[SEC]\")[0]\n",
    "            section = docinfo.split(\"/[SEC]\")[1]\n",
    "#             print(\"doctype: \", doctype)\n",
    "#             print(\"section: \", section)\n",
    "            key = doctype+\"\\t\"+section\n",
    "            if len(doctype)==0 and len(section)==0:\n",
    "                continue\n",
    "            \n",
    "            soap = section_types[key]\n",
    "            #print(\"soap: \", soap)\n",
    "            \n",
    "            if soap==\"else\":\n",
    "                label = 0\n",
    "            elif soap==\"assessment\":\n",
    "                label = 1\n",
    "             \n",
    "            sent_ids_tmp = []\n",
    "            for s in range(startline, endline):\n",
    "                sent_ids_tmp.append(s)            \n",
    "            \n",
    "            targetlines = textlines[startline:endline]\n",
    "            extractedlines = extractedlines + targetlines\n",
    "            labels = labels + [str(label)]*len(targetlines)\n",
    "            doc_id_lines = doc_id_lines + [str(doc_id)]*len(targetlines)\n",
    "            sent_id_lines = sent_id_lines + sent_ids_tmp \n",
    "            doctype_lines = doctype_lines + [str(doctype)]*len(targetlines)\n",
    "            section_lines = section_lines + [str(section)]*len(targetlines)\n",
    "            \n",
    "    \n",
    "    assert len(extractedlines)==len(labels)\n",
    "    \n",
    "    return doc_id_lines, sent_id_lines, doctype_lines, section_lines, extractedlines, labels\n",
    "\n",
    "\n",
    "def write_sample(doc_ids, sent_id_lines, doctype_lines, section_lines, \n",
    "                 extractedlines, labels, out_folder, filename):\n",
    "    outtext = []\n",
    "    for e in range(len(extractedlines)):\n",
    "#         print(labels[e]+\"\\t\"+extractedlines[e])\n",
    "        outtext.append(str(doc_ids[e])+\"\\t\"+\n",
    "                       str(sent_id_lines[e])+\"\\t\"+\n",
    "                       str(doctype_lines[e])+\"\\t\"+\n",
    "                       str(section_lines[e])+\"\\t\"+\n",
    "                       str(labels[e])+\"\\t\"+\n",
    "                       str(extractedlines[e]))\n",
    "    \n",
    "    if len(outtext)>0:\n",
    "        file = open(out_folder+\"/\"+filename, \"a\")\n",
    "        file.write(\"\\n\".join(outtext)+\"\\n\")\n",
    "        file.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a043357",
   "metadata": {},
   "source": [
    "## trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a4cdc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "docperfile = 50\n",
    "doc_samples = 20 # How many documents will be used per patient?\n",
    "rng = random.random()\n",
    "dup_factor = 1\n",
    "\n",
    "category = \"visits_2011to2020\"\n",
    "\n",
    "index_folder = \"./data/03_soap_index/train\"\n",
    "filepaths = glob.glob(index_folder+\"/*.txt\")\n",
    "filepaths.sort()\n",
    "print(\"len(filepaths): \", len(filepaths))\n",
    "\n",
    "out_folder = \"./data/04_sampled/train\"\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "\n",
    "for d in range(dup_factor):\n",
    "    for i in range(len(filepaths)):\n",
    "        #print(\"filepaths[i]: \", filepaths[i])\n",
    "        if i%100==0:\n",
    "            print(str(i)+\"/\"+str(len(filepaths)))\n",
    "        filename = filepaths[i].split(\"/\")[-1]\n",
    "        got_valid_docs = get_valid_documents(path=filepaths[i], doc_samples=doc_samples, rng=rng)\n",
    "        textlines = read_text(path=filepaths[i])\n",
    "        #print(\"textlines: \", textlines)\n",
    "        doc_id_lines, sent_id_lines, doctype_lines, section_lines, extractedlines, labels = extract_sample(got_valid_docs=got_valid_docs, textlines=textlines)\n",
    "        write_sample(doc_ids=doc_id_lines, sent_id_lines=sent_id_lines, \n",
    "                     doctype_lines=doctype_lines, \n",
    "                     section_lines=section_lines, \n",
    "                     extractedlines=extractedlines, \n",
    "                     labels=labels, out_folder=out_folder, \n",
    "                     filename=filename)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3dbef",
   "metadata": {},
   "source": [
    "## test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee845491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "docperfile = 50\n",
    "doc_samples = 5\n",
    "rng = random.random()\n",
    "dup_factor = 1\n",
    "\n",
    "category = \"visits_2011to2020\"\n",
    "\n",
    "index_folder = \"./data/03_soap_index/test\"\n",
    "filepaths = glob.glob(index_folder+\"/*.txt\")\n",
    "filepaths.sort()\n",
    "print(\"len(filepaths): \", len(filepaths))\n",
    "\n",
    "out_folder = \"./data/04_sampled/test\"\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "\n",
    "for d in range(dup_factor):\n",
    "    for i in range(len(filepaths)):\n",
    "        print(\"filepaths[i]: \", filepaths[i])\n",
    "        if i%100==0:\n",
    "            print(str(i)+\"/\"+str(len(filepaths)))\n",
    "        \n",
    "        filename = filepaths[i].split(\"/\")[-1]\n",
    "        got_valid_docs = get_valid_documents(path=filepaths[i], doc_samples=doc_samples, rng=rng)\n",
    "        textlines = read_text(path=filepaths[i])\n",
    "        doc_id_lines, sent_id_lines, doctype_lines, section_lines, extractedlines, labels = extract_sample(got_valid_docs=got_valid_docs, textlines=textlines)\n",
    "        write_sample(doc_ids=doc_id_lines, sent_id_lines=sent_id_lines, \n",
    "                     doctype_lines=doctype_lines, \n",
    "                     section_lines=section_lines, \n",
    "                     extractedlines=extractedlines, \n",
    "                     labels=labels, out_folder=out_folder, \n",
    "                     filename=filename)\n",
    "    \n",
    "print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6a057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e24a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc37aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
