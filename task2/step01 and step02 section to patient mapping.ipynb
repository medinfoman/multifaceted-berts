{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf83aa55",
   "metadata": {},
   "source": [
    "# step01. Make source text\n",
    "- In task 2, text from different sections must be extracted.\n",
    "- However, it is difficult to find the section in ../preprocessing/01_data4finetune/SNUH_visit_2011to2020.\n",
    "- Therefore, the preprocessing started from Excel file (../sourcedata/???.xlsx) containing the sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352bd270",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = ['../sources/visits_2011to2020/*.xlsx']\n",
    "docperfile = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pools = []\n",
    "train_pool = \"../preprocessing/01_data4finetune/pts_SNUH_visit_2011to2020_heldout_train.txt\"\n",
    "test_pool  = \"../preprocessing/01_data4finetune/pts_SNUH_visit_2011to2020_heldout_test.txt\"\n",
    "\n",
    "def target_ptnums(target_file):\n",
    "    target_pool = []\n",
    "    file = open(target_file, \"r\")\n",
    "    lines = file.readlines()\n",
    "    for l in range(len(lines)):\n",
    "        line = lines[l].strip(\"\\n\")\n",
    "        ptnum = line.split(\"/\")[-1].replace(\".txt\", \"\")\n",
    "        target_pool.append(ptnum)\n",
    "    file.close()\n",
    "    \n",
    "    return target_pool\n",
    "\n",
    "pools_train = target_ptnums(train_pool)\n",
    "pools_test  = target_ptnums(test_pool)\n",
    "pools = pools_train + pools_test\n",
    "print(\"len(pools): \", len(pools))\n",
    "print(pools[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348c8d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "for s in range(len(sources)):\n",
    "    outputdir = sources[s].split(\"/\")[-2]\n",
    "    output_directory = \"./data/\"+str(outputdir)\n",
    "    print(\"output_directory: \", output_directory)\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    filenames = glob.glob(sources[s])\n",
    "    filenames.sort()\n",
    "    print(filenames)\n",
    "    for f in range(len(filenames)):\n",
    "        reading_file = filenames[f].split(\"/\")[-1]\n",
    "        print(\"reading_file: \", reading_file)\n",
    "\n",
    "        print(\"reading...\")\n",
    "        ws = pd.read_excel(filenames[f])\n",
    "\n",
    "        print(\"processing...\")\n",
    "        \n",
    "        print(str(f) + \" / \" + str(len(filenames)) + \"...\" + filenames[f])\n",
    "        for row in range(len(ws)):\n",
    "            if (row+1)%1000==0:\n",
    "                print(str(row) + \" / \" + str(len(ws)))\n",
    "            \n",
    "            # 각 row 마다 처리\n",
    "            outtext = []\n",
    "            \n",
    "            pt_id = ws['fake_id'][row]\n",
    "#             print(\"pt_id: \", pt_id)\n",
    "            \n",
    "    \n",
    "            # process target pools only\n",
    "            zeros = \"\".join([\"0\"]*(8-len(str(pt_id))))\n",
    "            pt_id_str = zeros + str(pt_id)\n",
    "            \n",
    "            if pt_id_str not in pools:\n",
    "                continue\n",
    "            \n",
    "            date = ws['수진(진료)일'][row]\n",
    "            \n",
    "            doctype = ws['서식명'][row]\n",
    "            doctype = preprocessing.text_preprocessing(doctype)        \n",
    "            doctype = \"\".join(doctype)\n",
    "            doctype = doctype.replace(\"\\n\", \" \")\n",
    "#             print(\"doctype: \", doctype)\n",
    "            \n",
    "            section = ws['서식항목명'][row]\n",
    "            section = preprocessing.text_preprocessing(section)\n",
    "            section = \"\".join(section)\n",
    "            section = section.replace(\"\\n\", \" \")\n",
    "#             print(\"section: \", section)\n",
    "            \n",
    "            content = ws['서식내용'][row]\n",
    "            content = preprocessing.text_preprocessing(content)\n",
    "#             print(\"content: \", content)\n",
    "            \n",
    "            \n",
    "            for c in range(len(content)):\n",
    "                content_sub = content[c].split(\"\\n\")\n",
    "                for s in range(len(content_sub)):\n",
    "                    if content_sub[s]!=\"\":\n",
    "                        outtext.append(date+\"\\t\"+doctype+\"\\t\"+section+\"\\t\"+content_sub[s])\n",
    "            \n",
    "            target_folder = output_directory+\"/\"+str(((pt_id//docperfile)+1)*docperfile)\n",
    "            if not os.path.exists(target_folder):\n",
    "                os.makedirs(target_folder)\n",
    "                \n",
    "            file = open(target_folder+\"/\"+pt_id_str+\".txt\", \"a\")\n",
    "            file.write(\"\\n\".join(outtext)+\"\\n\")\n",
    "            file.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb7e9f8",
   "metadata": {},
   "source": [
    "# step02. Dictionary of 'section to patient id'\n",
    "- make a dictionary which cover section name to filename(=patient fake id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "paths = [\n",
    "    \"./data/visits_2011to2020/*\"\n",
    "]\n",
    "\n",
    "for p in range(len(paths)):\n",
    "    type_to_file = {}\n",
    "    \n",
    "    path = paths[p]\n",
    "    groups = glob.glob(path)\n",
    "    groups.sort()\n",
    "    \n",
    "    category = path.split(\"/\")[-2]\n",
    "    \n",
    "    for g in range(len(groups)):\n",
    "        files = glob.glob(groups[g]+\"/*.txt\")\n",
    "        files.sort()\n",
    "        \n",
    "        print(str(g)+\"/\"+str(len(groups)) +\" ... \"+str(path))\n",
    "#         print(\"groups[g]: \", groups[g])\n",
    "        \n",
    "        for f in range(len(files)):\n",
    "            file = open(files[f], \"r\")\n",
    "            lines = file.readlines()\n",
    "#             print(\"files[f]: \", files[f])\n",
    "            \n",
    "            #print(str(f)+\"/\"+str(len(files))+\" ... \"+str(g)+\"/\"+str(len(groups)) +\"...\"+str(path))\n",
    "            \n",
    "            for l in range(len(lines)):\n",
    "                line = lines[l]\n",
    "#                 print(\"line: \", line)\n",
    "                if line==\"\\n\":\n",
    "                    continue\n",
    "                date = line.split(\"\\t\")[0]\n",
    "                doctype = line.split(\"\\t\")[1]\n",
    "                section = line.split(\"\\t\")[2]\n",
    "                content_type = doctype+\"\\t\"+section\n",
    "#                 print(\"content_type: \", content_type)\n",
    "                \n",
    "                # type 과 파일 링크\n",
    "                if content_type not in type_to_file:\n",
    "                    type_to_file[content_type] = [files[f]]\n",
    "                else:\n",
    "                    link_paths = type_to_file[content_type]\n",
    "                    if files[f] not in link_paths:\n",
    "                        if len(link_paths)<=10:\n",
    "                            link_paths.append(files[f])\n",
    "                        \n",
    "                    type_to_file[content_type] = link_paths\n",
    "        \n",
    "    \n",
    "    outtext = []\n",
    "    for key in type_to_file:\n",
    "        vals = type_to_file[key]\n",
    "        for v in range(len(vals)):\n",
    "            outtext.append(key+\"\\t\"+vals[v])\n",
    "    \n",
    "    target_folder = \"./data/02_type_to_file_links\"\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    file = open(target_folder+\"/sections_\"+str(category)+\"_task2.txt\", \"w\")\n",
    "    file.write(\"\\n\".join(outtext))\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b45b3b",
   "metadata": {},
   "source": [
    "# # Sections_visits_2011to2020_task2.txt file was manually curated to classify what was assessment & plan and what was not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b44459",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibert",
   "language": "python",
   "name": "multibert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
